{"embeddings": [[19.685197830200195, 8.325590133666992], [18.672605514526367, 8.159607887268066], [21.40937614440918, 6.929281711578369], [19.889484405517578, 7.880576133728027], [19.207639694213867, 7.522487163543701], [20.1778507232666, 7.14797306060791], [18.984539031982422, 6.402237892150879], [19.398601531982422, 7.209878444671631], [19.513010025024414, 6.244405269622803], [19.04433250427246, 8.673678398132324], [20.255958557128906, 6.15014123916626], [20.813373565673828, 7.4748921394348145], [20.44625473022461, 8.212811470031738], [20.66844940185547, 6.181722164154053], [19.22344207763672, 9.009994506835938], [20.65273094177246, 6.932505130767822], [19.70671272277832, 7.344451904296875], [20.166181564331055, 5.752789497375488], [19.29920196533203, 8.160104751586914], [21.1240177154541, 6.088860511779785], [21.591806411743164, 6.497808933258057], [20.87076187133789, 7.944843292236328], [21.432079315185547, 5.749305725097656], [21.02181625366211, 8.31505012512207], [20.103317260742188, 9.042247772216797], [19.144977569580078, 5.915710926055908], [19.056690216064453, 5.569642543792725], [18.326231002807617, 7.234921932220459], [19.066781997680664, 7.761067867279053], [18.478561401367188, 7.708087921142578], [19.73192024230957, 8.871984481811523], [20.142040252685547, 7.533218860626221], [19.52617645263672, 6.668447971343994], [20.93365478515625, 6.574752330780029], [21.333412170410156, 7.281826972961426], [20.707000732421875, 5.70522928237915], [19.038654327392578, 6.827605247497559], [18.434154510498047, 6.506165981292725], [18.28616714477539, 6.886826992034912], [20.12114906311035, 8.575336456298828], [18.860565185546875, 6.935899257659912], [18.678159713745117, 5.7708940505981445]], "keys": ["2401.10927", "2401.10989", "2401.11", "2401.11001", "2401.1107", "2401.11075", "2401.11093", "2401.11096", "2401.11119", "2401.11128", "2401.11263", "2401.11265", "2401.11272", "2401.11278", "2401.11327", "2401.11332", "2401.11346", "2401.11352", "2401.11359", "2401.11368", "2401.11507", "2401.11515", "2401.11537", "2401.1154", "2401.11548", "2401.11554", "2401.11562", "2401.11607", "2401.11646", "2401.11665", "2401.11672", "2401.11789", "2401.11804", "2401.11827", "2401.11837", "2401.11842", "2401.11885", "2401.11896", "2401.12031", "2401.1219", "2401.12195", "2401.12216"], "additional_info": [{"arxiv_id": "2401.10927", "title": "Debiasing and a local analysis for population clustering using\n  semidefinite programming", "abstract": "In this paper, we consider the problem of partitioning a small data sample of\nsize $n$ drawn from a mixture of $2$ sub-gaussian distributions. In particular,\nwe analyze computational efficient algorithms proposed by the same author, to\npartition data into two groups approximately according to their population of\norigin given a small sample. This work is motivated by the application of\nclustering individuals according to their population of origin using $p$\nmarkers, when the divergence between any two of the populations is small. We\nbuild upon the semidefinite relaxation of an integer quadratic program that is\nformulated essentially as finding the maximum cut on a graph, where edge\nweights in the cut represent dissimilarity scores between two nodes based on\ntheir $p$ features. Here we use $\\Delta^2 :=p \\gamma$ to denote the $\\ell_2^2$\ndistance between two centers (mean vectors), namely, $\\mu^{(1)}$, $\\mu^{(2)}$\n$\\in$ $\\mathbb{R}^p$. The goal is to allow a full range of tradeoffs between\n$n, p, \\gamma$ in the sense that partial recovery (success rate $< 100\\%$) is\nfeasible once the signal to noise ratio $s^2 := \\min\\{np \\gamma^2, \\Delta^2\\}$\nis lower bounded by a constant. Importantly, we prove that the\nmisclassification error decays exponentially with respect to the SNR $s^2$.\nThis result was introduced earlier without a full proof. We therefore present\nthe full proof in the present work. Finally, for balanced partitions, we\nconsider a variant of the SDP1, and show that the new estimator has a superb\ndebiasing property. This is novel to the best of our knowledge.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.10989", "title": "Provably Scalable Black-Box Variational Inference with Structured\n  Variational Families", "abstract": "Variational families with full-rank covariance approximations are known not\nto work well in black-box variational inference (BBVI), both empirically and\ntheoretically. In fact, recent computational complexity results for BBVI have\nestablished that full-rank variational families scale poorly with the\ndimensionality of the problem compared to e.g. mean field families. This is\nparticularly critical to hierarchical Bayesian models with local variables;\ntheir dimensionality increases with the size of the datasets. Consequently, one\ngets an iteration complexity with an explicit $\\mathcal{O}(N^2)$ dependence on\nthe dataset size $N$. In this paper, we explore a theoretical middle ground\nbetween mean-field variational families and full-rank families: structured\nvariational families. We rigorously prove that certain scale matrix structures\ncan achieve a better iteration complexity of $\\mathcal{O}(N)$, implying better\nscaling with respect to $N$. We empirically verify our theoretical results on\nlarge-scale hierarchical models.", "field": "Statistics", "categories": "stat.ML,cs.LG,stat.CO"}, {"arxiv_id": "2401.11", "title": "Human-Centric and Integrative Lighting Asset Management in Public\n  Libraries: Qualitative Insights and Challenges from a Swedish Field Study", "abstract": "Traditional lighting source reliability evaluations, often covering just half\nof a lamp's volume, can misrepresent real-world performance. To overcome these\nlimitations,adopting advanced asset management strategies for a more holistic\nevaluation is crucial. This paper investigates human-centric and integrative\nlighting asset management in Swedish public libraries. Through field\nobservations, interviews, and gap analysis, the study highlights a disparity\nbetween current lighting conditions and stakeholder expectations, with issues\nlike eye strain suggesting significant improvement potential. We propose a\nshift towards more dynamic lighting asset management and reliability\nevaluations, emphasizing continuous enhancement and comprehensive training in\nhuman-centric and integrative lighting principles.", "field": "Statistics", "categories": "stat.OT"}, {"arxiv_id": "2401.11001", "title": "Reply to Comment by Schilling on their paper \"Optimal and fast\n  confidence intervals for hypergeometric successes\"", "abstract": "A response to a letter to the editor by Schilling regarding Bartroff, Lorden,\nand Wang (\"Optimal and fast confidence intervals for hypergeometric successes\"\n2022, arXiv:2109.05624)", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.1107", "title": "Efficient Data Reduction Strategies for Big Data and High-Dimensional\n  LASSO Regressions", "abstract": "The IBOSS approach proposed by Wang et al. (2019) selects the most\ninformative subset of n points. It assumes that the ordinary least squares\nmethod is used and requires that the number of variables, p, is not large.\nHowever, in many practical problems, p is very large and penalty-based model\nfitting methods such as LASSO is used. We study the big data problems, in which\nboth n and p are large. In the first part, we focus on reduction in data\npoints. We develop theoretical results showing that the IBOSS type of approach\ncan be applicable to penalty-based regressions such as LASSO. In the second\npart, we consider the situations where p is extremely large. We propose a\ntwo-step approach that involves first reducing the number of variables and then\nreducing the number of data points. Two separate algorithms are developed,\nwhose performances are studied through extensive simulation studies. Compared\nto existing methods including well-known split-and-conquer approach, the\nproposed methods enjoy advantages in terms of estimation accuracy, prediction\naccuracy, and computation time.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11075", "title": "Estimating the Hawkes process from a discretely observed sample path", "abstract": "The Hawkes process is a widely used model in many areas, such as\n  finance, seismology, neuroscience, epidemiology, and social\n  sciences. Estimation of the Hawkes process from continuous\n  observations of a sample path is relatively straightforward using\n  either the maximum likelihood or other methods. However, estimating\n  the parameters of a Hawkes process from observations of a sample\n  path at discrete time points only is challenging due to the\n  intractability of the likelihood with such data. In this work, we\n  introduce a method to estimate the Hawkes process from a discretely\n  observed sample path. The method takes advantage of a state-space\n  representation of the incomplete data problem and use the sequential\n  Monte Carlo (aka particle filtering) to approximate the likelihood\n  function. As an estimator of the likelihood function the SMC\n  approximation is unbiased, and therefore it can be used together\n  with the Metropolis-Hastings algorithm to construct Markov Chains to\n  approximate the likelihood distribution, or more generally, the\n  posterior distribution of model parameters. The performance of the\n  methodology is assessed using simulation experiments and compared\n  with other recently published methods. The proposed estimator is\n  found to have a smaller mean square error than the two benchmark\n  estimators. The proposed method has the additional advantage that\n  confidence intervals for the parameters are easily available. We\n  apply the proposed estimator to the analysis of weekly count data on\n  measles cases in Tokyo Japan and compare the results to those by\n  one of the benchmark methods.", "field": "Statistics", "categories": "stat.ME,stat.CO"}, {"arxiv_id": "2401.11093", "title": "Learned Image Compression with Dual-Branch Encoder and Conditional\n  Information Coding", "abstract": "Recent advancements in deep learning-based image compression are notable.\nHowever, prevalent schemes that employ a serial context-adaptive entropy model\nto enhance rate-distortion (R-D) performance are markedly slow. Furthermore,\nthe complexities of the encoding and decoding networks are substantially high,\nrendering them unsuitable for some practical applications. In this paper, we\npropose two techniques to balance the trade-off between complexity and\nperformance. First, we introduce two branching coding networks to independently\nlearn a low-resolution latent representation and a high-resolution latent\nrepresentation of the input image, discriminatively representing the global and\nlocal information therein. Second, we utilize the high-resolution latent\nrepresentation as conditional information for the low-resolution latent\nrepresentation, furnishing it with global information, thus aiding in the\nreduction of redundancy between low-resolution information. We do not utilize\nany serial entropy models. Instead, we employ a parallel channel-wise\nauto-regressive entropy model for encoding and decoding low-resolution and\nhigh-resolution latent representations. Experiments demonstrate that our method\nis approximately twice as fast in both encoding and decoding compared to the\nparallelizable checkerboard context model, and it also achieves a 1.2%\nimprovement in R-D performance compared to state-of-the-art learned image\ncompression schemes. Our method also outperforms classical image codecs\nincluding H.266/VVC-intra (4:4:4) and some recent learned methods in\nrate-distortion performance, as validated by both PSNR and MS-SSIM metrics on\nthe Kodak dataset.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.11096", "title": "Asymptotic Normality of the Conditional Value-at-Risk based Pickands\n  Estimator", "abstract": "The Pickands estimator for the extreme value index is beneficial due to its\nuniversal consistency, location, and scale invariance, which sets it apart from\nother types of estimators. However, similar to many extreme value index\nestimators, it is marked by poor asymptotic efficiency. Chen (2021) introduces\na Conditional Value-at-Risk (CVaR)-based Pickands estimator, establishes its\nconsistency, and demonstrates through simulations that this estimator\nsignificantly reduces mean squared error while preserving its location and\nscale invariance. The initial focus of this paper is on demonstrating the weak\nconvergence of the empirical CVaR in functional space. Subsequently, based on\nthe established weak convergence, the paper presents the asymptotic normality\nof the CVaR-based Pickands estimator. It further supports these theoretical\nfindings with empirical evidence obtained through simulation studies.", "field": "Statistics", "categories": "math.ST,stat.OT,stat.TH"}, {"arxiv_id": "2401.11119", "title": "Constraint-based measures of shift and relative shift for discrete\n  frequency distributions", "abstract": "Comparisons of frequency distributions often invoke the concept of shift to\ndescribe directional changes in properties such as the mean. In the present\nstudy, we sought to define shift as a property in and of itself. Specifically,\nwe define distributional shift (DS) as the concentration of frequencies away\nfrom the discrete class having the greatest value (e.g., the right-most bin of\na histogram). We derive a measure of DS using the normalized sum of\nexponentiated cumulative frequencies. We then define relative distributional\nshift (RDS) as the difference in DS between two distributions, revealing the\nmagnitude and direction by which one distribution is concentrated to lesser or\ngreater discrete classes relative to another. We find that RDS is highly\nrelated to popular measures that, while based on the comparison of frequency\ndistributions, do not explicitly consider shift. While RDS provides a useful\ncomplement to other comparative measures, DS allows shift to be quantified as a\nproperty of individual distributions, similar in concept to a statistical\nmoment.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11128", "title": "Regularized Estimation of Sparse Spectral Precision Matrices", "abstract": "Spectral precision matrix, the inverse of a spectral density matrix, is an\nobject of central interest in frequency-domain analysis of multivariate time\nseries. Estimation of spectral precision matrix is a key step in calculating\npartial coherency and graphical model selection of stationary time series. When\nthe dimension of a multivariate time series is moderate to large, traditional\nestimators of spectral density matrices such as averaged periodograms tend to\nbe severely ill-conditioned, and one needs to resort to suitable regularization\nstrategies involving optimization over complex variables.\n  In this work, we propose complex graphical Lasso (CGLASSO), an\n$\\ell_1$-penalized estimator of spectral precision matrix based on local\nWhittle likelihood maximization. We develop fast $\\textit{pathwise coordinate\ndescent}$ algorithms for implementing CGLASSO on large dimensional time series\ndata sets. At its core, our algorithmic development relies on a ring\nisomorphism between complex and real matrices that helps map a number of\noptimization problems over complex variables to similar optimization problems\nover real variables. This finding may be of independent interest and more\nbroadly applicable for high-dimensional statistical analysis with\ncomplex-valued data. We also present a complete non-asymptotic theory of our\nproposed estimator which shows that consistent estimation is possible in\nhigh-dimensional regime as long as the underlying spectral precision matrix is\nsuitably sparse. We compare the performance of CGLASSO with competing\nalternatives on simulated data sets, and use it to construct partial coherence\nnetwork among brain regions from a real fMRI data set.", "field": "Statistics", "categories": "stat.ME,stat.CO,62H12, 62J07, 62M10, 62M15,G.3; I.5.2"}, {"arxiv_id": "2401.11263", "title": "Estimating heterogeneous treatment effect from survival outcomes via\n  (orthogonal) censoring unbiased learning", "abstract": "Methods for estimating heterogeneous treatment effects (HTE) from\nobservational data have largely focused on continuous or binary outcomes, with\nless attention paid to survival outcomes and almost none to settings with\ncompeting risks. In this work, we develop censoring unbiased transformations\n(CUTs) for survival outcomes both with and without competing risks.After\nconverting time-to-event outcomes using these CUTs, direct application of HTE\nlearners for continuous outcomes yields consistent estimates of heterogeneous\ncumulative incidence effects, total effects, and separable direct effects. Our\nCUTs enable application of a much larger set of state of the art HTE learners\nfor censored outcomes than had previously been available, especially in\ncompeting risks settings. We provide generic model-free learner-specific oracle\ninequalities bounding the finite-sample excess risk. The oracle efficiency\nresults depend on the oracle selector and estimated nuisance functions from all\nsteps involved in the transformation. We demonstrate the empirical performance\nof the proposed methods in simulation studies.", "field": "Statistics", "categories": "stat.ME,stat.ML"}, {"arxiv_id": "2401.11265", "title": "Assessing the Competitiveness of Matrix-Free Block Likelihood Estimation\n  in Spatial Models", "abstract": "In geostatistics, block likelihood offers a balance between statistical\naccuracy and computational efficiency when estimating covariance functions.\nThis balance is reached by dividing the sample into blocks and computing a\nweighted sum of (sub) log-likelihoods corresponding to pairs of blocks.\nPractitioners often choose block sizes ranging from hundreds to a few thousand\nobservations, inherently involving matrix-based implementations. An\nalternative, residing at the opposite end of this methodological spectrum,\ntreats each observation as a block, resulting in the matrix-free pairwise\nlikelihood method. We propose an additional alternative within this broad\nmethodological landscape, systematically constructing blocks of size two and\nmerging pairs of blocks through conditioning. Importantly, our method\nstrategically avoids large-sized blocks, facilitating explicit calculations\nthat ultimately do not rely on matrix computations. Studies with both simulated\nand real data validate the effectiveness of our approach, on one hand\ndemonstrating its superiority over pairwise likelihood, and on the other,\nchallenging the intuitive notion that employing matrix-based versions\nuniversally lead to better statistical performance.", "field": "Statistics", "categories": "stat.ME,math.ST,stat.TH"}, {"arxiv_id": "2401.11272", "title": "Asymptotics for non-degenerate multivariate $U$-statistics with\n  estimated nuisance parameters under the null and local alternative hypotheses", "abstract": "The large-sample behavior of non-degenerate multivariate $U$-statistics of\narbitrary degree is investigated under the assumption that their kernel depends\non parameters that can be estimated consistently. Mild regularity conditions\nare given which guarantee that once properly normalized, such statistics are\nasymptotically multivariate Gaussian both under the null hypothesis and\nsequences of local alternatives. The work of Randles (1982, Ann. Statist.) is\nextended in three ways: the data and the kernel values can be multivariate\nrather than univariate, the limiting behavior under local alternatives is\nstudied for the first time, and the effect of knowing some of the nuisance\nparameters is quantified. These results can be applied to a broad range of\ngoodness-of-fit testing contexts, as shown in one specific example.", "field": "Statistics", "categories": "math.ST,math.PR,stat.AP,stat.ME,stat.TH,62E20, 62F05, 62F12, 62F03, 62H10, 62H12, 62H15"}, {"arxiv_id": "2401.11278", "title": "Handling incomplete outcomes and covariates in cluster-randomized\n  trials: doubly-robust estimation, efficiency considerations, and sensitivity\n  analysis", "abstract": "In cluster-randomized trials, missing data can occur in various ways,\nincluding missing values in outcomes and baseline covariates at the individual\nor cluster level, or completely missing information for non-participants. Among\nthe various types of missing data in CRTs, missing outcomes have attracted the\nmost attention. However, no existing method comprehensively addresses all the\naforementioned types of missing data simultaneously due to their complexity.\nThis gap in methodology may lead to confusion and potential pitfalls in the\nanalysis of CRTs. In this article, we propose a doubly-robust estimator for a\nvariety of estimands that simultaneously handles missing outcomes under a\nmissing-at-random assumption, missing covariates with the missing-indicator\nmethod (with no constraint on missing covariate distributions), and missing\ncluster-population sizes via a uniform sampling framework. Furthermore, we\nprovide three approaches to improve precision by choosing the optimal weights\nfor intracluster correlation, leveraging machine learning, and modeling the\npropensity score for treatment assignment. To evaluate the impact of violated\nmissing data assumptions, we additionally propose a sensitivity analysis that\nmeasures when missing data alter the conclusion of treatment effect estimation.\nSimulation studies and data applications both show that our proposed method is\nvalid and superior to the existing methods.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11327", "title": "Measuring hierarchically-organized interactions in dynamic networks\n  through spectral entropy rates: theory, estimation, and illustrative\n  application to physiological networks", "abstract": "Recent advances in signal processing and information theory are boosting the\ndevelopment of new approaches for the data-driven modelling of complex network\nsystems. In the fields of Network Physiology and Network Neuroscience where the\nsignals of interest are often rich of oscillatory content, the spectral\nrepresentation of network systems is essential to ascribe the analyzed\ninteractions to specific oscillations with physiological meaning. In this\ncontext, the present work formalizes a coherent framework which integrates\nseveral information dynamics approaches to quantify node-specific, pairwise and\nhigher-order interactions in network systems. The framework establishes a\nhierarchical organization of interactions of different order using measures of\nentropy rate, mutual information rate and O-information rate, to quantify\nrespectively the dynamics of individual nodes, the links between pairs of\nnodes, and the redundant/synergistic hyperlinks between groups of nodes. All\nmeasures are formulated in the time domain, and then expanded to the spectral\ndomain to obtain frequency-specific information. The practical computation of\nall measures is favored presenting a toolbox that implements their parametric\nand non-parametric estimation, and includes approaches to assess their\nstatistical significance. The framework is illustrated first using theoretical\nexamples where the properties of the measures are displayed in benchmark\nsimulated network systems, and then applied to representative examples of\nmultivariate time series in the context of Network Neuroscience and Network\nPhysiology.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11332", "title": "Mortality Modelling using Generalized Estimating Equations", "abstract": "This paper presents an application of Generalized Estimating Equations (GEE)\nfor analyzing age-specific death rates (ASDRs), constituting a longitudinal\ndataset with repeated measurements over time. GEE models, known for their\nrobustness in handling correlated data, offer a reliable solution when\nindividual data records lack independence, thus violating the commonly assumed\nindependence and identically distributed (iid) condition in traditional models.\nIn the context of ASDRs, where correlations emerge among observations within\nage groups, two distinct GEE models for single and multipopulation ASDRs are\nintroduced, providing robust estimates for regression parameters and their\nvariances. We explore correlation structures, encompassing independence, AR(1),\nunstructured, and exchangeable structures, offering a comprehensive evaluation\nof GEE model efficiency in both single and multipopulation ASDRs. We critically\nexamines the strengths and limitations of GEE models, shedding light on their\napplicability for mortality forecasting. Through detailed model specifications\nand empirical illustrations, the study contributes to an enhanced understanding\nof the nuanced capabilities of GEE models in predicting mortality rates.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.11346", "title": "Estimating Default Probability and Correlation using Stan", "abstract": "This work has the objective of estimating default probabilities and\ncorrelations of credit portfolios given default rate information through a\nBayesian framework using Stan. We use Vasicek's single factor credit model to\nestablish the theoretical framework for the behavior of the default rates, and\nuse NUTS Markov Chain Monte Carlo to estimate the parameters. We compare the\nBayesian estimates with classical estimates such as moments estimators and\nmaximum likelihood estimates. We apply the methodology both to simulated data\nand to corporate default rates, and perform inferences through Bayesian methods\nin order to exhibit the advantages of such a framework. We perform default\nforecasting and exhibit the importance of an adequate estimation of default\ncorrelations, and exhibit the advantage of using Stan to perform sampling\nregarding prior choice.", "field": "Statistics", "categories": "stat.AP,stat.ME"}, {"arxiv_id": "2401.11352", "title": "Geometric Insights and Empirical Observations on Covariate Adjustment\n  and Stratified Randomization in Randomized Clinical Trials", "abstract": "The statistical efficiency of randomized clinical trials can be improved by\nincorporating information from baseline covariates (i.e., pre-treatment patient\ncharacteristics). This can be done in the design stage using a\ncovariate-adaptive randomization scheme such as stratified (permutated block)\nrandomization, or in the analysis stage through covariate adjustment. This\narticle provides a geometric perspective on covariate adjustment and stratified\nrandomization in a unified framework where all regular, asymptotically linear\nestimators are identified as augmented estimators. From this perspective,\ncovariate adjustment can be viewed as an effort to approximate the optimal\naugmentation function, and stratified randomization aims to improve a given\napproximation by projecting it into an affine subspace containing the optimal\naugmentation function. The efficiency benefit of stratified randomization is\nasymptotically equivalent to making full use of stratum information in\ncovariate adjustment, which can be achieved using a simple calibration\nprocedure. Simulation results indicate that stratified randomization is clearly\nbeneficial to unadjusted estimators and much less so to adjusted ones and that\ncalibration is an effective way to recover the efficiency benefit of stratified\nrandomization without actually performing stratified randomization. These\ninsights and observations are illustrated using real clinical trial data.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11359", "title": "The Exact Risks of Reference Panel-based Regularized Estimators", "abstract": "Reference panel-based estimators have become widely used in genetic\nprediction of complex traits due to their ability to address data privacy\nconcerns and reduce computational and communication costs. These estimators\nestimate the covariance matrix of predictors using an external reference panel,\ninstead of relying solely on the original training data. In this paper, we\ninvestigate the performance of reference panel-based $L_1$ and $L_2$\nregularized estimators within a unified framework based on approximate message\npassing (AMP). We uncover several key factors that influence the accuracy of\nreference panel-based estimators, including the sample sizes of the training\ndata and reference panels, the signal-to-noise ratio, the underlying sparsity\nof the signal, and the covariance matrix among predictors. Our findings reveal\nthat, even when the sample size of the reference panel matches that of the\ntraining data, reference panel-based estimators tend to exhibit lower accuracy\ncompared to traditional regularized estimators. Furthermore, we observe that\nthis performance gap widens as the amount of training data increases,\nhighlighting the importance of constructing large-scale reference panels to\nmitigate this issue. To support our theoretical analysis, we develop a novel\nnon-separable matrix AMP framework capable of handling the complexities\nintroduced by a general covariance matrix and the additional randomness\nassociated with a reference panel. We validate our theoretical results through\nextensive simulation studies and real data analyses using the UK Biobank\ndatabase.", "field": "Statistics", "categories": "stat.ME,math.ST,stat.TH"}, {"arxiv_id": "2401.11368", "title": "When exposure affects subgroup membership: Framing relevant causal\n  questions in perinatal epidemiology and beyond", "abstract": "Perinatal epidemiology often aims to evaluate exposures on infant outcomes.\nWhen the exposure affects the composition of people who give birth to live\ninfants (e.g., by affecting fertility, behavior, or birth outcomes), this \"live\nbirth process\" mediates the exposure effect on infant outcomes. Causal\nestimands previously proposed for this setting include the total exposure\neffect on composite birth and infant outcomes, controlled direct effects (e.g.,\nenforcing birth), and principal stratum direct effects. Using perinatal HIV\ntransmission in the SEARCH Study as a motivating example, we present two\nalternative causal estimands: 1) conditional total effects; and 2) conditional\nstochastic direct effects, formulated under a hypothetical intervention to draw\nmediator values from some distribution (possibly conditional on covariates).\nThe proposed conditional total effect includes impacts of an intervention that\noperate by changing the types of people who have a live birth and the timing of\nbirths. The proposed conditional stochastic direct effects isolate the effect\nof an exposure on infant outcomes excluding any impacts through this live birth\nprocess. In SEARCH, this approach quantifies the impact of a universal testing\nand treatment intervention on infant HIV-free survival absent any effect of the\nintervention on the live birth process, within a clearly defined target\npopulation of women of reproductive age with HIV at study baseline. Our\napproach has implications for the evaluation of intervention effects in\nperinatal epidemiology broadly, and whenever causal effects within a subgroup\nare of interest and exposure affects membership in the subgroup.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11507", "title": "Redundant multiple testing corrections: The fallacy of using\n  family-based error rates to make inferences about individual hypotheses", "abstract": "During multiple testing, researchers often adjust their alpha level to\ncontrol the familywise error rate for a statistical inference about a joint\nunion alternative hypothesis (e.g., \"H1 or H2\"). However, in some cases, they\ndo not make this inference and instead make separate inferences about each of\nthe individual hypotheses that comprise the joint hypothesis (e.g., H1 and H2).\nFor example, a researcher might use a Bonferroni correction to adjust their\nalpha level from the conventional level of 0.050 to 0.025 when testing H1 and\nH2, find a significant result for H1 (p < 0.025) and not for H2 (p > .0.025),\nand so claim support for H1 and not for H2. However, these separate individual\ninferences do not require an alpha adjustment. Only a statistical inference\nabout the union alternative hypothesis \"H1 or H2\" requires an alpha adjustment\nbecause it is based on \"at least one\" significant result among the two tests,\nand so it depends on the familywise error rate. When a researcher corrects\ntheir alpha level during multiple testing but does not make an inference about\nthe union alternative hypothesis, their correction is redundant. In the present\narticle, I discuss this redundant correction problem, including its associated\nloss of statistical power and its potential causes vis-\\`a-vis error rate\nconfusions and the alpha adjustment ritual. I also provide three illustrations\nof redundant corrections from recent psychology studies. I conclude that\nredundant corrections represent a symptom of statisticism, and I call for a\nmore nuanced and context-specific approach to multiple testing corrections.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11515", "title": "Geometry-driven Bayesian Inference for Ultrametric Covariance Matrices", "abstract": "Ultrametric matrices arise as covariance matrices in latent tree models for\nmultivariate data with hierarchically correlated components. As a parameter\nspace in a model, the set of ultrametric matrices is neither convex nor a\nsmooth manifold, and focus in literature has hitherto mainly been restricted to\nestimation through projections and relaxation-based techniques. Leveraging the\nlink between an ultrametric matrix and a rooted tree, we equip the set of\nultrametric matrices with a convenient geometry based on the well-known\ngeometry of phylogenetic trees, whose attractive properties (e.g. unique\ngeodesics and Fr\\'{e}chet means) the set of ultrametric matrices inherits. This\nresults in a novel representation of an ultrametric matrix by coordinates of\nthe tree space, which we then use to define a class of Markovian and consistent\nprior distributions on the set of ultrametric matrices in a Bayesian model, and\ndevelop an efficient algorithm to sample from the posterior distribution that\ngenerates updates by making intrinsic local moves along geodesics within the\nset of ultrametric matrices. In simulation studies, our proposed algorithm\nrestores the underlying matrices with posterior samples that recover the tree\ntopology with a high frequency of true topology and generate element-wise\ncredible intervals with a high nominal coverage rate. We use the proposed\nalgorithm on the pre-clinical cancer data to investigate the mechanism\nsimilarity by constructing the underlying treatment tree and identify\ntreatments with high mechanism similarity also target correlated pathways in\nbiological literature.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11537", "title": "Addressing researcher degrees of freedom through minP adjustment", "abstract": "When different researchers study the same research question using the same\ndataset they may obtain different and potentially even conflicting results.\nThis is because there is often substantial flexibility in researchers'\nanalytical choices, an issue also referred to as ''researcher degrees of\nfreedom''. Combined with selective reporting of the smallest p-value or largest\neffect, researcher degrees of freedom may lead to an increased rate of false\npositive and overoptimistic results. In this paper, we address this issue by\nformalizing the multiplicity of analysis strategies as a multiple testing\nproblem. As the test statistics of different analysis strategies are usually\nhighly dependent, a naive approach such as the Bonferroni correction is\ninappropriate because it leads to an unacceptable loss of power. Instead, we\npropose using the ''minP'' adjustment method, which takes potential test\ndependencies into account and approximates the underlying null distribution of\nthe minimal p-value through a permutation-based procedure. This procedure is\nknown to achieve more power than simpler approaches while ensuring a weak\ncontrol of the family-wise error rate. We illustrate our approach for\naddressing researcher degrees of freedom by applying it to a study on the\nimpact of perioperative paO2 on post-operative complications after\nneurosurgery. A total of 48 analysis strategies are considered and adjusted\nusing the minP procedure. This approach allows to selectively report the result\nof the analysis strategy yielding the most convincing evidence, while\ncontrolling the type 1 error -- and thus the risk of publishing false positive\nresults that may not be replicable.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.1154", "title": "A new flexible class of kernel-based tests of independence", "abstract": "Spherical and hyperspherical data are commonly encountered in diverse applied\nresearch domains, underscoring the vital task of assessing independence within\nsuch data structures. In this context, we investigate the properties of test\nstatistics relying on distance correlation measures originally introduced for\nthe energy distance, and generalize the concept to strongly negative definite\nkernel-based distances. An important benefit of employing this method lies in\nits versatility across diverse forms of directional data, enabling the\nexamination of independence among vectors of varying types. The applicability\nof tests is demonstrated on several real datasets.", "field": "Statistics", "categories": "stat.ME,math.ST,stat.TH,62H11, 62H15, 62H20"}, {"arxiv_id": "2401.11548", "title": "Investigation of triangle counts in graphs evolved by uniform clustering\n  attachment", "abstract": "The clustering attachment model introduced in the paper Bagrow and Brockmann\n(2013) may be used as an evolution tool of random networks. We propose a new\nclustering attachment model which can be considered as the limit of the former\nclustering attachment model as model parameter $\\alpha$ tends to zero. We focus\non the study of a total triangle count that is considered in the literature as\nan important characteristic of the network clustering. It is proved that total\ntriangle count tends to infinity a.s. for the proposed model. Our simulation\nstudy is used for the modeling of sequences of triangle counts. It is based on\nthe interpretation of the clustering attachment as a generalized\nP\\'{o}lya-Eggenberger urn model that is introduced here at first time.", "field": "Statistics", "categories": "math.ST,stat.TH"}, {"arxiv_id": "2401.11554", "title": "Transfer Learning under Covariate Shift: Local $k$-Nearest Neighbours\n  Regression with Heavy-Tailed Design", "abstract": "Covariate shift is a common transfer learning scenario where the marginal\ndistributions of input variables vary between source and target data while the\nconditional distribution of the output variable remains consistent. The\nexisting notions describing differences between marginal distributions face\nlimitations in handling scenarios with unbounded support, particularly when the\ntarget distribution has a heavier tail. To overcome these challenges, we\nintroduce a new concept called density ratio exponent to quantify the relative\ndecay rates of marginal distributions' tails under covariate shift.\nFurthermore, we propose the local k-nearest neighbour regressor for transfer\nlearning, which adapts the number of nearest neighbours based on the marginal\nlikelihood of each test sample. From a theoretical perspective, convergence\nrates with and without supervision information on the target domain are\nestablished. Those rates indicate that our estimator achieves faster\nconvergence rates when the density ratio exponent satisfies certain conditions,\nhighlighting the benefits of using density estimation for determining different\nnumbers of nearest neighbours for each test sample. Our contributions enhance\nthe understanding and applicability of transfer learning under covariate shift,\nespecially in scenarios with unbounded support and heavy-tailed distributions.", "field": "Statistics", "categories": "math.ST,stat.TH"}, {"arxiv_id": "2401.11562", "title": "Enhancing selectivity using Wasserstein distance based reweighing", "abstract": "Given two labeled data-sets $\\mathcal{S}$ and $\\mathcal{T}$, we design a\nsimple and efficient greedy algorithm to reweigh the loss function such that\nthe limiting distribution of the neural network weights that result from\ntraining on $\\mathcal{S}$ approaches the limiting distribution that would have\nresulted by training on $\\mathcal{T}$.\n  On the theoretical side, we prove that when the metric entropy of the input\ndata-sets is bounded, our greedy algorithm outputs a close to optimal\nreweighing, i.e., the two invariant distributions of network weights will be\nprovably close in total variation distance. Moreover, the algorithm is simple\nand scalable, and we prove bounds on the efficiency of the algorithm as well.\n  Our algorithm can deliberately introduce distribution shift to perform (soft)\nmulti-criteria optimization. As a motivating application, we train a neural net\nto recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell\nsignaling) which are non-binders to MNK1 (a highly similar protein). We tune\nthe algorithm's parameter so that overall change in holdout loss is negligible,\nbut the selectivity, i.e., the fraction of top 100 MNK2 binders that are MNK1\nnon-binders, increases from 54\\% to 95\\%, as a result of our reweighing. Of the\n43 distinct small molecules predicted to be most selective from the enamine\ncatalog, 2 small molecules were experimentally verified to be selective, i.e.,\nthey reduced the enzyme activity of MNK2 below 50\\% but not MNK1, at 10$\\mu$M\n-- a 5\\% success rate.", "field": "Statistics", "categories": "stat.ML,cs.LG,q-bio.QM"}, {"arxiv_id": "2401.11607", "title": "An Interacting Wasserstein Gradient Flow Strategy to Robust Bayesian\n  Inference", "abstract": "Model Updating is frequently used in Structural Health Monitoring to\ndetermine structures' operating conditions and whether maintenance is required.\nData collected by sensors are used to update the values of some initially\nunknown physics-based model's parameters. Bayesian Inference techniques for\nmodel updating require the assumption of a prior distribution. This choice of\nprior may affect posterior predictions and subsequent decisions on maintenance\nrequirements, specially under the typical case in engineering applications of\nlittle informative data. Therefore, understanding how the choice of prior may\naffect the posterior prediction is of great interest. In this paper, a Robust\nBayesian Inference technique evaluates the optimal and worst-case prior in the\nvicinity of a chosen nominal prior, and their corresponding posteriors. This\ntechnique employs an interacting Wasserstein gradient flow formulation. Two\nnumerical case studies are used to showcase the proposed algorithm: a\ndouble-banana-posterior and a double beam structure. Optimal and worst-case\nprior are modelled by specifying an ambiguity set containing any distribution\nat a statistical distance to the nominal prior, less or equal to the radius.\nExamples show how particles flow from an initial assumed Gaussian distribution\nto the optimal worst-case prior distribution that lies inside the defined\nambiguity set, and the resulting particles from the approximation to the\nposterior. The resulting posteriors may be used to yield the lower and upper\nbounds on subsequent calculations used for decision-making. If the metric used\nfor decision-making is not sensitive to the resulting posteriors, it may be\nassumed that decisions taken are robust to prior uncertainty.", "field": "Statistics", "categories": "stat.CO"}, {"arxiv_id": "2401.11646", "title": "Nonparametric Estimation via Variance-Reduced Sketching", "abstract": "Nonparametric models are of great interest in various scientific and\nengineering disciplines. Classical kernel methods, while numerically robust and\nstatistically sound in low-dimensional settings, become inadequate in\nhigher-dimensional settings due to the curse of dimensionality. In this paper,\nwe introduce a new framework called Variance-Reduced Sketching (VRS),\nspecifically designed to estimate density functions and nonparametric\nregression functions in higher dimensions with a reduced curse of\ndimensionality. Our framework conceptualizes multivariable functions as\ninfinite-size matrices, and facilitates a new sketching technique motivated by\nnumerical linear algebra literature to reduce the variance in estimation\nproblems. We demonstrate the robust numerical performance of VRS through a\nseries of simulated experiments and real-world data applications. Notably, VRS\nshows remarkable improvement over existing neural network estimators and\nclassical kernel methods in numerous density estimation and nonparametric\nregression models. Additionally, we offer theoretical justifications for VRS to\nsupport its ability to deliver nonparametric estimation with a reduced curse of\ndimensionality.", "field": "Statistics", "categories": "stat.ML,cs.LG,cs.NA,math.NA,stat.ME"}, {"arxiv_id": "2401.11665", "title": "Accelerating Approximate Thompson Sampling with Underdamped Langevin\n  Monte Carlo", "abstract": "Approximate Thompson sampling with Langevin Monte Carlo broadens its reach\nfrom Gaussian posterior sampling to encompass more general smooth posteriors.\nHowever, it still encounters scalability issues in high-dimensional problems\nwhen demanding high accuracy. To address this, we propose an approximate\nThompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where\nthe latter is the go-to workhorse for simulations of high-dimensional\nposteriors. Based on the standard smoothness and log-concavity conditions, we\nstudy the accelerated posterior concentration and sampling using a specific\npotential function. This design improves the sample complexity for realizing\nlogarithmic regrets from $\\mathcal{\\tilde O}(d)$ to $\\mathcal{\\tilde\nO}(\\sqrt{d})$. The scalability and robustness of our algorithm are also\nempirically validated through synthetic experiments in high-dimensional bandit\nproblems.", "field": "Statistics", "categories": "stat.ML,cs.AI,cs.LG"}, {"arxiv_id": "2401.11672", "title": "Asymptotic distribution of spiked eigenvalues in the large\n  signal-plus-noise models", "abstract": "Consider large signal-plus-noise data matrices of the form $S + \\Sigma^{1/2}\nX$, where $S$ is a low-rank deterministic signal matrix and the noise\ncovariance matrix $\\Sigma$ can be anisotropic. We establish the asymptotic\njoint distribution of its spiked singular values when the dimensionality and\nsample size are comparably large and the signals are supercritical under\ngeneral assumptions concerning the structure of $(S, \\Sigma)$ and the\ndistribution of the random noise $X$. It turns out that the asymptotic\ndistributions exhibit nonuniversality in the sense of dependence on the\ndistributions of the entries of $X$, which contrasts with what has previously\nbeen established for the spiked sample eigenvalues in the context of spiked\npopulation models. Such a result yields the asymptotic distribution of the\nsample spiked eigenvalues associated with mixture models. We also explore the\napplication of these findings in detecting mean heterogeneity of data matrices.", "field": "Statistics", "categories": "math.ST,stat.TH,60F05, 60E05"}, {"arxiv_id": "2401.11789", "title": "Stein EWMA Control Charts for Count Processes", "abstract": "The monitoring of serially independent or autocorrelated count processes is\nconsidered, having a Poisson or (negative) binomial marginal distribution under\nin-control conditions. Utilizing the corresponding Stein identities,\nexponentially weighted moving-average (EWMA) control charts are constructed,\nwhich can be flexibly adapted to uncover zero inflation, over- or\nunderdispersion. The proposed Stein EWMA charts' performance is investigated by\nsimulations, and their usefulness is demonstrated by a real-world data example\nfrom health surveillance.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11804", "title": "Regression Copulas for Multivariate Responses", "abstract": "We propose a novel distributional regression model for a multivariate\nresponse vector based on a copula process over the covariate space. It uses the\nimplicit copula of a Gaussian multivariate regression, which we call a\n``regression copula''. To allow for large covariate vectors their coefficients\nare regularized using a novel multivariate extension of the horseshoe prior.\nBayesian inference and distributional predictions are evaluated using efficient\nvariational inference methods, allowing application to large datasets. An\nadvantage of the approach is that the marginal distributions of the response\nvector can be estimated separately and accurately, resulting in predictive\ndistributions that are marginally-calibrated. Two substantive applications of\nthe methodology highlight its efficacy in multivariate modeling. The first is\nthe econometric modeling and prediction of half-hourly regional Australian\nelectricity prices. Here, our approach produces more accurate distributional\nforecasts than leading benchmark methods. The second is the evaluation of\nmultivariate posteriors in likelihood-free inference (LFI) of a model for tree\nspecies abundance data, extending a previous univariate regression copula LFI\nmethod. In both applications, we demonstrate that our new approach exhibits a\ndesirable marginal calibration property.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11827", "title": "Flexible Models for Simple Longitudinal Data", "abstract": "We propose a new method for estimating subject-specific mean functions from\nlongitudinal data. We aim to do this in a flexible manner (without restrictive\nassumptions about the shape of the subject-specific mean functions), while\nexploiting similarities in the mean functions between different subjects.\nFunctional principal components analysis fulfils both requirements, and methods\nfor functional principal components analysis have been developed for\nlongitudinal data. However, we find that these existing methods sometimes give\nfitted mean functions which are more complex than needed to provide a good fit\nto the data. We develop a new penalised likelihood approach to flexibly model\nlongitudinal data, with a penalty term to control the balance between fit to\nthe data and smoothness of the subject-specific mean curves. We run simulation\nstudies to demonstrate that the new method substantially improves the quality\nof inference relative to existing methods across a range of examples, and apply\nthe method to data on changes in body composition in adolescent girls.", "field": "Statistics", "categories": "stat.ME,stat.AP"}, {"arxiv_id": "2401.11837", "title": "The NOSTRA model: coherent estimation of infection sources in the case\n  of possible nosocomial transmission", "abstract": "Nosocomial infections have important consequences for patients and hospital\nstaff: they worsen patient outcomes and their management stresses already\noverburdened health systems. Accurate judgements of whether an infection is\nnosocomial helps staff make appropriate choices to protect other patients\nwithin the hospital. Nosocomiality cannot be properly assessed without\nconsidering whether the infected patient came into contact with high risk\npotential infectors within the hospital. We developed a Bayesian model that\nintegrates epidemiological, contact and pathogen genetic data to determine how\nlikely an infection is to be nosocomial and the probability of given infection\ncandidates being the source of the infection.", "field": "Statistics", "categories": "stat.AP,q-bio.QM"}, {"arxiv_id": "2401.11842", "title": "Subgroup analysis methods for time-to-event outcomes in heterogeneous\n  randomized controlled trials", "abstract": "Non-significant randomized control trials can hide subgroups of good\nresponders to experimental drugs, thus hindering subsequent development.\nIdentifying such heterogeneous treatment effects is key for precision medicine\nand many post-hoc analysis methods have been developed for that purpose. While\nseveral benchmarks have been carried out to identify the strengths and\nweaknesses of these methods, notably for binary and continuous endpoints,\nsimilar systematic empirical evaluation of subgroup analysis for time-to-event\nendpoints are lacking. This work aims to fill this gap by evaluating several\nsubgroup analysis algorithms in the context of time-to-event outcomes, by means\nof three different research questions: Is there heterogeneity? What are the\nbiomarkers responsible for such heterogeneity? Who are the good responders to\ntreatment? In this context, we propose a new synthetic and semi-synthetic data\ngeneration process that allows one to explore a wide range of heterogeneity\nscenarios with precise control on the level of heterogeneity. We provide an\nopen source Python package, available on Github, containing our generation\nprocess and our comprehensive benchmark framework. We hope this package will be\nuseful to the research community for future investigations of heterogeneity of\ntreatment effects and subgroup analysis methods benchmarking.", "field": "Statistics", "categories": "stat.ME,stat.AP,stat.ML"}, {"arxiv_id": "2401.11885", "title": "Bootstrap prediction regions for daily curves of electricity demand and\n  price using functional data", "abstract": "The aim of this paper is to compute one-day-ahead prediction regions for\ndaily curves of electricity demand and price. Three model-based procedures to\nconstruct general prediction regions are proposed, all of them using bootstrap\nalgorithms. The first proposed method considers any $L_p$ norm for functional\ndata to measure the distance between curves, the second one is designed to take\ndifferent variabilities along the curve into account, and the third one takes\nadvantage of the notion of depth of a functional data. The regression model\nwith functional response on which our proposed prediction regions are based is\nrather general: it allows to include both endogenous and exogenous functional\nvariables, as well as exogenous scalar variables; in addition, the effect of\nsuch variables on the response one is modeled in a parametric, nonparametric or\nsemi-parametric way. A comparative study is carried out to analyse the\nperformance of these prediction regions for the electricity market of mainland\nSpain, in year 2012. This work extends and complements the methods and results\nin Aneiros et al. (2016) (focused on curve prediction) and Vilar et al. (2018)\n(focused on prediction intervals), which use the same database as here.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.11896", "title": "Comparison of Model Output Statistics and Neural Networks to Postprocess\n  Wind Gusts", "abstract": "Wind gust prediction plays an important role in warning strategies of\nnational meteorological services due to the high impact of its extreme values.\nHowever, forecasting wind gusts is challenging because they are influenced by\nsmall-scale processes and local characteristics. To account for the different\nsources of uncertainty, meteorological centers run ensembles of forecasts and\nderive probabilities of wind gusts exceeding a threshold. These probabilities\noften exhibit systematic errors and require postprocessing. Model Output\nStatistics (MOS) is a common operational postprocessing technique, although\nmore modern methods such as neural network-bases approaches have shown\npromising results in research studies. The transition from research to\noperations requires an exhaustive comparison of both techniques. Taking a first\nstep into this direction, our study presents a comparison of a postprocessing\ntechnique based on linear and logistic regression approaches with different\nneural network methods proposed in the literature to improve wind gust\npredictions, specifically distributional regression networks and Bernstein\nquantile networks. We further contribute to investigating optimal design\nchoices for neural network-based postprocessing methods regarding changes of\nthe numerical model in the training period, the use of persistence predictors,\nand the temporal composition of training datasets. The performance of the\ndifferent techniques is compared in terms of calibration, accuracy, reliability\nand resolution based on case studies of wind gust forecasts from the\noperational weather model of the German weather service and observations from\n170 weather stations.", "field": "Statistics", "categories": "stat.AP,physics.ao-ph"}, {"arxiv_id": "2401.12031", "title": "Multi-objective optimisation using expected quantile improvement for\n  decision making in disease outbreaks", "abstract": "Optimization under uncertainty is important in many applications,\nparticularly to inform policy and decision making in areas such as public\nhealth. A key source of uncertainty arises from the incorporation of\nenvironmental variables as inputs into computational models or simulators. Such\nvariables represent uncontrollable features of the optimization problem and\nreliable decision making must account for the uncertainty they propagate to the\nsimulator outputs. Often, multiple, competing objectives are defined from these\noutputs such that the final optimal decision is a compromise between different\ngoals.\n  Here, we present emulation-based optimization methodology for such problems\nthat extends expected quantile improvement (EQI) to address multi-objective\noptimization. Focusing on the practically important case of two objectives, we\nuse a sequential design strategy to identify the Pareto front of optimal\nsolutions. Uncertainty from the environmental variables is integrated out using\nMonte Carlo samples from the simulator. Interrogation of the expected output\nfrom the simulator is facilitated by use of (Gaussian process) emulators. The\nmethodology is demonstrated on an optimization problem from public health\ninvolving the dispersion of anthrax spores across a spatial terrain.\nEnvironmental variables include meteorological features that impact the\ndispersion, and the methodology identifies the Pareto front even when there is\nconsiderable input uncertainty.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.1219", "title": "Concentration inequalities for the sample correlation coefficient", "abstract": "The sample correlation coefficient $R$ plays an important role in many\nstatistical analyses. We study the moments of $R$ under the bivariate Gaussian\nmodel assumption, provide a novel approximation for its finite sample mean and\nconnect it with known results for the variance. We exploit these approximations\nto present non-asymptotic concentration inequalities for $R$. Finally, we\nillustrate our results in a simulation experiment that further validates the\napproximations presented in this work.", "field": "Statistics", "categories": "math.ST,stat.TH"}, {"arxiv_id": "2401.12195", "title": "Using spatial extreme-value theory with machine learning to model and\n  understand spatially compounding extremes", "abstract": "When extreme weather events affect large areas, their regional to\nsub-continental spatial scale is important for their impacts. We propose a\nnovel methodology that combines spatial extreme-value theory with a machine\nlearning (ML) algorithm to model weather extremes and quantify probabilities\nassociated with the occurrence, intensity and spatial extent of these events.\nThe model is here applied to Western European summertime heat extremes. Using\nnew loss functions adapted to extreme values, we fit a theoretically-motivated\nspatial model to extreme positive temperature anomaly fields from 1959-2022,\nusing the daily 500-hpa geopotential height fields across the Euro-Atlantic\nregion and the local soil moisture as predictors. Our generative model reveals\nthe importance of individual circulation features in determining different\nfacets of heat extremes, thereby enriching our process understanding of them\nfrom a data-driven perspective. The occurrence, intensity, and spatial extent\nof heat extremes are sensitive to the relative position of individual ridges\nand troughs that are part of a large-scale wave pattern. Heat extremes in\nEurope are thus the result of a complex interplay between local and remote\nphysical processes. Our approach is able to extrapolate beyond the range of the\ndata to make risk-related probabilistic statements, and applies more generally\nto other weather extremes. It also offers an attractive alternative to physical\nmodel-based techniques, or to ML approaches that optimise scores focusing on\npredicting well the bulk instead of the tail of the data distribution.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.12216", "title": "Mitigating Covariate Shift in Misspecified Regression with Applications\n  to Reinforcement Learning", "abstract": "A pervasive phenomenon in machine learning applications is distribution\nshift, where training and deployment conditions for a machine learning model\ndiffer. As distribution shift typically results in a degradation in\nperformance, much attention has been devoted to algorithmic interventions that\nmitigate these detrimental effects. In this paper, we study the effect of\ndistribution shift in the presence of model misspecification, specifically\nfocusing on $L_{\\infty}$-misspecified regression and adversarial covariate\nshift, where the regression target remains fixed while the covariate\ndistribution changes arbitrarily. We show that empirical risk minimization, or\nstandard least squares regression, can result in undesirable misspecification\namplification where the error due to misspecification is amplified by the\ndensity ratio between the training and testing distributions. As our main\nresult, we develop a new algorithm -- inspired by robust optimization\ntechniques -- that avoids this undesirable behavior, resulting in no\nmisspecification amplification while still obtaining optimal statistical rates.\nAs applications, we use this regression procedure to obtain new guarantees in\noffline and online reinforcement learning with misspecification and establish\nnew separations between previously studied structural conditions and notions of\ncoverage.", "field": "Statistics", "categories": "stat.ML,cs.LG,math.OC"}]}