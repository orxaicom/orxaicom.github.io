{"embeddings": [[21.41000747680664, 7.295377731323242], [20.85730743408203, 7.414745330810547], [21.866708755493164, 7.057835102081299], [20.727275848388672, 6.293524265289307], [20.441577911376953, 7.619271755218506], [19.602170944213867, 5.452977180480957], [19.923442840576172, 8.37043571472168], [19.044498443603516, 5.615733623504639], [20.510601043701172, 8.26650333404541], [21.930217742919922, 7.57841682434082], [20.050186157226562, 5.962540149688721], [21.320207595825195, 5.959844589233398], [19.900184631347656, 7.299402713775635], [21.49225425720215, 6.391751766204834], [20.969253540039062, 8.00598430633545], [19.40717124938965, 5.073478698730469], [22.073217391967773, 6.5113325119018555], [20.750181198120117, 6.859032154083252], [21.748807907104492, 7.93996524810791], [20.470792770385742, 8.850951194763184], [21.37431526184082, 8.852056503295898], [19.46730613708496, 5.940135955810547], [21.115896224975586, 8.458757400512695], [19.79460906982422, 6.611648082733154], [19.36123275756836, 7.8031907081604]], "keys": ["2401.13758", "2401.1376", "2401.13777", "2401.13787", "2401.1382", "2401.13875", "2401.1388", "2401.13884", "2401.13943", "2401.13948", "2401.13975", "2401.14052", "2401.14094", "2401.14122", "2401.14161", "2401.14186", "2401.14239", "2401.14283", "2401.14294", "2401.14306", "2401.14338", "2401.1434", "2401.14355", "2401.14359", "2401.14393"], "additional_info": [{"arxiv_id": "2401.13758", "title": "Assumptions and Bounds in the Instrumental Variable Model", "abstract": "In this note we give proofs for results relating to the Instrumental Variable\n(IV) model with binary response $Y$ and binary treatment $X$, but with an\ninstrument $Z$ that takes $K$ states that were originally stated in Richardson\n& Robins (2014), \"ACE Bounds; SEMS with Equilibrium Conditions,\"\narXiv:1410.0470.", "field": "Statistics", "categories": "math.ST,cs.AI,stat.TH,62A01 (Primary) 62D20, 62H22 (Secondary)"}, {"arxiv_id": "2401.1376", "title": "Early Detection of Treatments Side Effect: A Sequential Approach", "abstract": "With the emergence and spread of infectious diseases with pandemic potential,\nsuch as COVID- 19, the urgency for vaccine development have led to\nunprecedented compressed and accelerated schedules that shortened the standard\ndevelopment timeline. In a relatively short time, the leading pharmaceutical\ncompanies1, received an Emergency Use Authorization (EUA) for vaccine\\prime s\nen-mass deployment To monitor the potential side effect(s) of the vaccine\nduring the (initial) vaccination campaign, we developed an optimal sequential\ntest that allows for the early detection of potential side effect(s). This test\nemploys a rule to stop the vaccination process once the observed number of side\neffect incidents exceeds a certain (pre-determined) threshold. The optimality\nof the proposed sequential test is justified when compared with the ({\\alpha},\n{\\beta}) optimality of the non-randomized fixed-sample Uniformly Most Powerful\n(UMP) test. In the case of a single side effect, we study the properties of the\nsequential test and derive the exact expressions of the Average Sample Number\n(ASN) curve of the stopping time (and its variance) via the regularized\nincomplete beta function. Additionally, we derive the asymptotic distribution\nof the relative savings in ASN as compared to maximal sample size. Moreover, we\nconstruct the post-test parameter estimate and studied its sampling properties,\nincluding its asymptotic behavior under local-type alternatives. These limiting\nbehavior results are the consistency and asymptotic normality of the post-test\nparameter estimator. We conclude the paper with a small simulation study\nillustrating the asymptotic performance of the point and interval estimation\nand provide a detailed example, based on COVID-19 side effect data (see Beatty\net al. (2021)) of our suggested testing procedure.", "field": "Statistics", "categories": "stat.AP,math.ST,stat.TH,62L10, 62L12"}, {"arxiv_id": "2401.13777", "title": "Revisiting the memoryless property -- testing for the Pareto type I\n  distribution", "abstract": "We propose new goodness-of-fit tests for the Pareto type I distribution.\nThese tests are based on a multiplicative version of the memoryless property\nwhich characterises this distribution. We present the results of a Monte Carlo\npower study demonstrating that the proposed tests are powerful compared to\nexisting tests. As a result of independent interest, we demonstrate that tests\nspecifically developed for the Pareto type I distribution substantially\noutperform tests for exponentiality applied to log-transformed data (since\nPareto type I distributed values can be transformed to exponentiality via a\nsimple log-transformation). Specifically, the newly proposed tests based on the\nmultiplicative memoryless property of the Pareto distribution substantially\noutperform a test based on the memoryless property of the exponential\ndistribution. The practical use of tests is illustrated by testing the\nhypothesis that two sets of observed golfers' earnings (those of the PGA and\nLIV tours) are realised from Pareto distributions.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.13787", "title": "Bayesian Analysis of the Beta Regression Model Subject to Linear\n  Inequality Restrictions with Application", "abstract": "ReRecent studies in machine learning are based on models in which parameters\nor state variables are bounded restricted. These restrictions are from prior\ninformation to ensure the validity of scientific theories or structural\nconsistency based on physical phenomena. The valuable information contained in\nthe restrictions must be considered during the estimation process to improve\nestimation accuracy. Many researchers have focused on linear regression models\nsubject to linear inequality restrictions, but generalized linear models have\nreceived little attention. In this paper, the parameters of beta Bayesian\nregression models subjected to linear inequality restrictions are estimated.\nThe proposed Bayesian restricted estimator, which is demonstrated by simulated\nstudies, outperforms ordinary estimators. Even in the presence of\nmulticollinearity, it outperforms the ridge estimator in terms of the standard\ndeviation and the mean squared error. The results confirm that the proposed\nBayesian restricted estimator makes sparsity in parameter estimating without\nusing the regularization penalty. Finally, a real data set is analyzed by the\nnew proposed Bayesian estimation method.", "field": "Statistics", "categories": "stat.ME,stat.CO,62F15, 62J12"}, {"arxiv_id": "2401.1382", "title": "A Bayesian hierarchical mixture cure modelling framework to utilize\n  multiple survival datasets for long-term survivorship estimates: A case study\n  from previously untreated metastatic melanoma", "abstract": "Time to an event of interest over a lifetime is a central measure of the\nclinical benefit of an intervention used in a health technology assessment\n(HTA). Within the same trial multiple end-points may also be considered. For\nexample, overall and progression-free survival time for different drugs in\noncology studies. A common challenge is when an intervention is only effective\nfor some proportion of the population who are not clinically identifiable.\nTherefore, latent group membership as well as separate survival models for\ngroups identified need to be estimated. However, follow-up in trials may be\nrelatively short leading to substantial censoring. We present a general\nBayesian hierarchical framework that can handle this complexity by exploiting\nthe similarity of cure fractions between end-points; accounting for the\ncorrelation between them and improving the extrapolation beyond the observed\ndata. Assuming exchangeability between cure fractions facilitates the borrowing\nof information between end-points. We show the benefits of using our approach\nwith a motivating example, the CheckMate 067 phase 3 trial consisting of\npatients with metastatic melanoma treated with first line therapy.", "field": "Statistics", "categories": "stat.AP,stat.ME"}, {"arxiv_id": "2401.13875", "title": "Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?", "abstract": "Dense-to-sparse gating mixture of experts (MoE) has recently become an\neffective alternative to a well-known sparse MoE. Rather than fixing the number\nof activated experts as in the latter model, which could limit the\ninvestigation of potential experts, the former model utilizes the temperature\nto control the softmax weight distribution and the sparsity of the MoE during\ntraining in order to stabilize the expert specialization. Nevertheless, while\nthere are previous attempts to theoretically comprehend the sparse MoE, a\ncomprehensive analysis of the dense-to-sparse gating MoE has remained elusive.\nTherefore, we aim to explore the impacts of the dense-to-sparse gate on the\nmaximum likelihood estimation under the Gaussian MoE in this paper. We\ndemonstrate that due to interactions between the temperature and other model\nparameters via some partial differential equations, the convergence rates of\nparameter estimations are slower than any polynomial rates, and could be as\nslow as $\\mathcal{O}(1/\\log(n))$, where $n$ denotes the sample size. To address\nthis issue, we propose using a novel activation dense-to-sparse gate, which\nroutes the output of a linear layer to an activation function before delivering\nthem to the softmax function. By imposing linearly independence conditions on\nthe activation function and its derivatives, we show that the parameter\nestimation rates are significantly improved to polynomial rates.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.1388", "title": "Principal Component Regression to Study the Impact of Economic Factors\n  on Disadvantaged Communities", "abstract": "The Council on Environmental Quality's Climate and Economic Justice Screening\nTool defines \"disadvantaged communities\" (DAC) in the USA, highlighting census\ntracts where benefits of climate and energy investments are not accruing. We\nuse a principal component generalized linear model, which addresses the\nintertwined nature of economic factors, income and employment and model their\nrelationship to DAC status. Our study 1) identifies the most significant income\ngroups and employment industries that impact DAC status, 2) provides the\nprobability of DAC status across census tracts and compares the predictive\naccuracy with widely used machine learning approaches, 3) obtains historical\npredictions of the probability of DAC status, 4) obtains spatial downscaling of\nDAC status across block groups. Our study provides valuable insights for\npolicymakers and stakeholders to develop strategies that promote sustainable\ndevelopment and address inequities in climate and energy investments in the\nUSA.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.13884", "title": "Constant Stepsize Q-learning: Distributional Convergence, Bias and\n  Extrapolation", "abstract": "Stochastic Approximation (SA) is a widely used algorithmic approach in\nvarious fields, including optimization and reinforcement learning (RL). Among\nRL algorithms, Q-learning is particularly popular due to its empirical success.\nIn this paper, we study asynchronous Q-learning with constant stepsize, which\nis commonly used in practice for its fast convergence. By connecting the\nconstant stepsize Q-learning to a time-homogeneous Markov chain, we show the\ndistributional convergence of the iterates in Wasserstein distance and\nestablish its exponential convergence rate. We also establish a Central Limit\nTheory for Q-learning iterates, demonstrating the asymptotic normality of the\naveraged iterates. Moreover, we provide an explicit expansion of the asymptotic\nbias of the averaged iterate in stepsize. Specifically, the bias is\nproportional to the stepsize up to higher-order terms and we provide an\nexplicit expression for the linear coefficient. This precise characterization\nof the bias allows the application of Richardson-Romberg (RR) extrapolation\ntechnique to construct a new estimate that is provably closer to the optimal Q\nfunction. Numerical results corroborate our theoretical finding on the\nimprovement of the RR extrapolation method.", "field": "Statistics", "categories": "stat.ML,cs.LG,math.OC"}, {"arxiv_id": "2401.13943", "title": "Is the age pension in Australia sustainable and fair? Evidence from\n  forecasting the old-age dependency ratio using the Hamilton-Perry model", "abstract": "The age pension aims to assist eligible elderly Australians meet specific age\nand residency criteria in maintaining basic living standards. In designing\nefficient pension systems, government policymakers seek to satisfy the\nexpectations of the overall aging population in Australia. However, the\npopulation's unique demographic characteristics at the state and territory\nlevel are often overlooked due to the lack of available data. We use the\nHamilton-Perry model, which requires minimum input, to model and forecast the\nevolution of age-specific populations at the state level. We also integrate the\nobtained sub-national demographic information to determine sustainable pension\nages up to 2051. We also investigate pension welfare distribution in all states\nand territories to identify disadvantaged residents under the current pension\nsystem. Using the sub-national mortality data for Australia from 1971 to 2021\nobtained from AHMD (2023), we implement the Hamilton-Perry model with the help\nof functional time series forecasting techniques. With forecasts of\nage-specific population sizes for each state and territory, we compute the old\nage dependency ratio to determine the nationwide sustainable pension age.", "field": "Statistics", "categories": "stat.AP,stat.ME,62R10"}, {"arxiv_id": "2401.13948", "title": "Z-estimation system: a modular approach to asymptotic analysis", "abstract": "Asymptotic analysis for related inference problems often involves similar\nsteps and proofs. These intermediate results could be shared across problems if\neach of them is made self-contained and easily identified. However, asymptotic\nanalysis using Taylor expansions is limited for result borrowing because it is\na step-to-step procedural approach. This article introduces EEsy, a modular\nsystem for estimating finite and infinitely dimensional parameters in related\ninference problems. It is based on the infinite-dimensional Z-estimation\ntheorem, Donsker and Glivenko-Cantelli preservation theorems, and weight\ncalibration techniques. This article identifies the systematic nature of these\ntools and consolidates them into one system containing several modules, which\ncan be built, shared, and extended in a modular manner. This change to the\nstructure of method development allows related methods to be developed in\nparallel and complex problems to be solved collaboratively, expediting the\ndevelopment of new analytical methods. This article considers four related\ninference problems -- estimating parameters with random sampling, two-phase\nsampling, auxiliary information incorporation, and model misspecification. We\nillustrate this modular approach by systematically developing 9 parameter\nestimators and 18 variance estimators for the four related inference problems\nregarding semi-parametric additive hazards models. Simulation studies show the\nobtained asymptotic results for these 27 estimators are valid. In the end, I\ndescribe how this system can simplify the use of empirical process theory, a\npowerful but challenging tool to be adopted by the broad community of methods\ndevelopers. I discuss challenges and the extension of this system to other\ninference problems.", "field": "Statistics", "categories": "math.ST,stat.TH,62"}, {"arxiv_id": "2401.13975", "title": "Sparse signal recovery and source localization via covariance learning", "abstract": "In the Multiple Measurements Vector (MMV) model, measurement vectors are\nconnected to unknown, jointly sparse signal vectors through a linear regression\nmodel employing a single known measurement matrix (or dictionary). Typically,\nthe number of atoms (columns of the dictionary) is greater than the number\nmeasurements and the sparse signal recovery problem is generally ill-posed. In\nthis paper, we treat the signals and measurement noise as independent Gaussian\nrandom vectors with unknown signal covariance matrix and noise variance,\nrespectively, and derive fixed point (FP) equation for solving the likelihood\nequation for signal powers, thereby enabling the recovery of the sparse signal\nsupport (sources with non-zero variances). Two practical algorithms, a block\ncoordinate descent (BCD) and a cyclic coordinate descent (CCD) algorithms, that\nleverage on the FP characterization of the likelihood equation are then\nproposed. Additionally, a greedy pursuit method, analogous to popular\nsimultaneous orthogonal matching pursuit (OMP), is introduced. Our numerical\nexamples demonstrate effectiveness of the proposed covariance learning (CL)\nalgorithms both in classic sparse signal recovery as well as in\ndirection-of-arrival (DOA) estimation problems where they perform favourably\ncompared to the state-of-the-art algorithms under a broad variety of settings.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.14052", "title": "Testing Alpha in High Dimensional Linear Factor Pricing Models with\n  Dependent Observations", "abstract": "In this study, we introduce three distinct testing methods for testing alpha\nin high dimensional linear factor pricing model that deals with dependent data.\nThe first method is a sum-type test procedure, which exhibits high performance\nwhen dealing with dense alternatives. The second method is a max-type test\nprocedure, which is particularly effective for sparse alternatives. For a\nbroader range of alternatives, we suggest a Cauchy combination test procedure.\nThis is predicated on the asymptotic independence of the sum-type and max-type\ntest statistics. Both simulation studies and practical data application\ndemonstrate the effectiveness of our proposed methods when handling dependent\nobservations.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.14094", "title": "ODC and ROC curves, comparison curves, and stochastic dominance", "abstract": "We discuss two novel approaches to the classical two-sample problem. Our\nstarting point are properly standardized and combined, very popular in several\nareas of statistics and data analysis, ordinal dominance and receiver\ncharacteristic curves, denoted by ODC and ROC, respectively. The proposed new\ncurves are termed the comparison curves. Their estimates, being weighted rank\nprocesses on (0,1), form the basis of inference. These weighted processes are\nintuitive, well-suited for visual inspection of data at hand, and are also\nuseful for constructing some formal inferential procedures. They can be applied\nto several variants of two-sample problem. Their use can help to improve some\nexisting procedures both in terms of power and the ability to identify the\nsources of departures from the postulated model. To simplify interpretation of\nfinite sample results we restrict attention to values of the processes on a\nfinite grid of points. This results in the so-called bar plots (B-plots) which\nreadably summarize the information contained in the data. What is more, we show\nthat B-plots along with adjusted simultaneous acceptance regions provide\nprincipled information about where the model departs from the data. This leads\nto a framework which facilitates identification of regions with locally\nsignificant differences.\n  We show an implementation of the considered techniques to a standard\nstochastic dominance testing problem. Some min-type statistics are introduced\nand investigated. A simulation study compares two tests pertinent to the\ncomparison curves to well-established tests in the literature and demonstrates\nthe strong and competitive performance of the former in many typical\nsituations. Some real data applications illustrate simplicity and practical\nusefulness of the proposed approaches. A range of other applications of\nconsidered weighted processes is briefly discussed too.", "field": "Statistics", "categories": "stat.ME,math.ST,stat.TH,62A09 (Primary) 62G10, 60E15 (Secondary)"}, {"arxiv_id": "2401.14122", "title": "On a Novel Skewed Generalized t Distribution: Properties, Estimations\n  and its Applications", "abstract": "With the progress of information technology, large amounts of asymmetric,\nleptokurtic and heavy-tailed data are arising in various fields, such as\nfinance, engineering, genetics and medicine. It is very challenging to model\nthose kinds of data, especially for extremely skewed data, accompanied by very\nhigh kurtosis or heavy tails. In this paper, we propose a class of novel skewed\ngeneralized t distribution (SkeGTD) as a scale mixture of skewed generalized\nnormal. The proposed SkeGTD has excellent adaptiveness to various data, because\nof its capability of allowing for a large range of skewness and kurtosis and\nits compatibility of the separated location, scale, skewness and shape\nparameters. We investigate some important properties of this family of\ndistributions. The maximum likelihood estimation, L-moments estimation and\ntwo-step estimation for the SkeGTD are explored. To illustrate the usefulness\nof the proposed methodology, we present simulation studies and analyze two real\ndatasets.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.14161", "title": "Adapting tree-based multiple imputation methods for multi-level data? A\n  simulation study", "abstract": "This simulation study evaluates the effectiveness of multiple imputation (MI)\ntechniques for multilevel data. It compares the performance of traditional\nMultiple Imputation by Chained Equations (MICE) with tree-based methods such as\nChained Random Forests with Predictive Mean Matching and Extreme Gradient\nBoosting. Adapted versions that include dummy variables for cluster membership\nare also included for the tree-based methods. Methods are evaluated for\ncoefficient estimation bias, statistical power, and type I error rates on\nsimulated hierarchical data with different cluster sizes (25 and 50) and levels\nof missingness (10\\% and 50\\%). Coefficients are estimated using random\nintercept and random slope models. The results show that while MICE is\npreferred for accurate rejection rates, Extreme Gradient Boosting is\nadvantageous for reducing bias. Furthermore, the study finds that bias levels\nare similar across different cluster sizes, but rejection rates tend to be less\nfavorable with fewer clusters (lower power, higher type I error). In addition,\nthe inclusion of cluster dummies in tree-based methods improves estimation for\nLevel 1 variables, but is less effective for Level 2 variables. When data\nbecome too complex and MICE is too slow, extreme gradient boosting is a good\nalternative for hierarchical data.\n  Keywords: Multiple imputation; multi-level data; MICE; missRanger; mixgb", "field": "Statistics", "categories": "stat.AP,stat.ML"}, {"arxiv_id": "2401.14186", "title": "Graph-accelerated Markov Chain Monte Carlo using Approximate Samples", "abstract": "In recent years, it has become increasingly easy to obtain approximate\nposterior samples via efficient computation algorithms, such as those in\nvariational Bayes. On the other hand, concerns exist on the accuracy of\nuncertainty estimates, which make it tempting to consider exploiting the\napproximate samples in canonical Markov chain Monte Carlo algorithms. A major\ntechnical barrier is that the approximate sample, when used as a proposal in\nMetropolis-Hastings steps, tends to have a low acceptance rate as the dimension\nincreases. In this article, we propose a simple yet general solution named\n''graph-accelerated Markov Chain Monte Carlo''. We first build a graph with\neach node location assigned to an approximate sample, then we run Markov chain\nMonte Carlo with random walks over the graph. In the first stage, we optimize\nthe choice of graph edges to enforce small differences in posterior\ndensity/probability between neighboring nodes, while encouraging edges to\ncorrespond to large distances in the parameter space. This optimized graph\nallows us to accelerate a canonical Markov transition kernel through mixing\nwith a large-jump Metropolis-Hastings step, when collecting Markov chain\nsamples at the second stage. Due to its simplicity, this acceleration can be\napplied to most of the existing Markov chain Monte Carlo algorithms. We\ntheoretically quantify the rate of acceptance as dimension increases, and show\nthe effects on improved mixing time. We demonstrate our approach through\nimproved mixing performances for challenging sampling problems, such as those\ninvolving multiple modes, non-convex density contour, or large-dimension latent\nvariables.", "field": "Statistics", "categories": "stat.CO"}, {"arxiv_id": "2401.14239", "title": "spINAR: An R Package for Semiparametric and Parametric Estimation and\n  Bootstrapping of Integer-Valued Autoregressive (INAR) Models", "abstract": "Although the statistical literature extensively covers continuous-valued time\nseries processes and their parametric, non-parametric and semiparametric\nestimation, the literature on count data time series is considerably less\nadvanced. Among the count data time series models, the integer-valued\nautoregressive (INAR) model is arguably the most popular one finding\napplications in a wide variety of fields such as medical sciences,\nenvironmentology and economics. While many contributions have been made during\nthe last decades, the majority of the literature focuses on parametric INAR\nmodels and estimation techniques. Our emphasis is on the complex but efficient\nand non-restrictive semiparametric estimation of INAR models. The appeal of\nthis approach lies in the absence of a commitment to a parametric family of\ninnovation distributions. In this paper, we describe the need and the features\nof our R package spINAR which combines semiparametric simulation, estimation\nand bootstrapping of INAR models also covering its parametric versions.", "field": "Statistics", "categories": "stat.CO"}, {"arxiv_id": "2401.14283", "title": "Information Leakage Detection through Approximate Bayes-optimal\n  Prediction", "abstract": "In today's data-driven world, the proliferation of publicly available\ninformation intensifies the challenge of information leakage (IL), raising\nsecurity concerns. IL involves unintentionally exposing secret (sensitive)\ninformation to unauthorized parties via systems' observable information.\nConventional statistical approaches, which estimate mutual information (MI)\nbetween observable and secret information for detecting IL, face challenges\nsuch as the curse of dimensionality, convergence, computational complexity, and\nMI misestimation. Furthermore, emerging supervised machine learning (ML)\nmethods, though effective, are limited to binary system-sensitive information\nand lack a comprehensive theoretical framework. To address these limitations,\nwe establish a theoretical framework using statistical learning theory and\ninformation theory to accurately quantify and detect IL. We demonstrate that MI\ncan be accurately estimated by approximating the log-loss and accuracy of the\nBayes predictor. As the Bayes predictor is typically unknown in practice, we\npropose to approximate it with the help of automated machine learning (AutoML).\nFirst, we compare our MI estimation approaches against current baselines, using\nsynthetic data sets generated using the multivariate normal (MVN) distribution\nwith known MI. Second, we introduce a cut-off technique using one-sided\nstatistical tests to detect IL, employing the Holm-Bonferroni correction to\nincrease confidence in detection decisions. Our study evaluates IL detection\nperformance on real-world data sets, highlighting the effectiveness of the\nBayes predictor's log-loss estimation, and finds our proposed method to\neffectively estimate MI on synthetic data sets and thus detect ILs accurately.", "field": "Statistics", "categories": "stat.ML,cs.LG,94A15, 62H30, 94A60,I.5.1; G.3; E.3"}, {"arxiv_id": "2401.14294", "title": "Heteroscedasticity-aware stratified sampling to improve uplift modeling", "abstract": "In many business applications, including online marketing and customer churn\nprevention, randomized controlled trials (RCT's) are conducted to investigate\non the effect of specific treatment (coupon offers, advertisement\nmailings,...). Such RCT's allow for the estimation of average treatment effects\nas well as the training of (uplift) models for the heterogeneity of treatment\neffects between individuals. The problem with these RCT's is that they are\ncostly and this cost increases with the number of individuals included into the\nRCT. For this reason, there is research how to conduct experiments involving a\nsmall number of individuals while still obtaining precise treatment effect\nestimates. We contribute to this literature a heteroskedasticity-aware\nstratified sampling (HS) scheme, which leverages the fact that different\nindividuals have different noise levels in their outcome and precise treatment\neffect estimation requires more observations from the \"high-noise\" individuals\nthan from the \"low-noise\" individuals. By theory as well as by empirical\nexperiments, we demonstrate that our HS-sampling yields significantly more\nprecise estimates of the ATE, improves uplift models and makes their evaluation\nmore reliable compared to RCT data sampled completely randomly. Due to the\nrelative ease of application and the significant benefits, we expect\nHS-sampling to be valuable in many real-world applications.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.14306", "title": "Using Geographically Weighted Models to Explore Temporal and Spatial\n  Varying Impacts on Commute Trip Change Due to Covid-19", "abstract": "COVID-19 has deeply affected daily life and travel behaviors. Understanding\nthese changes is crucial, prompting an investigation into socio-demographic and\nsocio-economic factors. This study used large-scale mobile device location data\nin Washington, D.C., Maryland, and Virginia (DMV area) to unveil the impacts of\nthese variables on commute trip changes. It reflected short and long-term\nimpacts through linear regression and geographically weighted regression\nmodels. Findings indicated that counties with a higher percentage of people\nusing walking and biking during the initial phase of COVID-19 experienced\ngreater reductions in commute trips. For the long-term effect in November, the\nimpact of active modes became insignificant, and individuals using public modes\nshowed more significant trip reductions. Positive correlations were observed\nbetween median income levels and reduced commute trips. Sectors requiring\nongoing outdoor operations during the pandemic showed substantial negative\ncorrelations. In the DMV area, counties with a higher proportion of Democratic\nvoters experienced less trip reduction. Applying Geographically Weighted\nRegression models captured local spatial relationships, showing the emergence\nof local correlations as the pandemic evolved, suggesting a geographical impact\npattern. Initially global, the pandemic's impact on commuting behaviors became\nmore influenced by spatial factors over time, showing localized effects.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.14338", "title": "Case-crossover designs and overdispersion with application in air\n  pollution epidemiology", "abstract": "Over the last three decades, case-crossover designs have found many\napplications in health sciences, especially in air pollution epidemiology. They\nare typically used, in combination with partial likelihood techniques, to\ndefine a conditional logistic model for the responses, usually health outcomes,\nconditional on the exposures. Despite the fact that conditional logistic models\nhave been shown equivalent, in typical air pollution epidemiology setups, to\nspecific instances of the well-known Poisson time series model, it is often\nclaimed that they cannot allow for overdispersion. This paper clarifies the\nrelationship between case-crossover designs, the models that ensue from their\nuse, and overdispersion. In particular, we propose to relax the assumption of\nindependence between individuals traditionally made in case-crossover analyses,\nin order to explicitly introduce overdispersion in the conditional logistic\nmodel. As we show, the resulting overdispersed conditional logistic model\ncoincides with the overdispersed, conditional Poisson model, in the sense that\ntheir likelihoods are simple re-expressions of one another. We further provide\nthe technical details of a Bayesian implementation of the proposed\ncase-crossover model, which we use to demonstrate, by means of a large\nsimulation study, that standard case-crossover models can lead to dramatically\nunderestimated coverage probabilities, while the proposed models do not. We\nalso perform an illustrative analysis of the association between air pollution\nand morbidity in Toronto, Canada, which shows that the proposed models are more\nrobust than standard ones to outliers such as those associated with public\nholidays.", "field": "Statistics", "categories": "stat.ME,62J12, 62F15, 62P10,G.3"}, {"arxiv_id": "2401.1434", "title": "Estimation of partially known Gaussian graphical models with score-based\n  structural priors", "abstract": "We propose a novel algorithm for the support estimation of partially known\nGaussian graphical models that incorporates prior information about the\nunderlying graph. In contrast to classical approaches that provide a point\nestimate based on a maximum likelihood or a maximum a posteriori criterion\nusing (simple) priors on the precision matrix, we consider a prior on the graph\nand rely on annealed Langevin diffusion to generate samples from the posterior\ndistribution. Since the Langevin sampler requires access to the score function\nof the underlying graph prior, we use graph neural networks to effectively\nestimate the score from a graph dataset (either available beforehand or\ngenerated from a known distribution). Numerical experiments demonstrate the\nbenefits of our approach.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.14355", "title": "Multiply Robust Estimation of Causal Effect Curves for\n  Difference-in-Differences Designs", "abstract": "Researchers commonly use difference-in-differences (DiD) designs to evaluate\npublic policy interventions. While established methodologies exist for\nestimating effects in the context of binary interventions, policies often\nresult in varied exposures across regions implementing the policy. Yet,\nexisting approaches for incorporating continuous exposures face substantial\nlimitations in addressing confounding variables associated with intervention\nstatus, exposure levels, and outcome trends. These limitations significantly\nconstrain policymakers' ability to fully comprehend policy impacts and design\nfuture interventions. In this study, we propose innovative estimators for\ncausal effect curves within the DiD framework, accounting for multiple sources\nof confounding. Our approach accommodates misspecification of a subset of\ntreatment, exposure, and outcome models while avoiding any parametric\nassumptions on the effect curve. We present the statistical properties of the\nproposed methods and illustrate their application through simulations and a\nstudy investigating the diverse effects of a nutritional excise tax.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.14359", "title": "Minimum Covariance Determinant: Spectral Embedding and Subset Size\n  Determination", "abstract": "This paper introduces several ideas to the minimum covariance determinant\nproblem for outlier detection and robust estimation of means and covariances.\nWe leverage the principal component transform to achieve dimension reduction,\npaving the way for improved analyses. Our best subset selection algorithm\nstrategically combines statistical depth and concentration steps. To ascertain\nthe appropriate subset size and number of principal components, we introduce a\nnovel bootstrap procedure that estimates the instability of the best subset\nalgorithm. The parameter combination exhibiting minimal instability proves\nideal for the purposes of outlier detection and robust estimation. Rigorous\nbenchmarking against prominent MCD variants showcases our approach's superior\ncapability in outlier detection and computational speed in high dimensions.\nApplication to a fruit spectra data set and a cancer genomics data set\nillustrates our claims.", "field": "Statistics", "categories": "stat.ME,stat.CO"}, {"arxiv_id": "2401.14393", "title": "Clustering-based spatial interpolation of parametric post-processing\n  models", "abstract": "Since the start of the operational use of ensemble prediction systems,\nensemble-based probabilistic forecasting has become the most advanced approach\nin weather prediction. However, despite the persistent development of the last\nthree decades, ensemble forecasts still often suffer from the lack of\ncalibration and might exhibit systematic bias, which calls for some form of\nstatistical post-processing. Nowadays, one can choose from a large variety of\npost-processing approaches, where parametric methods provide full predictive\ndistributions of the investigated weather quantity. Parameter estimation in\nthese models is based on training data consisting of past forecast-observation\npairs, thus post-processed forecasts are usually available only at those\nlocations where training data are accessible. We propose a general\nclustering-based interpolation technique of extending calibrated predictive\ndistributions from observation stations to any location in the ensemble domain\nwhere there are ensemble forecasts at hand. Focusing on the ensemble model\noutput statistics (EMOS) post-processing technique, in a case study based on\nwind speed ensemble forecasts of the European Centre for Medium-Range Weather\nForecasts, we demonstrate the predictive performance of various versions of the\nsuggested method and show its superiority over the regionally estimated and\ninterpolated EMOS models and the raw ensemble forecasts as well.", "field": "Statistics", "categories": "stat.AP,stat.ME"}]}