{"embeddings": [[4.4839768409729, 18.386016845703125], [3.7679049968719482, 18.433799743652344], [3.8320493698120117, 17.67044448852539], [3.2150769233703613, 17.620708465576172], [4.609272003173828, 19.18505096435547], [5.79130220413208, 18.16644287109375], [3.3220200538635254, 18.868772506713867], [3.9695165157318115, 19.295516967773438], [5.386803150177002, 18.892742156982422], [4.575499534606934, 17.90143394470215], [4.339300632476807, 17.204347610473633], [2.9684102535247803, 18.309728622436523], [3.7974743843078613, 16.8064022064209], [5.608522891998291, 17.485515594482422], [5.203926086425781, 18.36507225036621], [4.625824928283691, 16.751995086669922], [5.003507137298584, 17.491201400756836]], "keys": ["2401.09435", "2401.09559", "2401.09602", "2401.0966", "2401.09696", "2401.09715", "2401.09719", "2401.09816", "2401.09874", "2401.09979", "2401.09994", "2401.1001", "2401.10057", "2401.10124", "2401.1018", "2401.10193", "2401.10196"], "additional_info": [{"arxiv_id": "2401.09435", "title": "Reasoning with random sets: An agenda for the future", "abstract": "In this paper, we discuss a potential agenda for future work in the theory of\nrandom sets and belief functions, touching upon a number of focal issues: the\ndevelopment of a fully-fledged theory of statistical reasoning with random\nsets, including the generalisation of logistic regression and of the classical\nlaws of probability; the further development of the geometric approach to\nuncertainty, to include general random sets, a wider range of uncertainty\nmeasures and alternative geometric representations; the application of this new\ntheory to high-impact areas such as climate change, machine learning and\nstatistical learning theory.", "field": "Statistics", "categories": "math.ST,cs.AI,stat.ML,stat.TH,62A01, 62A86, 60A05, 60C05, 60D05, 60E05"}, {"arxiv_id": "2401.09559", "title": "Asymptotic Online FWER Control for Dependent Test Statistics", "abstract": "In online multiple testing, an a priori unknown number of hypotheses are\ntested sequentially, i.e. at each time point a test decision for the current\nhypothesis has to be made using only the data available so far. Although many\npowerful test procedures have been developed for online error control in recent\nyears, most of them are designed solely for independent or at most locally\ndependent test statistics. In this work, we provide a new framework for\nderiving online multiple test procedures which ensure asymptotical (with\nrespect to the sample size) control of the familywise error rate (FWER),\nregardless of the dependence structure between test statistics. In this\ncontext, we give a few concrete examples of such test procedures and discuss\ntheir properties. Furthermore, we conduct a simulation study in which the type\nI error control of these test procedures is also confirmed for a finite sample\nsize and a gain in power is indicated.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.09602", "title": "Evaluating tree-based imputation methods as an alternative to MICE PMM\n  for drawing inference in empirical studies", "abstract": "Dealing with missing data is an important problem in statistical analysis\nthat is often addressed with imputation procedures. The performance and\nvalidity of such methods are of great importance for their application in\nempirical studies. While the prevailing method of Multiple Imputation by\nChained Equations (MICE) with Predictive Mean Matching (PMM) is considered\nstandard in the social science literature, the increase in complex datasets may\nrequire more advanced approaches based on machine learning. In particular,\ntree-based imputation methods have emerged as very competitive approaches.\nHowever, the performance and validity are not completely understood,\nparticularly compared to the standard MICE PMM. This is especially true for\ninference in linear models. In this study, we investigate the impact of various\nimputation methods on coefficient estimation, Type I error, and power, to gain\ninsights that can help empirical researchers deal with missingness more\neffectively. We explore MICE PMM alongside different tree-based methods, such\nas MICE with Random Forest (RF), Chained Random Forests with and without PMM\n(missRanger), and Extreme Gradient Boosting (MIXGBoost), conducting a realistic\nsimulation study using the German National Educational Panel Study (NEPS) as\nthe original data source. Our results reveal that Random Forest-based\nimputations, especially MICE RF and missRanger with PMM, consistently perform\nbetter in most scenarios. Standard MICE PMM shows partially increased bias and\noverly conservative test decisions, particularly with non-true zero\ncoefficients. Our results thus underscore the potential advantages of\ntree-based imputation methods, albeit with a caveat that all methods perform\nworse with an increased missingness, particularly missRanger.", "field": "Statistics", "categories": "stat.AP,stat.ML"}, {"arxiv_id": "2401.0966", "title": "Data-Driven Assessment of the County-Level Breast Cancer Incidence in\n  the United States: Impacts of Modifiable and Non-Modifiable Factors", "abstract": "Female breast cancer (FBC) incidence rate (IR) varies greatly by counties\nacross the United States (US). Factors responsible for such high spatial\ndisparities are not well understood, making it challenging to design effective\nintervention strategies. We predicted FBC IRs using prevailing machine learning\ntechniques for 1,754 US counties with a female population over 10,000. Outlier\ncounties with the unexpectedly high or low FBC IRs were identified by\ncontrolling the non-modifiable factors (demographics and socioeconomics).\nImpacts of the modifiable factors (lifestyle, healthcare accessibility, and\nenvironment) were mapped. Our study also shed light on hidden FBC risk factors\nat the regional scale. Methods developed in our study may be used to discover\nthe place-specific, population-level, modifiable factors for the intervention\nof other types of cancer or chronic diseases.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.09696", "title": "Rejection Sampling with Vertical Weighted Strips", "abstract": "A number of distributions that arise in statistical applications can be\nexpressed in the form of a weighted density: the product of a base density and\na nonnegative weight function. Generating variates from such a distribution may\nbe nontrivial and can involve an intractable normalizing constant. Rejection\nsampling may be used to generate exact draws, but requires formulation of a\nsuitable proposal distribution. To be practically useful, the proposal must\nboth be convenient to sample from and not reject candidate draws too\nfrequently. A well-known approach to design a proposal involves decomposing the\ntarget density into a finite mixture, whose components may correspond to a\npartition of the support. This work considers such a construction that focuses\non majorization of the weight function. This approach may be applicable when\nassumptions for adaptive rejection sampling and related algorithms are not met.\nAn upper bound for the rejection probability based on this construction can be\nexpressed to evaluate the efficiency of the proposal before sampling. A method\nto partition the support is considered where regions are bifurcated based on\ntheir contribution to the bound. Examples based on the von Mises Fisher\ndistribution and Gaussian Process regression are provided to illustrate the\nmethod.", "field": "Statistics", "categories": "stat.ME,stat.CO"}, {"arxiv_id": "2401.09715", "title": "Fast Variational Inference of Latent Space Models for Dynamic Networks\n  Using Bayesian P-Splines", "abstract": "Latent space models (LSMs) are often used to analyze dynamic (time-varying)\nnetworks that evolve in continuous time. Existing approaches to Bayesian\ninference for these models rely on Markov chain Monte Carlo algorithms, which\ncannot handle modern large-scale networks. To overcome this limitation, we\nintroduce a new prior for continuous-time LSMs based on Bayesian P-splines that\nallows the posterior to adapt to the dimension of the latent space and the\ntemporal variation in each latent position. We propose a stochastic variational\ninference algorithm to estimate the model parameters. We use stochastic\noptimization to subsample both dyads and observed time points to design a fast\nalgorithm that is linear in the number of edges in the dynamic network.\nFurthermore, we establish non-asymptotic error bounds for point estimates\nderived from the variational posterior. To our knowledge, this is the first\nsuch result for Bayesian estimators of continuous-time LSMs. Lastly, we use the\nmethod to analyze a large data set of international conflicts consisting of\n4,456,095 relations from 2018 to 2022.", "field": "Statistics", "categories": "stat.ME,stat.CO"}, {"arxiv_id": "2401.09719", "title": "Kernel-based multi-marker tests of association based on the accelerated\n  failure time model", "abstract": "Kernel-based multi-marker tests for survival outcomes use primarily the Cox\nmodel to adjust for covariates. The proportional hazards assumption made by the\nCox model could be unrealistic, especially in the long-term follow-up. We\ndevelop a suite of novel multi-marker survival tests for genetic association\nbased on the accelerated failure time model, which is a popular alternative to\nthe Cox model due to its direct physical interpretation. The tests are based on\nthe asymptotic distributions of their test statistics and are thus\ncomputationally efficient. The association tests can account for the\nheterogeneity of genetic effects across sub-populations/individuals to increase\nthe power. All the new tests can deal with competing risks and left truncation.\nMoreover, we develop small-sample corrections to the tests to improve their\naccuracy under small samples. Extensive numerical experiments show that the new\ntests perform very well in various scenarios. An application to a genetic\ndataset of Alzheimer's disease illustrates the tests' practical utility.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.09816", "title": "Jackknife empirical likelihood ratio test for testing the equality of\n  semivariance", "abstract": "Semivariance is a measure of the dispersion of all observations that fall\nabove the mean or target value of a random variable and it plays an important\nrole in life-length, actuarial and income studies. In this paper, we develop a\nnew non-parametric test for equality of upper semi-variance. We use the\nU-statistic theory to derive the test statistic and then study the asymptotic\nproperties of the test statistic. We also develop a jackknife empirical\nlikelihood (JEL) ratio test for equality of upper Semivariance. Extensive Monte\nCarlo simulation studies are carried out to validate the performance of the\nproposed JEL-based test. We illustrate the test procedure using real data.", "field": "Statistics", "categories": "stat.ME,62G10"}, {"arxiv_id": "2401.09874", "title": "A Quantile Nelson-Siegel model", "abstract": "A widespread approach to modelling the interaction between macroeconomic\nvariables and the yield curve relies on three latent factors usually\ninterpreted as the level, slope, and curvature (Diebold et al., 2006). This\napproach is inherently focused on the conditional mean of the yields and\npostulates a dynamic linear model where the latent factors smoothly change over\ntime. However, periods of deep crisis, such as the Great Recession and the\nrecent pandemic, have highlighted the importance of statistical models that\naccount for asymmetric shocks and are able to forecast the tails of a\nvariable's distribution. A new version of the dynamic three-factor model is\nproposed to address this issue based on quantile regressions. The novel\napproach leverages the potential of quantile regression to model the entire\n(conditional) distribution of the yields instead of restricting to its mean. An\napplication to US data from the 1970s shows the significant heterogeneity of\nthe interactions between financial and macroeconomic variables across different\nquantiles. Moreover, an out-of-sample forecasting exercise showcases the\nproposed method's advantages in predicting the yield distribution tails\ncompared to the standard conditional mean model. Finally, by inspecting the\nposterior distribution of the three factors during the recent major crises, new\nevidence is found that supports the greater and longer-lasting negative impact\nof the great recession on the yields compared to the COVID-19 pandemic.", "field": "Statistics", "categories": "stat.AP,econ.EM"}, {"arxiv_id": "2401.09979", "title": "False Discovery Rate Control for Gaussian Graphical Models via\n  Neighborhood Screening", "abstract": "Gaussian graphical models emerge in a wide range of fields. They model the\nstatistical relationships between variables as a graph, where an edge between\ntwo variables indicates conditional dependence. Unfortunately, well-established\nestimators, such as the graphical lasso or neighborhood selection, are known to\nbe susceptible to a high prevalence of false edge detections. False detections\nmay encourage inaccurate or even incorrect scientific interpretations, with\nmajor implications in applications, such as biomedicine or healthcare. In this\npaper, we introduce a nodewise variable selection approach to graph learning\nand provably control the false discovery rate of the selected edge set at a\nself-estimated level. A novel fusion method of the individual neighborhoods\noutputs an undirected graph estimate. The proposed method is parameter-free and\ndoes not require tuning by the user. Benchmarks against competing false\ndiscovery rate controlling methods in numerical experiments considering\ndifferent graph topologies show a significant gain in performance.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.09994", "title": "Bayesian modeling of spatial ordinal data from health surveys", "abstract": "Health surveys allow exploring health indicators that are of great value from\na public health point of view and that cannot normally be studied from regular\nhealth registries. These indicators are usually coded as ordinal variables and\nmay depend on covariates associated with individuals. In this paper, we propose\na Bayesian individual-level model for small-area estimation of survey-based\nhealth indicators. A categorical likelihood is used at the first level of the\nmodel hierarchy to describe the ordinal data, and spatial dependence among\nsmall areas is taken into account by using a conditional autoregressive (CAR)\ndistribution. Post-stratification of the results of the proposed\nindividual-level model allows extrapolating the results to any administrative\nareal division, even for small areas. We apply this methodology to the analysis\nof the Health Survey of the Region of Valencia (Spain) of 2016 to describe the\ngeographical distribution of a self-perceived health indicator of interest in\nthis region.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.1001", "title": "A global kernel estimator for partially linear varying coefficient\n  additive hazards models", "abstract": "In biomedical studies, we are often interested in the association between\ndifferent types of covariates and the times to disease events. Because the\nrelationship between the covariates and event times is often complex, standard\nsurvival models that assume a linear covariate effect are inadequate. A\nflexible class of models for capturing complex interaction effects among types\nof covariates is the varying coefficient models, where the effects of a type of\ncovariates can be modified by another type of covariates. In this paper, we\nstudy kernel-based estimation methods for varying coefficient additive hazards\nmodels. Unlike many existing kernel-based methods that use a local neighborhood\nof subjects for the estimation of the varying coefficient function, we propose\na novel global approach that is generally more efficient. We establish\ntheoretical properties of the proposed estimators and demonstrate their\nsuperior performance compared with existing local methods through large-scale\nsimulation studies. To illustrate the proposed method, we provide an\napplication to a motivating cancer genomic study.", "field": "Statistics", "categories": "stat.ME,62N02"}, {"arxiv_id": "2401.10057", "title": "A method for characterizing disease emergence curves from paired\n  pathogen detection and serology data", "abstract": "Wildlife disease surveillance programs and research studies track infection\nand identify risk factors for wild populations, humans, and agriculture. Often,\nseveral types of samples are collected from individuals to provide more\ncomplete information about an animal's infection history. Methods that jointly\nanalyze multiple data streams to study disease emergence and drivers of\ninfection via epidemiological process models remain underdeveloped.\nJoint-analysis methods can more thoroughly analyze all available data, more\nprecisely quantifying epidemic processes, outbreak status, and risks. We\ncontribute a paired data modeling approach that analyzes multiple samples from\nindividuals. We use \"characterization maps\" to link paired data to\nepidemiological processes through a hierarchical statistical observation model.\nOur approach can provide both Bayesian and frequentist estimates of\nepidemiological parameters and state. We motivate our approach through the need\nto use paired pathogen and antibody detection tests to estimate parameters and\ninfection trajectories for the widely applicable susceptible, infectious,\nrecovered (SIR) model. We contribute general formulas to link characterization\nmaps to arbitrary process models and datasets and an extended SIR model that\nbetter accommodates paired data. We find via simulation that paired data can\nmore efficiently estimate SIR parameters than unpaired data, requiring samples\nfrom 5-10 times fewer individuals. We then study SARS-CoV-2 in wild\nWhite-tailed deer (Odocoileus virginianus) from three counties in the United\nStates. Estimates for average infectious times corroborate captive animal\nstudies. Our methods use general statistical theory to let applications extend\nbeyond the SIR model we consider, and to more complicated examples of paired\ndata.", "field": "Statistics", "categories": "stat.ME,physics.soc-ph,stat.AP"}, {"arxiv_id": "2401.10124", "title": "Lower Ricci Curvature for Efficient Community Detection", "abstract": "This study introduces the Lower Ricci Curvature (LRC), a novel, scalable, and\nscale-free discrete curvature designed to enhance community detection in\nnetworks. Addressing the computational challenges posed by existing\ncurvature-based methods, LRC offers a streamlined approach with linear\ncomputational complexity, making it well-suited for large-scale network\nanalysis. We further develop an LRC-based preprocessing method that effectively\naugments popular community detection algorithms. Through comprehensive\nsimulations and applications on real-world datasets, including the NCAA\nfootball league network, the DBLP collaboration network, the Amazon product\nco-purchasing network, and the YouTube social network, we demonstrate the\nefficacy of our method in significantly improving the performance of various\ncommunity detection algorithms.", "field": "Statistics", "categories": "stat.ME,cs.SI,physics.soc-ph,stat.AP"}, {"arxiv_id": "2401.1018", "title": "Generalized Decomposition Priors on R2", "abstract": "The adoption of continuous shrinkage priors in high-dimensional linear models\nhas gained momentum, driven by their theoretical and practical advantages. One\nof these shrinkage priors is the R2D2 prior, which comes with intuitive\nhyperparameters and well understood theoretical properties. The core idea is to\nspecify a prior on the percentage of explained variance $R^2$ and to conduct a\nDirichlet decomposition to distribute the explained variance among all the\nregression terms of the model. Due to the properties of the Dirichlet\ndistribution, the competition among variance components tends to gravitate\ntowards negative dependence structures, fully determined by the individual\ncomponents' means. Yet, in reality, specific coefficients or groups may compete\ndifferently for the total variability than the Dirichlet would allow for. In\nthis work we address this limitation by proposing a generalization of the R2D2\nprior, which we term the Generalized Decomposition R2 (GDR2) prior.\n  Our new prior provides great flexibility in expressing dependency structures\nas well as enhanced shrinkage properties. Specifically, we explore the\ncapabilities of variance decomposition via logistic normal distributions.\nThrough extensive simulations and real-world case studies, we demonstrate that\nGDR2 priors yield strongly improved out-of-sample predictive performance and\nparameter recovery compared to R2D2 priors with similar hyper-parameter\nchoices.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.10193", "title": "tinyVAST: R package with an expressive interface to specify lagged and\n  simultaneous effects in multivariate spatio-temporal models", "abstract": "Multivariate spatio-temporal models are widely applicable, but specifying\ntheir structure is complicated and may inhibit wider use. We introduce the R\npackage tinyVAST from two viewpoints: the software user and the statistician.\nFrom the user viewpoint, tinyVAST adapts a widely used formula interface to\nspecify generalized additive models, and combines this with arguments to\nspecify spatial and spatio-temporal interactions among variables. These\ninteractions are specified using arrow notation (from structural equation\nmodels), or an extended arrow-and-lag notation that allows simultaneous,\nlagged, and recursive dependencies among variables over time. The user also\nspecifies a spatial domain for areal (gridded), continuous (point-count), or\nstream-network data. From the statistician viewpoint, tinyVAST constructs\nsparse precision matrices representing multivariate spatio-temporal variation,\nand parameters are estimated by specifying a generalized linear mixed model\n(GLMM). This expressive interface encompasses vector autoregressive, empirical\northogonal functions, spatial factor analysis, and ARIMA models. To\ndemonstrate, we fit to data from two survey platforms sampling corals, sponges,\nrockfishes, and flatfishes in the Gulf of Alaska and Aleutian Islands. We then\ncompare eight alternative model structures using different assumptions about\nhabitat drivers and survey detectability. Model selection suggests that\ntowed-camera and bottom trawl gears have spatial variation in detectability but\nsample the same underlying density of flatfishes and rockfishes, and that\nrockfishes are positively associated with sponges while flatfishes are\nnegatively associated with corals. We conclude that tinyVAST can be used to\ntest complicated dependencies representing alternative structural assumptions\nfor research and real-world policy evaluation.", "field": "Statistics", "categories": "stat.ME,stat.AP"}, {"arxiv_id": "2401.10196", "title": "Functional Conditional Gaussian Graphical Models", "abstract": "Functional data has become a commonly encountered data type. In this paper,\nwe contribute to the literature on functional graphical modelling by extending\nthe notion of conditional Gaussian Graphical models and proposing a\ndouble-penalized estimator by which to recover the edge-set of the\ncorresponding graph. Penalty parameters play a crucial role in determining the\nprecision matrices for the response variables and the regression matrices. The\nperformance and model selection process in the proposed framework are\ninvestigated using information criteria. Moreover, we propose a novel version\nof the Kullback-Leibler cross-validation designed for conditional joint\nGaussian Graphical Models. The evaluation of model performance is done in terms\nof Kullback-Leibler divergence and graph recovery power.", "field": "Statistics", "categories": "stat.ME"}]}