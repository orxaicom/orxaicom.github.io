{"embeddings": [[11.344491958618164, 6.727543354034424], [11.837392807006836, 6.056991100311279], [11.740362167358398, 4.809117794036865], [11.228102684020996, 5.2944817543029785], [12.575927734375, 5.166530132293701], [11.642684936523438, 5.7255859375], [11.731675148010254, 7.382083415985107], [10.54751205444336, 5.279926300048828], [11.875773429870605, 5.285520553588867], [11.904472351074219, 6.758167743682861], [11.271343231201172, 6.996811866760254], [10.740229606628418, 6.904654026031494], [11.806350708007812, 7.8524861335754395], [11.344926834106445, 7.858896255493164], [12.822226524353027, 6.794633388519287], [11.25780963897705, 6.037052154541016], [12.505019187927246, 6.3795013427734375], [12.725419044494629, 5.81941556930542], [12.33925724029541, 5.633474826812744], [10.94420051574707, 7.506500720977783], [12.546870231628418, 7.3363165855407715], [10.458137512207031, 6.059876918792725], [11.21318531036377, 4.393450736999512], [13.190329551696777, 6.1283159255981445], [11.718244552612305, 4.346390724182129], [13.029646873474121, 7.272180080413818], [10.667085647583008, 4.831198692321777]], "keys": ["2401.12268", "2401.1227", "2401.12272", "2401.12331", "2401.12362", "2401.12409", "2401.1242", "2401.12476", "2401.12482", "2401.12598", "2401.12621", "2401.1264", "2401.12667", "2401.12697", "2401.1273", "2401.12753", "2401.12776", "2401.12827", "2401.12836", "2401.12865", "2401.12905", "2401.12911", "2401.12923", "2401.12924", "2401.12934", "2401.12937", "2401.12967"], "additional_info": [{"arxiv_id": "2401.12268", "title": "Generalized Ordinal Patterns Allowing for Ties and Their Applications in\n  Hydrology", "abstract": "When using ordinal patterns, which describe the ordinal structure within a\ndata vector, the problem of ties appeared permanently. So far, model classes\nwere used which do not allow for ties; randomization has been another attempt\nto overcome this problem. Often, time periods with constant values even have\nbeen counted as times of monotone increase. To overcome this, a new approach is\nproposed: it explicitly allows for ties and, hence, considers more patterns\nthan before. Ties are no longer seen as nuisance, but to carry valuable\ninformation. Limit theorems in the new framework are provided, both, for a\nsingle time series and for the dependence between two time series. The methods\nare used on hydrological data sets. It is common to distinguish five flood\nclasses (plus 'absence of flood'). Considering data vectors of these classes at\na certain gauge in a river basin, one will usually encounter several ties.\nCo-monotonic behavior between the data sets of two gauges (increasing,\nconstant, decreasing) can be detected by the method as well as spatial\npatterns. Thus, it helps to analyze the strength of dependence between\ndifferent gauges in an intuitive way. This knowledge can be used to asses risk\nand to plan future construction projects.", "field": "Statistics", "categories": "stat.AP,math.PR"}, {"arxiv_id": "2401.1227", "title": "Advances in the characterization of curvature of two-dimentional\n  probability manifolds", "abstract": "In this work some advances in the theory of curvature of two-dimensional\nprobability manifolds corresponding to families of distributions are proposed.\nIt is proved that location-scale distributions are hyperbolic in the\nInformation Geometry sense even when the generatrix is non-even or non-smooth.\nA novel formula is obtained for the computation of curvature in the case of\nexponential families: this formula implies some new flatness criteria in\ndimension 2. Finally, it is observed that many two parameter distributions,\nwidely used in applications, are locally hyperbolic, which highlights the role\nof hyperbolic geometry in the study of commonly employed probability manifolds.\nThese results have benefited from the use of explainable computational tools,\nwhich can substantially boost scientific productivity.", "field": "Statistics", "categories": "math.ST,math.PR,stat.TH"}, {"arxiv_id": "2401.12272", "title": "Transfer Learning for Nonparametric Regression: Non-asymptotic Minimax\n  Analysis and Adaptive Procedure", "abstract": "Transfer learning for nonparametric regression is considered. We first study\nthe non-asymptotic minimax risk for this problem and develop a novel estimator\ncalled the confidence thresholding estimator, which is shown to achieve the\nminimax optimal risk up to a logarithmic factor. Our results demonstrate two\nunique phenomena in transfer learning: auto-smoothing and super-acceleration,\nwhich differentiate it from nonparametric regression in a traditional setting.\nWe then propose a data-driven algorithm that adaptively achieves the minimax\nrisk up to a logarithmic factor across a wide range of parameter spaces.\nSimulation studies are conducted to evaluate the numerical performance of the\nadaptive transfer learning algorithm, and a real-world example is provided to\ndemonstrate the benefits of the proposed method.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.12331", "title": "Transfer Learning for Functional Mean Estimation: Phase Transition and\n  Adaptive Algorithms", "abstract": "This paper studies transfer learning for estimating the mean of random\nfunctions based on discretely sampled data, where, in addition to observations\nfrom the target distribution, auxiliary samples from similar but distinct\nsource distributions are available. The paper considers both common and\nindependent designs and establishes the minimax rates of convergence for both\ndesigns. The results reveal an interesting phase transition phenomenon under\nthe two designs and demonstrate the benefits of utilizing the source samples in\nthe low sampling frequency regime. For practical applications, this paper\nproposes novel data-driven adaptive algorithms that attain the optimal rates of\nconvergence within a logarithmic factor simultaneously over a large collection\nof parameter spaces. The theoretical findings are complemented by a simulation\nstudy that further supports the effectiveness of the proposed algorithms", "field": "Statistics", "categories": "math.ST,stat.TH"}, {"arxiv_id": "2401.12362", "title": "VC dimension of Graph Neural Networks with Pfaffian activation functions", "abstract": "Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool\nto learn tasks across a wide range of graph domains in a data-driven fashion;\nbased on a message passing mechanism, GNNs have gained increasing popularity\ndue to their intuitive formulation, closely linked with the Weisfeiler-Lehman\n(WL) test for graph isomorphism, to which they have proven equivalent. From a\ntheoretical point of view, GNNs have been shown to be universal approximators,\nand their generalization capability (namely, bounds on the Vapnik Chervonekis\n(VC) dimension) has recently been investigated for GNNs with piecewise\npolynomial activation functions. The aim of our work is to extend this analysis\non the VC dimension of GNNs to other commonly used activation functions, such\nas sigmoid and hyperbolic tangent, using the framework of Pfaffian function\ntheory. Bounds are provided with respect to architecture parameters (depth,\nnumber of neurons, input size) as well as with respect to the number of colors\nresulting from the 1-WL test applied on the graph domain. The theoretical\nanalysis is supported by a preliminary experimental study.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.12409", "title": "On Efficient Sampling Schemes for the Eigenvalues of Complex Wishart\n  Matrices", "abstract": "The paper \"An efficient sampling scheme for the eigenvalues of dual Wishart\nmatrices\", by I.~Santamar\\'ia and V.~Elvira, [\\emph{IEEE Signal Processing\nLetters}, vol.~28, pp.~2177--2181, 2021] \\cite{SE21}, poses the question of\nefficient sampling from the eigenvalue probability density function of the $n\n\\times n$ central complex Wishart matrices with variance matrix equal to the\nidentity. Underlying such complex Wishart matrices is a rectangular $R \\times\nn$ $(R \\ge n)$ standard complex Gaussian matrix, requiring then $2Rn$ real\nrandom variables for their generation. The main result of \\cite{SE21} gives a\nformula involving just two classical distributions specifying the two\neigenvalues in the case $n=2$. The purpose of this Letter is to point out that\nexisting results in the literature give two distinct ways to efficiently sample\nthe eigenvalues in the general $n$ case. One is in terms of the eigenvalues of\na tridiagonal matrix which factors as the product of a bidiagonal matrix and\nits transpose, with the $2n+1$ nonzero entries of the latter given by (the\nsquare root of) certain chi-squared random variables. The other is as the\ngeneralised eigenvalues for a pair of bidiagonal matrices, also containing a\ntotal of $2n+1$ chi-squared random variables. Moreover, these characterisation\npersist in the case of that the variance matrix consists of a single spike, and\nfor the case of real Wishart matrices.", "field": "Statistics", "categories": "math.ST,stat.TH"}, {"arxiv_id": "2401.1242", "title": "Rank-based estimators of global treatment effects for cluster randomized\n  trials with multiple endpoints", "abstract": "Cluster randomization trials commonly employ multiple endpoints. When a\nsingle summary of treatment effects across endpoints is of primary interest,\nglobal hypothesis testing/effect estimation methods represent a common analysis\nstrategy. However, specification of the joint distribution required by these\nmethods is non-trivial, particularly when endpoint properties differ. We\ndevelop rank-based interval estimators for a global treatment effect referred\nto as the \"global win probability,\" or the probability that a treatment\nindividual responds better than a control individual on average. Using\nendpoint-specific ranks among the combined sample and within each arm, each\nindividual-level observation is converted to a \"win fraction\" which quantifies\nthe proportion of wins experienced over every observation in the comparison\narm. An individual's multiple observations are then replaced by a single\n\"global win fraction,\" constructed by averaging win fractions across endpoints.\nA linear mixed model is applied directly to the global win fractions to recover\npoint, variance, and interval estimates of the global win probability adjusted\nfor clustering. Simulation demonstrates our approach performs well concerning\ncoverage and type I error, and methods are easily implemented using standard\nsoftware. A case study using publicly available data is provided with\ncorresponding R and SAS code.", "field": "Statistics", "categories": "stat.ME,stat.AP"}, {"arxiv_id": "2401.12476", "title": "Bayesian identification of nonseparable Hamiltonians with multiplicative\n  noise using deep learning and reduced-order modeling", "abstract": "This paper presents a structure-preserving Bayesian approach for learning\nnonseparable Hamiltonian systems using stochastic dynamic models allowing for\nstatistically-dependent, vector-valued additive and multiplicative measurement\nnoise. The approach is comprised of three main facets. First, we derive a\nGaussian filter for a statistically-dependent, vector-valued, additive and\nmultiplicative noise model that is needed to evaluate the likelihood within the\nBayesian posterior. Second, we develop a novel algorithm for cost-effective\napplication of Bayesian system identification to high-dimensional systems.\nThird, we demonstrate how structure-preserving methods can be incorporated into\nthe proposed framework, using nonseparable Hamiltonians as an illustrative\nsystem class. We compare the Bayesian method to a state-of-the-art machine\nlearning method on a canonical nonseparable Hamiltonian model and a chaotic\ndouble pendulum model with small, noisy training datasets. The results show\nthat using the Bayesian posterior as a training objective can yield upwards of\n724 times improvement in Hamiltonian mean squared error using training data\nwith up to 10% multiplicative noise compared to a standard training objective.\nLastly, we demonstrate the utility of the novel algorithm for parameter\nestimation of a 64-dimensional model of the spatially-discretized nonlinear\nSchr\\\"odinger equation with data corrupted by up to 20% multiplicative noise.", "field": "Statistics", "categories": "stat.ML,cs.LG,math.DS,physics.data-an,stat.CO"}, {"arxiv_id": "2401.12482", "title": "Nonparametric logistic regression with deep learning", "abstract": "Consider the nonparametric logistic regression problem. In the logistic\nregression, we usually consider the maximum likelihood estimator, and the\nexcess risk is the expectation of the Kullback-Leibler (KL) divergence between\nthe true and estimated conditional class probabilities. However, in the\nnonparametric logistic regression, the KL divergence could diverge easily, and\nthus, the convergence of the excess risk is difficult to prove or does not\nhold. Several existing studies show the convergence of the KL divergence under\nstrong assumptions. In most cases, our goal is to estimate the true conditional\nclass probabilities. Thus, instead of analyzing the excess risk itself, it\nsuffices to show the consistency of the maximum likelihood estimator in some\nsuitable metric. In this paper, using a simple unified approach for analyzing\nthe nonparametric maximum likelihood estimator (NPMLE), we directly derive the\nconvergence rates of the NPMLE in the Hellinger distance under mild\nassumptions. Although our results are similar to the results in some existing\nstudies, we provide simple and more direct proofs for these results. As an\nimportant application, we derive the convergence rates of the NPMLE with deep\nneural networks and show that the derived rate nearly achieves the minimax\noptimal rate.", "field": "Statistics", "categories": "math.ST,stat.ML,stat.TH"}, {"arxiv_id": "2401.12598", "title": "Asymptotic confidence interval for R2 in multiple linear regression", "abstract": "Following White's approach of robust multiple linear regression, we give\nasymptotic confidence intervals for the multiple correlation coefficient R2\nunder minimal moment conditions. We also give the asymptotic joint distribution\nof the empirical estimators of the individual R2's. Through different sets of\nsimulations, we show that the procedure is indeed robust (contrary to the\nprocedure involving the near exact distribution of the empirical estimator of\nR2 is the multivariate Gaussian case) and can be also applied to count linear\nregression.", "field": "Statistics", "categories": "math.ST,stat.TH"}, {"arxiv_id": "2401.12621", "title": "Classification non supervis{\u00e9}e des processus d'{\u00e9}v{\u00e9}nements\n  r{\u00e9}currents", "abstract": "Event of the same type occurring several times for one individual (recurrent\nevents) are present in various domains (industrial systems reliability,\nepisodes of unemployment, political conflicts, chronic diseases episodes).\nAnalysis of such kind of data should account for the whole recurrence process\ndynamics rather than only focusing on the number of observed events.\nStatistical models for recurrent events analysis are developed in the counting\nprocess probabilistic framework. One of the often-used models is the\nAndersen-Gill model, a generalization of the well-known Cox model for\ndurations, which assumes that the baseline intensity of the recurrence process\nis time-dependent and is adjusted for covariates. For an individual i with\ncovariates Xi, the intensity is as follows: $\\lambda$_{ik}(t;$\\theta$) =\n$\\lambda$_0(t) exp (X_i $\\beta$). The baseline intensity can be specified\nparametrically, in a form of Weibull: $\\lambda$_0 (t) = $\\gamma$_{1}\n$\\gamma$_{2} t^{$\\gamma$_2-1}, with $\\gamma$1 scale parameter et $\\gamma$2\nshape parameter. However, the observed covariates are often insufficient to\nexplain the observed heterogeneity in data. This is often the case of clinical\ntrials data containing information on patients. In this article a mixture model\nfor recurrent events analysis is proposed. This model allows to account for\nunobserved heterogeneity and to cluster individuals according to their\nrecurrence process. The intensity of the process is parametrically specified\nwithin each class and depend on observed covariates. Thus, the intensity\nbecomes specific to class k: $\\lambda$_{ik} (t; $\\theta$_k) = $\\gamma$_{1k}\n$\\gamma$_{2k} t^{$\\gamma$_{2k}-1} exp (X_i $\\beta$_k). The model parameters are\nestimated by the Maximum Likelihood method, using the EM algorithm. The BIC\ncriterion is employed to choose the optimal number of classes. Model\nfeasibility is verified by Monte Carlo simulations. An application to real data\nconcerning hospital readmissions of elderly patients is proposed. The proposed\nmodel feasibility is empirically verified (the optimization algorithm\nconverges, providing non-biased estimates). The real data application allows to\nidentify two clinically relevant classes of patients.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.1264", "title": "Multilevel network meta-regression for general likelihoods: synthesis of\n  individual and aggregate data with applications to survival analysis", "abstract": "Network meta-analysis combines aggregate data (AgD) from multiple randomised\ncontrolled trials, assuming that any effect modifiers are balanced across\npopulations. Individual patient data (IPD) meta-regression is the ``gold\nstandard'' method to relax this assumption, however IPD are frequently only\navailable in a subset of studies. Multilevel network meta-regression (ML-NMR)\nextends IPD meta-regression to incorporate AgD studies whilst avoiding\naggregation bias, but currently requires the aggregate-level likelihood to have\na known closed form. Notably, this prevents application to time-to-event\noutcomes.\n  We extend ML-NMR to individual-level likelihoods of any form, by integrating\nthe individual-level likelihood function over the AgD covariate distributions\nto obtain the respective marginal likelihood contributions. We illustrate with\ntwo examples of time-to-event outcomes, showing the performance of ML-NMR in a\nsimulated comparison with little loss of precision from a full IPD analysis,\nand demonstrating flexible modelling of baseline hazards using cubic M-splines\nwith synthetic data on newly diagnosed multiple myeloma.\n  ML-NMR is a general method for synthesising individual and aggregate level\ndata in networks of all sizes. Extension to general likelihoods, including for\nsurvival outcomes, greatly increases the applicability of the method. R and\nStan code is provided, and the methods are implemented in the multinma R\npackage.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.12667", "title": "Feature Selection via Robust Weighted Score for High Dimensional Binary\n  Class-Imbalanced Gene Expression Data", "abstract": "In this paper, a robust weighted score for unbalanced data (ROWSU) is\nproposed for selecting the most discriminative feature for high dimensional\ngene expression binary classification with class-imbalance problem. The method\naddresses one of the most challenging problems of highly skewed class\ndistributions in gene expression datasets that adversely affect the performance\nof classification algorithms. First, the training dataset is balanced by\nsynthetically generating data points from minority class observations. Second,\na minimum subset of genes is selected using a greedy search approach. Third, a\nnovel weighted robust score, where the weights are computed by support vectors,\nis introduced to obtain a refined set of genes. The highest-scoring genes based\non this approach are combined with the minimum subset of genes selected by the\ngreedy search approach to form the final set of genes. The novel method ensures\nthe selection of the most discriminative genes, even in the presence of skewed\nclass distribution, thus improving the performance of the classifiers. The\nperformance of the proposed ROWSU method is evaluated on $6$ gene expression\ndatasets. Classification accuracy and sensitivity are used as performance\nmetrics to compare the proposed ROWSU algorithm with several other\nstate-of-the-art methods. Boxplots and stability plots are also constructed for\na better understanding of the results. The results show that the proposed\nmethod outperforms the existing feature selection procedures based on\nclassification performance from k nearest neighbours (kNN) and random forest\n(RF) classifiers.", "field": "Statistics", "categories": "stat.ML,cs.LG,14J60"}, {"arxiv_id": "2401.12697", "title": "A Computationally Efficient Approach to False Discovery Rate Control and\n  Power Maximisation via Randomisation and Mirror Statistic", "abstract": "Simultaneously performing variable selection and inference in\nhigh-dimensional regression models is an open challenge in statistics and\nmachine learning. The increasing availability of vast amounts of variables\nrequires the adoption of specific statistical procedures to accurately select\nthe most important predictors in a high-dimensional space, while controlling\nthe False Discovery Rate (FDR) arising from the underlying multiple hypothesis\ntesting. In this paper we propose the joint adoption of the Mirror Statistic\napproach to FDR control, coupled with outcome randomisation to maximise the\nstatistical power of the variable selection procedure. Through extensive\nsimulations we show how our proposed strategy allows to combine the benefits of\nthe two techniques. The Mirror Statistic is a flexible method to control FDR,\nwhich only requires mild model assumptions, but requires two sets of\nindependent regression coefficient estimates, usually obtained after splitting\nthe original dataset. Outcome randomisation is an alternative to Data\nSplitting, that allows to generate two independent outcomes, which can then be\nused to estimate the coefficients that go into the construction of the Mirror\nStatistic. The combination of these two approaches provides increased testing\npower in a number of scenarios, such as highly correlated covariates and high\npercentages of active variables. Moreover, it is scalable to very\nhigh-dimensional problems, since the algorithm has a low memory footprint and\nonly requires a single run on the full dataset, as opposed to iterative\nalternatives such as Multiple Data Splitting.", "field": "Statistics", "categories": "stat.ME,stat.AP"}, {"arxiv_id": "2401.1273", "title": "On the visualisation of the correlation matrix", "abstract": "Extensions of earlier algorithms and enhanced visualization techniques for\napproximating a correlation matrix are presented. The visualization problems\nthat result from using column or colum--and--row adjusted correlation matrices,\nwhich give numerically a better fit, are addressed. For visualization of a\ncorrelation matrix a weighted alternating least squares algorithm is used, with\neither a single scalar adjustment, or a column-only adjustment with symmetric\nfactorization; these choices form a compromise between the numerical accuracy\nof the approximation and the comprehensibility of the obtained correlation\nbiplots. Some illustrative examples are discussed.", "field": "Statistics", "categories": "stat.CO,62,G.3"}, {"arxiv_id": "2401.12753", "title": "Optimal Confidence Bands for Shape-restricted Regression in\n  Multidimensions", "abstract": "In this paper, we propose and study construction of confidence bands for\nshape-constrained regression functions when the predictor is multivariate. In\nparticular, we consider the continuous multidimensional white noise model given\nby $d Y(\\mathbf{t}) = n^{1/2} f(\\mathbf{t}) \\,d\\mathbf{t} + d W(\\mathbf{t})$,\nwhere $Y$ is the observed stochastic process on $[0,1]^d$ ($d\\ge 1$), $W$ is\nthe standard Brownian sheet on $[0,1]^d$, and $f$ is the unknown function of\ninterest assumed to belong to a (shape-constrained) function class, e.g.,\ncoordinate-wise monotone functions or convex functions. The constructed\nconfidence bands are based on local kernel averaging with bandwidth chosen\nautomatically via a multivariate multiscale statistic. The confidence bands\nhave guaranteed coverage for every $n$ and for every member of the underlying\nfunction class. Under monotonicity/convexity constraints on $f$, the proposed\nconfidence bands automatically adapt (in terms of width) to the global and\nlocal (H\\\"{o}lder) smoothness and intrinsic dimensionality of the unknown $f$;\nthe bands are also shown to be optimal in a certain sense. These bands have\n(almost) parametric ($n^{-1/2}$) widths when the underlying function has\n``low-complexity'' (e.g., piecewise constant/affine).", "field": "Statistics", "categories": "math.ST,stat.ME,stat.TH"}, {"arxiv_id": "2401.12776", "title": "Sub-model aggregation for scalable eigenvector spatial filtering:\n  Application to spatially varying coefficient modeling", "abstract": "This study proposes a method for aggregating/synthesizing global and local\nsub-models for fast and flexible spatial regression modeling. Eigenvector\nspatial filtering (ESF) was used to model spatially varying coefficients and\nspatial dependence in the residuals by sub-model, while the generalized\nproduct-of-experts method was used to aggregate these sub-models. The major\nadvantages of the proposed method are as follows: (i) it is highly scalable for\nlarge samples in terms of accuracy and computational efficiency; (ii) it is\neasily implemented by estimating sub-models independently first and\naggregating/averaging them thereafter; and (iii) likelihood-based inference is\navailable because the marginal likelihood is available in closed-form. The\naccuracy and computational efficiency of the proposed method are confirmed\nusing Monte Carlo simulation experiments. This method was then applied to\nresidential land price analysis in Japan. The results demonstrate the\nusefulness of this method for improving the interpretability of spatially\nvarying coefficients. The proposed method is implemented in an R package\nspmoran (version 0.3.0 or later).", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.12827", "title": "Distributed Empirical Likelihood Inference With or Without Byzantine\n  Failures", "abstract": "Empirical likelihood is a very important nonparametric approach which is of\nwide application. However, it is hard and even infeasible to calculate the\nempirical log-likelihood ratio statistic with massive data. The main challenge\nis the calculation of the Lagrange multiplier. This motivates us to develop a\ndistributed empirical likelihood method by calculating the Lagrange multiplier\nin a multi-round distributed manner. It is shown that the distributed empirical\nlog-likelihood ratio statistic is asymptotically standard chi-squared under\nsome mild conditions. The proposed algorithm is communication-efficient and\nachieves the desired accuracy in a few rounds. Further, the distributed\nempirical likelihood method is extended to the case of Byzantine failures. A\nmachine selection algorithm is developed to identify the worker machines\nwithout Byzantine failures such that the distributed empirical likelihood\nmethod can be applied. The proposed methods are evaluated by numerical\nsimulations and illustrated with an analysis of airline on-time performance\nstudy and a surface climate analysis of Yangtze River Economic Belt.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.12836", "title": "Empirical Likelihood Inference over Decentralized Networks", "abstract": "As a nonparametric statistical inference approach, empirical likelihood has\nbeen found very useful in numerous occasions. However, it encounters serious\ncomputational challenges when applied directly to the modern massive dataset.\nThis article studies empirical likelihood inference over decentralized\ndistributed networks, where the data are locally collected and stored by\ndifferent nodes. To fully utilize the data, this article fuses Lagrange\nmultipliers calculated in different nodes by employing a penalization\ntechnique. The proposed distributed empirical log-likelihood ratio statistic\nwith Lagrange multipliers solved by the penalized function is asymptotically\nstandard chi-squared under regular conditions even for a divergent machine\nnumber. Nevertheless, the optimization problem with the fused penalty is still\nhard to solve in the decentralized distributed network. To address the problem,\ntwo alternating direction method of multipliers (ADMM) based algorithms are\nproposed, which both have simple node-based implementation schemes.\nTheoretically, this article establishes convergence properties for proposed\nalgorithms, and further proves the linear convergence of the second algorithm\nin some specific network structures. The proposed methods are evaluated by\nnumerical simulations and illustrated with analyses of census income and Ford\ngobike datasets.", "field": "Statistics", "categories": "stat.ME,stat.CO"}, {"arxiv_id": "2401.12865", "title": "Gridsemble: Selective Ensembling for False Discovery Rates", "abstract": "In this paper, we introduce Gridsemble, a data-driven selective ensembling\nalgorithm for estimating local false discovery rates (fdr) in large-scale\nmultiple hypothesis testing. Existing methods for estimating fdr often yield\ndifferent conclusions, yet the unobservable nature of fdr values prevents the\nuse of traditional model selection. There is limited guidance on choosing a\nmethod for a given dataset, making this an arbitrary decision in practice.\nGridsemble circumvents this challenge by ensembling a subset of methods with\nweights based on their estimated performances, which are computed on synthetic\ndatasets generated to mimic the observed data while including ground truth. We\ndemonstrate through simulation studies and an experimental application that\nthis method outperforms three popular R software packages with their default\nparameter values$\\unicode{x2014}$common choices given the current landscape.\nWhile our applications are in the context of high throughput transcriptomics,\nwe emphasize that Gridsemble is applicable to any use of large-scale multiple\nhypothesis testing, an approach that is utilized in many fields. We believe\nthat Gridsemble will be a useful tool for computing reliable estimates of fdr\nand for improving replicability in the presence of multiple hypotheses by\neliminating the need for an arbitrary choice of method. Gridsemble is\nimplemented in an open-source R software package available on GitHub at\njennalandy/gridsemblefdr.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.12905", "title": "Estimating the construct validity of Principal Components Analysis", "abstract": "In many scientific disciplines, the features of interest cannot be observed\ndirectly, so must instead be inferred from observed behaviour. Latent variable\nanalyses are increasingly employed to systematise these inferences, and\nPrincipal Components Analysis (PCA) is perhaps the simplest and most popular of\nthese methods. Here, we examine how the assumptions that we are prepared to\nentertain, about the latent variable system, mediate the likelihood that\nPCA-derived components will capture the true sources of variance underlying\ndata. As expected, we find that this likelihood is excellent in the best case,\nand robust to empirically reasonable levels of measurement noise, but best-case\nperformance is also: (a) not robust to violations of the method's more\nprominent assumptions, of linearity and orthogonality; and also (b) requires\nthat other subtler assumptions be made, such as that the latent variables\nshould have varying importance, and that weights relating latent variables to\nobserved data have zero mean. Neither variance explained, nor replication in\nindependent samples, could reliably predict which (if any) PCA-derived\ncomponents will capture true sources of variance in data. We conclude by\ndescribing a procedure to fit these inferences more directly to empirical data,\nand use it to find that components derived via PCA from two different empirical\nneuropsychological datasets, are less likely to have meaningful referents in\nthe brain than we hoped.", "field": "Statistics", "categories": "stat.ME,stat.AP"}, {"arxiv_id": "2401.12911", "title": "Pretraining and the Lasso", "abstract": "Pretraining is a popular and powerful paradigm in machine learning. As an\nexample, suppose one has a modest-sized dataset of images of cats and dogs, and\nplans to fit a deep neural network to classify them from the pixel features.\nWith pretraining, we start with a neural network trained on a large corpus of\nimages, consisting of not just cats and dogs but hundreds of other image types.\nThen we fix all of the network weights except for the top layer (which makes\nthe final classification) and train (or \"fine tune\") those weights on our\ndataset. This often results in dramatically better performance than the network\ntrained solely on our smaller dataset.\n  In this paper, we ask the question \"Can pretraining help the lasso?\". We\ndevelop a framework for the lasso in which an overall model is fit to a large\nset of data, and then fine-tuned to a specific task on a smaller dataset. This\nlatter dataset can be a subset of the original dataset, but does not need to\nbe. We find that this framework has a wide variety of applications, including\nstratified models, multinomial targets, multi-response models, conditional\naverage treatment estimation and even gradient boosting.\n  In the stratified model setting, the pretrained lasso pipeline estimates the\ncoefficients common to all groups at the first stage, and then group specific\ncoefficients at the second \"fine-tuning\" stage. We show that under appropriate\nassumptions, the support recovery rate of the common coefficients is superior\nto that of the usual lasso trained only on individual groups. This separate\nidentification of common and individual coefficients can also be useful for\nscientific understanding.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.12923", "title": "Deep multitask neural networks for solving some stochastic optimal\n  control problems", "abstract": "Most existing neural network-based approaches for solving stochastic optimal\ncontrol problems using the associated backward dynamic programming principle\nrely on the ability to simulate the underlying state variables. However, in\nsome problems, this simulation is infeasible, leading to the discretization of\nstate variable space and the need to train one neural network for each data\npoint. This approach becomes computationally inefficient when dealing with\nlarge state variable spaces. In this paper, we consider a class of this type of\nstochastic optimal control problems and introduce an effective solution\nemploying multitask neural networks. To train our multitask neural network, we\nintroduce a novel scheme that dynamically balances the learning across tasks.\nThrough numerical experiments on real-world derivatives pricing problems, we\nprove that our method outperforms state-of-the-art approaches.", "field": "Statistics", "categories": "stat.ML,cs.LG,cs.SY,eess.SY"}, {"arxiv_id": "2401.12924", "title": "Performance Analysis of Support Vector Machine (SVM) on Challenging\n  Datasets for Forest Fire Detection", "abstract": "This article delves into the analysis of performance and utilization of\nSupport Vector Machines (SVMs) for the critical task of forest fire detection\nusing image datasets. With the increasing threat of forest fires to ecosystems\nand human settlements, the need for rapid and accurate detection systems is of\nutmost importance. SVMs, renowned for their strong classification capabilities,\nexhibit proficiency in recognizing patterns associated with fire within images.\nBy training on labeled data, SVMs acquire the ability to identify distinctive\nattributes associated with fire, such as flames, smoke, or alterations in the\nvisual characteristics of the forest area. The document thoroughly examines the\nuse of SVMs, covering crucial elements like data preprocessing, feature\nextraction, and model training. It rigorously evaluates parameters such as\naccuracy, efficiency, and practical applicability. The knowledge gained from\nthis study aids in the development of efficient forest fire detection systems,\nenabling prompt responses and improving disaster management. Moreover, the\ncorrelation between SVM accuracy and the difficulties presented by\nhigh-dimensional datasets is carefully investigated, demonstrated through a\nrevealing case study. The relationship between accuracy scores and the\ndifferent resolutions used for resizing the training datasets has also been\ndiscussed in this article. These comprehensive studies result in a definitive\noverview of the difficulties faced and the potential sectors requiring further\nimprovement and focus.", "field": "Statistics", "categories": "stat.ML,cs.LG,stat.ME"}, {"arxiv_id": "2401.12934", "title": "Reward-Relevance-Filtered Linear Offline Reinforcement Learning", "abstract": "This paper studies offline reinforcement learning with linear function\napproximation in a setting with decision-theoretic, but not estimation\nsparsity. The structural restrictions of the data-generating process presume\nthat the transitions factor into a sparse component that affects the reward and\ncould affect additional exogenous dynamics that do not affect the reward.\nAlthough the minimally sufficient adjustment set for estimation of full-state\ntransition properties depends on the whole state, the optimal policy and\ntherefore state-action value function depends only on the sparse component: we\ncall this causal/decision-theoretic sparsity. We develop a method for\nreward-filtering the estimation of the state-action value function to the\nsparse component by a modification of thresholded lasso in least-squares policy\nevaluation. We provide theoretical guarantees for our reward-filtered linear\nfitted-Q-iteration, with sample complexity depending only on the size of the\nsparse component.", "field": "Statistics", "categories": "stat.ML,cs.LG,math.OC"}, {"arxiv_id": "2401.12937", "title": "Are the Signs of Factor Loadings Arbitrary in Confirmatory Factor\n  Analysis? Problems and Solutions", "abstract": "The replication crisis in social and behavioral sciences has raised concerns\nabout the reliability and validity of empirical studies. While research in the\nliterature has explored contributing factors to this crisis, the issues related\nto analytical tools have received less attention. This study focuses on a\nwidely used analytical tool - confirmatory factor analysis (CFA) - and\ninvestigates one issue that is typically overlooked in practice: accurately\nestimating factor-loading signs. Incorrect loading signs can distort the\nrelationship between observed variables and latent factors, leading to\nunreliable or invalid results in subsequent analyses. Our study aims to\ninvestigate and address the estimation problem of factor-loading signs in CFA\nmodels. Based on an empirical demonstration and Monte Carlo simulation studies,\nwe found current methods have drawbacks in estimating loading signs. To address\nthis problem, three solutions are proposed and proven to work effectively. The\napplications of these solutions are discussed and elaborated.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.12967", "title": "Measure transport with kernel mean embeddings", "abstract": "Kalman filters constitute a scalable and robust methodology for approximate\nBayesian inference, matching first and second order moments of the target\nposterior. To improve the accuracy in nonlinear and non-Gaussian settings, we\nextend this principle to include more or different characteristics, based on\nkernel mean embeddings (KMEs) of probability measures into their corresponding\nHilbert spaces. Focusing on the continuous-time setting, we develop a family of\ninteracting particle systems (termed $\\textit{KME-dynamics}$) that bridge\nbetween the prior and the posterior, and that include the Kalman-Bucy filter as\na special case. A variant of KME-dynamics has recently been derived from an\noptimal transport perspective by Maurais and Marzouk, and we expose further\nconnections to (kernelised) diffusion maps, leading to a variational\nformulation of regression type. Finally, we conduct numerical experiments on\ntoy examples and the Lorenz-63 model, the latter of which show particular\npromise for a hybrid modification (called Kalman-adjusted KME-dynamics).", "field": "Statistics", "categories": "math.ST,cs.NA,math.NA,stat.ME,stat.TH"}]}