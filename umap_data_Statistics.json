{"embeddings": [[0.06326711922883987, 6.461483001708984], [-0.11672782897949219, 7.039000988006592], [1.2885404825210571, 7.590418815612793], [0.6541584730148315, 8.039681434631348], [1.480798602104187, 6.934531211853027], [0.5707998871803284, 5.497675895690918], [-0.038771502673625946, 7.985702037811279], [-0.7499953508377075, 7.322258949279785], [0.790458619594574, 6.273219108581543], [1.3320863246917725, 5.850271701812744], [-0.23602911829948425, 5.611671447753906], [1.9139010906219482, 7.64982795715332], [-0.7560397982597351, 6.421485900878906], [0.5812097191810608, 7.002264022827148]], "keys": ["2401.10233", "2401.10235", "2401.10275", "2401.10276", "2401.10349", "2401.10437", "2401.10558", "2401.10592", "2401.10605", "2401.10796", "2401.10811", "2401.10824", "2401.10867", "2401.10869"], "additional_info": [{"arxiv_id": "2401.10233", "title": "Likelihood-ratio inference on differences in quantiles", "abstract": "Quantiles can represent key operational and business metrics, but the\ncomputational challenges associated with inference has hampered their adoption\nin online experimentation. One-sample confidence intervals are trivial to\nconstruct; however, two-sample inference has traditionally required\nbootstrapping or a density estimator. This paper presents a new two-sample\ndifference-in-quantile hypothesis test and confidence interval based on a\nlikelihood-ratio test statistic. A conservative version of the test does not\ninvolve a density estimator; a second version of the test, which uses a density\nestimator, yields confidence intervals very close to the nominal coverage\nlevel. It can be computed using only four order statistics from each sample.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.10235", "title": "Semi-parametric local variable selection under misspecification", "abstract": "Local variable selection aims to discover localized effects by assessing the\nimpact of covariates on outcomes within specific regions defined by other\ncovariates. We outline some challenges of local variable selection in the\npresence of non-linear relationships and model misspecification. Specifically,\nwe highlight a potential drawback of common semi-parametric methods: even\nslight model misspecification can result in a high rate of false positives. To\naddress these shortcomings, we propose a methodology based on orthogonal cut\nsplines that achieves consistent local variable selection in high-dimensional\nscenarios. Our approach offers simplicity, handles both continuous and discrete\ncovariates, and provides theory for high-dimensional covariates and model\nmisspecification. We discuss settings with either independent or dependent\ndata. Our proposal allows including adjustment covariates that do not undergo\nselection, enhancing flexibility in modeling complex scenarios. We illustrate\nits application in simulation studies with both independent and functional\ndata, as well as with two real datasets. One dataset evaluates salary gaps\nassociated with discrimination factors at different ages, while the other\nexamines the effects of covariates on brain activation over time. The approach\nis implemented in the R package mombf.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.10275", "title": "Symbolic principal plane with Duality Centers Method", "abstract": "In \\cite{ref11} and \\cite{ref3}, the authors proposed the Centers and the\nVertices Methods to extend the well known principal components analysis method\nto a particular kind of symbolic objects characterized by multi--valued\nvariables of interval type. Nevertheless the authors use the classical circle\nof correlation to represent the variables. The correlation between the\nvariables and the principal components are not symbolic, because they compute\nthe standard correlations between the midpoints of variables and the midpoints\nof the principal components. It is well known that in standard principal\ncomponent analysis we may compute the correlation between the variables and the\nprincipal components using the duality relations starting from the coordinates\nof the individuals in the principal plane, also we can compute the coordinates\nof the individuals in the principal plane using duality relations starting from\nthe correlation between the variables and the principal components. In this\npaper we propose a new method to compute the symbolic correlation circle using\nduality relations in the case of interval-valued variables. Besides, the reader\nmay use all the methods presented herein and verify the results using the {\\tt\nRSDA} package written in {\\tt R} language, that can be downloaded and installed\ndirectly from {\\tt CRAN} \\cite{Rod2014}.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.10276", "title": "Correspondence Analysis for Symbolic Multi--Valued Variables", "abstract": "This paper sets a proposal of a new method and two new algorithms for\nCorrespondence Analysis when we have Symbolic Multi--Valued Variables (SymCA).\nIn our method, there are two multi--valued variables $X$ and $Y$, that is to\nsay, the modality that takes the variables for a given individual is a finite\nset formed by the possible modalities taken for the variables in a given\nindividual, that which allows to apply the Correspondence Analysis to multiple\nselection questionnaires. Then, starting from all the possible classic\ncontingency tables an interval contingency table can be built, which will be\nthe point of departure of the proposed method.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.10349", "title": "Two-sample tests for relevant differences in persistence diagrams", "abstract": "We study two-sample tests for relevant differences in persistence diagrams\nobtained from $L^p$-$m$-approximable data $(\\mathcal{X}_t)_t$ and\n$(\\mathcal{Y}_t)_t$. To this end, we compare variance estimates w.r.t.\\ the\nWasserstein metrics on the space of persistence diagrams. In detail, we\nconsider two test procedures. The first compares the Fr{\\'e}chet variances of\nthe two samples based on estimators for the Fr{\\'e}chet mean of the observed\npersistence diagrams $PD(\\mathcal{X}_i)$ ($1\\le i\\le m$), resp.,\n$PD(\\mathcal{Y}_j)$ ($1\\le j\\le n$) of a given feature dimension. We use\nclassical functional central limit theorems to establish consistency of the\ntesting procedure. The second procedure relies on a comparison of the so-called\nindependent copy variances of the respective samples. Technically, this leads\nto functional central limit theorems for U-statistics built on\n$L^p$-$m$-approximable sample data.", "field": "Statistics", "categories": "math.ST,math.PR,stat.TH"}, {"arxiv_id": "2401.10437", "title": "Optimal Sensor Allocation with Multiple Linear Dispersion Processes", "abstract": "This paper considers the optimal sensor allocation for estimating the\nemission rates of multiple sources in a two-dimensional spatial domain.\nLocations of potential emission sources are known (e.g., factory stacks), and\nthe number of sources is much greater than the number of sensors that can be\ndeployed, giving rise to the optimal sensor allocation problem. In particular,\nwe consider linear dispersion forward models, and the optimal sensor allocation\nis formulated as a bilevel optimization problem. The outer problem determines\nthe optimal sensor locations by minimizing the overall Mean Squared Error of\nthe estimated emission rates over various wind conditions, while the inner\nproblem solves an inverse problem that estimates the emission rates. Two\nalgorithms, including the repeated Sample Average Approximation and the\nStochastic Gradient Descent based bilevel approximation, are investigated in\nsolving the sensor allocation problem. Convergence analysis is performed to\nobtain the performance guarantee, and numerical examples are presented to\nillustrate the proposed approach.", "field": "Statistics", "categories": "stat.CO"}, {"arxiv_id": "2401.10558", "title": "ICBeLLM: High Quality International Events Data with Open Source Large\n  Language Models on Consumer Hardware", "abstract": "The International Crises Behavior Events (ICBe) ontology provides high\ncoverage over the thoughts, communications, and actions that constitute\ninternational relations. A major disadvantage of that level of detail is that\nit requires large human capital costs to apply it manually to new texts.\nWhether such an ontolgy is practical for international relations research given\nlimited human and financial resources is a pressing concern. We introduce a\nworking proof of concept showing that ICBe codings can be reliably extracted\nfrom new texts using the current generation of open source large language\nmodels (LLM) running on consumer grade computer hardware. Our solution requires\nno finetuning and only limited prompt engineering. We detail our solution and\npresent benchmarks against the original ICBe codings. We conclude by discussing\nthe implications of very high quality event coding of any text being within\nreach of individual researchers with limited resources.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.10592", "title": "Bayesian sample size determination using robust commensurate priors with\n  interpretable discrepancy weights", "abstract": "Randomized controlled clinical trials provide the gold standard for evidence\ngeneration in relation to the effectiveness of a new treatment in medical\nresearch. Relevant information from previous studies may be desirable to\nincorporate in the design of a new trial, with the Bayesian paradigm providing\na coherent framework to formally incorporate prior knowledge. Many established\nmethods involve the use of a discounting factor, sometimes related to a measure\nof `similarity' between historical sources and the new trial. However, it is\noften the case that the sample size is highly nonlinear in those discounting\nfactors. This hinders communication with subject-matter experts to elicit\nsensible values for borrowing strength at the trial design stage.\n  Focusing on a sample size formula that can incorporate historical data from\nmultiple sources, we propose a linearization technique such that the sample\nsize changes evenly over values of the discounting factors (hereafter referred\nto as `weights'). Our approach leads to interpretable weights that directly\nrepresent the dissimilarity between historical and new trial data on the\nprobability scale, and could therefore facilitate easier elicitation of expert\nopinion on their values.\n  Inclusion of historical data in the design of clinical trials is not common\npractice. Part of the reason might be difficulty in interpretability of\ndiscrepancy parameters. We hope our work will help to bridge this gap and\nencourage uptake of these innovative methods.\n  Keywords: Bayesian sample size determination; Commensurate priors; Historical\nborrowing; Prior aggregation; Uniform shrinkage.", "field": "Statistics", "categories": "stat.ME,stat.AP"}, {"arxiv_id": "2401.10605", "title": "The Unconditional Performance of Control Charts for Zero-Inflated\n  Processes with Estimated Parameters", "abstract": "Control charts for zero-inflated processes have attracted the interest of the\nresearchers in the recent years. In this work we investigate the performance of\nShewhart-type charts for zero-inflated Poisson and zero-inflated Binomial\nprocesses, in the case of estimated parameters. This is a case that usually\noccurs in practice, especially prior to starting the process monitoring. Using\nMonte Carlo simulation we evaluate charts' performance under an unconditional\nperspective and provide guidelines for their use in practice. We examine both\nthe in-control and the out-of-control performance.", "field": "Statistics", "categories": "stat.AP,62P30"}, {"arxiv_id": "2401.10796", "title": "Reliability analysis for data-driven noisy models using active learning", "abstract": "Reliability analysis aims at estimating the failure probability of an\nengineering system. It often requires multiple runs of a limit-state function,\nwhich usually relies on computationally intensive simulations. Traditionally,\nthese simulations have been considered deterministic, i.e., running them\nmultiple times for a given set of input parameters always produces the same\noutput. However, this assumption does not always hold, as many studies in the\nliterature report non-deterministic computational simulations (also known as\nnoisy models). In such cases, running the simulations multiple times with the\nsame input will result in different outputs. Similarly, data-driven models that\nrely on real-world data may also be affected by noise. This characteristic\nposes a challenge when performing reliability analysis, as many classical\nmethods, such as FORM and SORM, are tailored to deterministic models. To bridge\nthis gap, this paper provides a novel methodology to perform reliability\nanalysis on models contaminated by noise. In such cases, noise introduces\nlatent uncertainty into the reliability estimator, leading to an incorrect\nestimation of the real underlying reliability index, even when using Monte\nCarlo simulation. To overcome this challenge, we propose the use of denoising\nregression-based surrogate models within an active learning reliability\nanalysis framework. Specifically, we combine Gaussian process regression with a\nnoise-aware learning function to efficiently estimate the probability of\nfailure of the underlying noise-free model. We showcase the effectiveness of\nthis methodology on standard benchmark functions and a finite element model of\na realistic structural frame.", "field": "Statistics", "categories": "stat.CO,stat.AP,stat.ME"}, {"arxiv_id": "2401.10811", "title": "Simulation Based Bayesian Optimization", "abstract": "Bayesian Optimization (BO) is a powerful method for optimizing black-box\nfunctions by combining prior knowledge with ongoing function evaluations. BO\nconstructs a probabilistic surrogate model of the objective function given the\ncovariates, which is in turn used to inform the selection of future evaluation\npoints through an acquisition function. For smooth continuous search spaces,\nGaussian Processes (GPs) are commonly used as the surrogate model as they offer\nanalytical access to posterior predictive distributions, thus facilitating the\ncomputation and optimization of acquisition functions. However, in complex\nscenarios involving optimizations over categorical or mixed covariate spaces,\nGPs may not be ideal.\n  This paper introduces Simulation Based Bayesian Optimization (SBBO) as a\nnovel approach to optimizing acquisition functions that only requires\n\\emph{sampling-based} access to posterior predictive distributions. SBBO allows\nthe use of surrogate probabilistic models tailored for combinatorial spaces\nwith discrete variables. Any Bayesian model in which posterior inference is\ncarried out through Markov chain Monte Carlo can be selected as the surrogate\nmodel in SBBO. In applications involving combinatorial optimization, we\ndemonstrate empirically the effectiveness of SBBO method using various choices\nof surrogate models.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.10824", "title": "The trivariate wrapped Cauchy copula -- a multi-purpose model for\n  angular data", "abstract": "In this paper, we will present a new flexible distribution for\nthree-dimensional angular data, or data on the three-dimensional torus. Our\ntrivariate wrapped Cauchy copula has the following benefits: (i) simple form of\ndensity, (ii) adjustable degree of dependence between every pair of variables,\n(iii) interpretable and well-estimable parameters, (iv) well-known conditional\ndistributions, (v) a simple data generating mechanism, (vi) unimodality.\nMoreover, our construction allows for linear marginals, implying that our\ncopula can also model cylindrical data. Parameter estimation via maximum\nlikelihood is explained, a comparison with the competitors in the existing\nliterature is given, and two real datasets are considered, one concerning\nprotein dihedral angles and another about data obtained by a buoy in the\nAdriatic Sea.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.10867", "title": "Learning Optimal Dynamic Treatment Regimes from Longitudinal Data", "abstract": "Studies often report estimates of the average treatment effect. While the ATE\nsummarizes the effect of a treatment on average, it does not provide any\ninformation about the effect of treatment within any individual. A treatment\nstrategy that uses an individual's information to tailor treatment to maximize\nbenefit is known as an optimal dynamic treatment rule. Treatment, however, is\ntypically not limited to a single point in time; consequently, learning an\noptimal rule for a time-varying treatment may involve not just learning the\nextent to which the comparative treatments' benefits vary across the\ncharacteristics of individuals, but also learning the extent to which the\ncomparative treatments' benefits vary as relevant circumstances evolve within\nan individual. The goal of this paper is to provide a tutorial for estimating\nODTR from longitudinal observational and clinical trial data for applied\nresearchers. We describe an approach that uses a doubly-robust unbiased\ntransformation of the conditional average treatment effect. We then learn a\ntime-varying ODTR for when to increase buprenorphine-naloxone dose to minimize\nreturn-to-regular-opioid-use among patients with opioid use disorder. Our\nanalysis highlights the utility of ODTRs in the context of sequential decision\nmaking: the learned ODTR outperforms a clinically defined strategy.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.10869", "title": "Variable selection for partially linear additive models", "abstract": "Among semiparametric regression models, partially linear additive models\nprovide a useful tool to include additive nonparametric components as well as a\nparametric component, when explaining the relationship between the response and\na set of explanatory variables. This paper concerns such models under sparsity\nassumptions for the covariates included in the linear component. Sparse\ncovariates are frequent in regression problems where the task of variable\nselection is usually of interest. As in other settings, outliers either in the\nresiduals or in the covariates involved in the linear component have a harmful\neffect. To simultaneously achieve model selection for the parametric component\nof the model and resistance to outliers, we combine preliminary robust\nestimators of the additive component, robust linear $MM-$regression estimators\nwith a penalty such as SCAD on the coefficients in the parametric part. Under\nmild assumptions, consistency results and rates of convergence for the proposed\nestimators are derived. A Monte Carlo study is carried out to compare, under\ndifferent models and contamination schemes, the performance of the robust\nproposal with its classical counterpart. The obtained results show the\nadvantage of using the robust approach. Through the analysis of a real data\nset, we also illustrate the benefits of the proposed procedure.", "field": "Statistics", "categories": "stat.ME,math.ST,stat.TH"}]}