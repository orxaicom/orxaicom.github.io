{"embeddings": [[23.612457275390625, -2.391362190246582], [22.128746032714844, -1.8591656684875488], [21.42101287841797, -2.071483612060547], [22.69620132446289, -1.30691659450531], [21.994361877441406, -1.300270676612854], [21.594316482543945, -2.8943188190460205], [22.33790397644043, -2.8359971046447754], [23.05487060546875, -3.2373476028442383], [24.213258743286133, -2.196542978286743], [20.998729705810547, -2.9363412857055664], [22.69192123413086, -2.1375620365142822], [23.4487361907959, -3.639099359512329], [23.808259963989258, -2.9175565242767334], [23.5078067779541, -1.7416324615478516], [22.203563690185547, -3.497727155685425], [20.902402877807617, -1.62052321434021], [20.81342887878418, -2.298891544342041]], "keys": ["2401.08642", "2401.08691", "2401.08702", "2401.08735", "2401.08915", "2401.08941", "2401.08975", "2401.08978", "2401.09009", "2401.09028", "2401.09144", "2401.09184", "2401.09339", "2401.09346", "2401.09379", "2401.09381", "2401.09401"], "additional_info": [{"arxiv_id": "2401.08642", "title": "Prediction problem for continuous time stochastic processes with\n  periodically correlated increments observed with noise", "abstract": "We propose solution of the problem of the mean square optimal estimation of\nlinear functionals which depend on the unobserved values of a continuous time\nstochastic process with periodically correlated increments based on\nobservations of this process with periodically stationary noise. To solve the\nproblem, we transform the processes to the sequences of stochastic functions\nwhich form an infinite dimensional vector stationary sequences. In the case of\nknown spectral densities of these sequences, we obtain formulas for calculating\nvalues of the mean square errors and the spectral characteristics of the\noptimal estimates of the functionals. Formulas determining the least favorable\nspectral densities and the minimax (robust) spectral characteristics of the\noptimal linear estimates of functionals are derived in the case where the sets\nof admissible spectral densities are given.", "field": "Statistics", "categories": "math.ST,stat.TH,60G10, 60G25, 60G35, 62M20, 62P20, 93E10, 93E11"}, {"arxiv_id": "2401.08691", "title": "Towards Responsible AI in Banking: Addressing Bias for Fair\n  Decision-Making", "abstract": "In an era characterized by the pervasive integration of artificial\nintelligence into decision-making processes across diverse industries, the\ndemand for trust has never been more pronounced. This thesis embarks on a\ncomprehensive exploration of bias and fairness, with a particular emphasis on\ntheir ramifications within the banking sector, where AI-driven decisions bear\nsubstantial societal consequences. In this context, the seamless integration of\nfairness, explainability, and human oversight is of utmost importance,\nculminating in the establishment of what is commonly referred to as\n\"Responsible AI\". This emphasizes the critical nature of addressing biases\nwithin the development of a corporate culture that aligns seamlessly with both\nAI regulations and universal human rights standards, particularly in the realm\nof automated decision-making systems. Nowadays, embedding ethical principles\ninto the development, training, and deployment of AI models is crucial for\ncompliance with forthcoming European regulations and for promoting societal\ngood. This thesis is structured around three fundamental pillars: understanding\nbias, mitigating bias, and accounting for bias. These contributions are\nvalidated through their practical application in real-world scenarios, in\ncollaboration with Intesa Sanpaolo. This collaborative effort not only\ncontributes to our understanding of fairness but also provides practical tools\nfor the responsible implementation of AI-based decision-making systems. In line\nwith open-source principles, we have released Bias On Demand and FairView as\naccessible Python packages, further promoting progress in the field of AI\nfairness.", "field": "Statistics", "categories": "stat.ML,cs.CY,cs.LG,stat.AP"}, {"arxiv_id": "2401.08702", "title": "Do We Really Even Need Data?", "abstract": "As artificial intelligence and machine learning tools become more accessible,\nand scientists face new obstacles to data collection (e.g. rising costs,\ndeclining survey response rates), researchers increasingly use predictions from\npre-trained algorithms as outcome variables. Though appealing for financial and\nlogistical reasons, using standard tools for inference can misrepresent the\nassociation between independent variables and the outcome of interest when the\ntrue, unobserved outcome is replaced by a predicted value. In this paper, we\ncharacterize the statistical challenges inherent to this so-called\n``post-prediction inference'' problem and elucidate three potential sources of\nerror: (i) the relationship between predicted outcomes and their true,\nunobserved counterparts, (ii) robustness of the machine learning model to\nresampling or uncertainty about the training data, and (iii) appropriately\npropagating not just bias but also uncertainty from predictions into the\nultimate inference procedure. We also contrast the framework for\npost-prediction inference with classical work spanning several related fields,\nincluding survey sampling, missing data, and semi-supervised learning. This\ncontrast elucidates the role of design in both classical and modern inference\nproblems.", "field": "Statistics", "categories": "stat.ME,cs.LG"}, {"arxiv_id": "2401.08735", "title": "A Framework for Scalable Ambient Air Pollution Concentration Estimation", "abstract": "Ambient air pollution remains a critical issue in the United Kingdom, where\ndata on air pollution concentrations form the foundation for interventions\naimed at improving air quality. However, the current air pollution monitoring\nstation network in the UK is characterized by spatial sparsity, heterogeneous\nplacement, and frequent temporal data gaps, often due to issues such as power\noutages. We introduce a scalable data-driven supervised machine learning model\nframework designed to address temporal and spatial data gaps by filling missing\nmeasurements. This approach provides a comprehensive dataset for England\nthroughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning\ntechniques and real-world data from the sparsely distributed monitoring\nstations, we generate 355,827 synthetic monitoring stations across the study\narea, yielding data valued at approximately \\pounds70 billion. Validation was\nconducted to assess the model's performance in forecasting, estimating missing\nlocations, and capturing peak concentrations. The resulting dataset is of\nparticular interest to a diverse range of stakeholders engaged in downstream\nassessments supported by outdoor air pollution concentration data for NO2, O3,\nPM10, PM2.5, and SO2. This resource empowers stakeholders to conduct studies at\na higher resolution than was previously possible.", "field": "Statistics", "categories": "stat.AP,cs.LG"}, {"arxiv_id": "2401.08915", "title": "How do transportation professionals perceive the impacts of AI\n  applications in transportation? A latent class cluster analysis", "abstract": "Recent years have witnessed an increasing number of artificial intelligence\n(AI) applications in transportation. As a new and emerging technology, AI's\npotential to advance transportation goals and the full extent of its impacts on\nthe transportation sector is not yet well understood. As the transportation\ncommunity explores these topics, it is critical to understand how\ntransportation professionals, the driving force behind AI Transportation\napplications, perceive AI's potential efficiency and equity impacts. Toward\nthis goal, we surveyed transportation professionals in the United States and\ncollected a total of 354 responses. Based on the survey responses, we conducted\nboth descriptive analysis and latent class cluster analysis (LCCA). The former\nprovides an overview of prevalent attitudes among transportation professionals,\nwhile the latter allows the identification of distinct segments based on their\nlatent attitudes toward AI. We find widespread optimism regarding AI's\npotential to improve many aspects of transportation (e.g., efficiency, cost\nreduction, and traveler experience); however, responses are mixed regarding\nAI's potential to advance equity. Moreover, many respondents are concerned that\nAI ethics are not well understood in the transportation community and that AI\nuse in transportation could exaggerate existing inequalities. Through LCCA, we\nhave identified four latent segments: AI Neutral, AI Optimist, AI Pessimist,\nand AI Skeptic. The latent class membership is significantly associated with\nrespondents' age, education level, and AI knowledge level. Overall, the study\nresults shed light on the extent to which the transportation community as a\nwhole is ready to leverage AI systems to transform current practices and inform\ntargeted education to improve the understanding of AI among transportation\nprofessionals.", "field": "Statistics", "categories": "stat.AP,cs.CY"}, {"arxiv_id": "2401.08941", "title": "A Powerful and Precise Feature-level Filter using Group Knockoffs", "abstract": "Selecting important features that have substantial effects on the response\nwith provable type-I error rate control is a fundamental concern in statistics,\nwith wide-ranging practical applications. Existing knockoff filters, although\nshown to provide theoretical guarantee on false discovery rate (FDR) control,\noften struggle to strike a balance between high power and precision in\npinpointing important features when there exist large groups of strongly\ncorrelated features. To address this challenge, we develop a new filter using\ngroup knockoffs to achieve both powerful and precise selection of important\nfeatures. Via experiments of simulated data and analysis of a real Alzheimer's\ndisease genetic dataset, it is found that the proposed filter can not only\ncontrol the proportion of false discoveries but also identify important\nfeatures with comparable power and greater precision than the existing group\nknockoffs filter.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.08975", "title": "Nonparametric Mean and Variance Adaptive Classification Rule for\n  High-Dimensional Data with Heteroscedastic Variances", "abstract": "In this study, we introduce an innovative methodology aimed at enhancing\nFisher's Linear Discriminant Analysis (LDA) in the context of high-dimensional\ndata classification scenarios, specifically addressing situations where each\nfeature exhibits distinct variances. Our approach leverages Nonparametric\nMaximum Likelihood Estimation (NPMLE) techniques to estimate both the mean and\nvariance parameters. By accommodating varying variances among features, our\nproposed method leads to notable improvements in classification performance. In\nparticular, unlike numerous prior studies that assume the distribution of\nheterogeneous variances follows a right-skewed inverse gamma distribution, our\nproposed method demonstrates excellent performance even when the distribution\nof heterogeneous variances takes on left-skewed, symmetric, or right-skewed\nforms. We conducted a series of rigorous experiments to empirically validate\nthe effectiveness of our approach. The results of these experiments demonstrate\nthat our proposed methodology excels in accurately classifying high-dimensional\ndata characterized by heterogeneous variances.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.08978", "title": "Trade-off Between Dependence and Complexity for Nonparametric Learning\n  -- an Empirical Process Approach", "abstract": "Empirical process theory for i.i.d. observations has emerged as a ubiquitous\ntool for understanding the generalization properties of various statistical\nproblems. However, in many applications where the data exhibit temporal\ndependencies (e.g., in finance, medical imaging, weather forecasting etc.), the\ncorresponding empirical processes are much less understood. Motivated by this\nobservation, we present a general bound on the expected supremum of empirical\nprocesses under standard $\\beta/\\rho$-mixing assumptions. Unlike most prior\nwork, our results cover both the long and the short-range regimes of\ndependence. Our main result shows that a non-trivial trade-off between the\ncomplexity of the underlying function class and the dependence among the\nobservations characterizes the learning rate in a large class of nonparametric\nproblems. This trade-off reveals a new phenomenon, namely that even under\nlong-range dependence, it is possible to attain the same rates as in the i.i.d.\nsetting, provided the underlying function class is complex enough. We\ndemonstrate the practical implications of our findings by analyzing various\nstatistical estimators in both fixed and growing dimensions. Our main examples\ninclude a comprehensive case study of generalization error bounds in\nnonparametric regression over smoothness classes in fixed as well as growing\ndimension using neural nets, shape-restricted multivariate convex regression,\nestimating the optimal transport (Wasserstein) distance between two probability\ndistributions, and classification under the Mammen-Tsybakov margin condition --\nall under appropriate mixing assumptions. In the process, we also develop\nbounds on $L_r$ ($1\\le r\\le 2$)-localized empirical processes with dependent\nobservations, which we then leverage to get faster rates for (a) tuning-free\nadaptation, and (b) set-structured learning problems.", "field": "Statistics", "categories": "math.ST,stat.ML,stat.TH,Primary 37A25, 62G05, Secondary 62M10, 60E15"}, {"arxiv_id": "2401.09009", "title": "Estimation of Tsallis entropy for exponentially distributed several\n  populations", "abstract": "We study the estimation of Tsallis entropy of a finite number of independent\npopulations, each following an exponential distribution with the same scale\nparameter and distinct location parameters for $q>0$. We derive a Stein-type\nimproved estimate, establishing the inadmissibility of the best affine\nequivariant estimate of the parameter function. A class of smooth estimates\nutilizing the Brewster technique is obtained, resulting in a significant\nimprovement in the risk value. We computed the Brewster-Zidek estimates for\nboth one and two populations, to illustrate the comparison with best affine\nequivariant and Stein-type estimates. We further derive that the Bayesian\nestimate, employing an inverse gamma prior, which takes the best affine\nequivariant estimate as a particular case. We provide a numerical illustration\nutilizing simulated samples for a single population. The purpose is to\ndemonstrate the impact of sample size, location parameter, and entropic index\non the estimates.", "field": "Statistics", "categories": "math.ST,stat.TH"}, {"arxiv_id": "2401.09028", "title": "A Novel Interpretable Fusion Analytic Framework for Investigating\n  Functional Brain Connectivity Differences in Cognitive Impairments", "abstract": "Functional magnetic resonance imaging (fMRI) data is characterized by its\ncomplexity and high--dimensionality, encompassing signals from various regions\nof interests (ROIs) that exhibit intricate correlations. Analyzing fMRI data\ndirectly proves challenging due to its intricate structure. Nevertheless, ROIs\nconvey crucial information about brain activities through their connections,\noffering insights into distinctive brain activity characteristics between\ndifferent groups. To address this, we propose a cutting-edge interpretable\nfusion analytic framework that facilitates the identification and understanding\nof ROI connectivity disparities between two groups, thereby revealing their\nunique features. Our novel approach encompasses three key steps. Firstly, we\nconstruct ROI functional connectivity networks (FCNs) to effectively manage\nfMRI data. Secondly, employing the FCNs, we utilize a self--attention deep\nlearning model for binary classification, generating an attention distribution\nthat encodes group differences. Lastly, we employ a latent space item-response\nmodel to extract group representative ROI features, visualizing these features\non the group summary FCNs. We validate the effectiveness of our framework by\nanalyzing four types of cognitive impairments, showcasing its capability to\nidentify significant ROIs contributing to the differences between the two\ndisease groups. This novel interpretable fusion analytic framework holds\nimmense potential for advancing our understanding of cognitive impairments and\ncould pave the way for more targeted therapeutic interventions.", "field": "Statistics", "categories": "stat.AP"}, {"arxiv_id": "2401.09144", "title": "Monitoring Machine Learning Forecasts for Platform Data Streams", "abstract": "Data stream forecasts are essential inputs for decision making at digital\nplatforms. Machine learning algorithms are appealing candidates to produce such\nforecasts. Yet, digital platforms require a large-scale forecast framework that\ncan flexibly respond to sudden performance drops. Re-training ML algorithms at\nthe same speed as new data batches enter is usually computationally too costly.\nOn the other hand, infrequent re-training requires specifying the re-training\nfrequency and typically comes with a severe cost of forecast deterioration. To\nensure accurate and stable forecasts, we propose a simple data-driven\nmonitoring procedure to answer the question when the ML algorithm should be\nre-trained. Instead of investigating instability of the data streams, we test\nif the incoming streaming forecast loss batch differs from a well-defined\nreference batch. Using a novel dataset constituting 15-min frequency data\nstreams from an on-demand logistics platform operating in London, we apply the\nmonitoring procedure to popular ML algorithms including random forest, XGBoost\nand lasso. We show that monitor-based re-training produces accurate forecasts\ncompared to viable benchmarks while preserving computational feasibility.\nMoreover, the choice of monitoring procedure is more important than the choice\nof ML algorithm, thereby permitting practitioners to combine the proposed\nmonitoring procedure with one's favorite forecasting algorithm.", "field": "Statistics", "categories": "stat.AP,stat.ML"}, {"arxiv_id": "2401.09184", "title": "A Two-Scale Complexity Measure for Deep Learning Models", "abstract": "We introduce a novel capacity measure 2sED for statistical models based on\nthe effective dimension. The new quantity provably bounds the generalization\nerror under mild assumptions on the model. Furthermore, simulations on standard\ndata sets and popular model architectures show that 2sED correlates well with\nthe training error. For Markovian models, we show how to efficiently\napproximate 2sED from below through a layerwise iterative approach, which\nallows us to tackle deep learning models with a large number of parameters.\nSimulation results suggest that the approximation is good for different\nprominent models and data sets.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.09339", "title": "Central Limit Theorem for Two-Timescale Stochastic Approximation with\n  Markovian Noise: Theory and Applications", "abstract": "Two-timescale stochastic approximation (TTSA) is among the most general\nframeworks for iterative stochastic algorithms. This includes well-known\nstochastic optimization methods such as SGD variants and those designed for\nbilevel or minimax problems, as well as reinforcement learning like the family\nof gradient-based temporal difference (GTD) algorithms. In this paper, we\nconduct an in-depth asymptotic analysis of TTSA under controlled Markovian\nnoise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA\ninfluenced by the underlying Markov chain, which has not been addressed by\nprevious CLT results of TTSA only with Martingale difference noise. Building\nupon our CLT, we expand its application horizon of efficient sampling\nstrategies from vanilla SGD to a wider TTSA context in distributed learning,\nthus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT\nresult to deduce the statistical properties of GTD algorithms with nonlinear\nfunction approximation using Markovian samples and show their identical\nasymptotic performance, a perspective not evident from current finite-time\nbounds.", "field": "Statistics", "categories": "stat.ML,cs.LG,math.OC"}, {"arxiv_id": "2401.09346", "title": "High Confidence Level Inference is Almost Free using Parallel Stochastic\n  Optimization", "abstract": "Uncertainty quantification for estimation through stochastic optimization\nsolutions in an online setting has gained popularity recently. This paper\nintroduces a novel inference method focused on constructing confidence\nintervals with efficient computation and fast convergence to the nominal level.\nSpecifically, we propose to use a small number of independent multi-runs to\nacquire distribution information and construct a t-based confidence interval.\nOur method requires minimal additional computation and memory beyond the\nstandard updating of estimates, making the inference process almost cost-free.\nWe provide a rigorous theoretical guarantee for the confidence interval,\ndemonstrating that the coverage is approximately exact with an explicit\nconvergence rate and allowing for high confidence level inference. In\nparticular, a new Gaussian approximation result is developed for the online\nestimators to characterize the coverage properties of our confidence intervals\nin terms of relative errors. Additionally, our method also allows for\nleveraging parallel computing to further accelerate calculations using multiple\ncores. It is easy to implement and can be integrated with existing stochastic\nalgorithms without the need for complicated modifications.", "field": "Statistics", "categories": "stat.ML,cs.LG"}, {"arxiv_id": "2401.09379", "title": "Merging uncertainty sets via majority vote", "abstract": "Given $K$ uncertainty sets that are arbitrarily dependent -- for example,\nconfidence intervals for an unknown parameter obtained with $K$ different\nestimators, or prediction sets obtained via conformal prediction based on $K$\ndifferent algorithms on shared data -- we address the question of how to\nefficiently combine them in a black-box manner to produce a single uncertainty\nset. We present a simple and broadly applicable majority vote procedure that\nproduces a merged set with nearly the same error guarantee as the input sets.\nWe then extend this core idea in a few ways: we show that weighted averaging\ncan be a powerful way to incorporate prior information, and a simple\nrandomization trick produces strictly smaller merged sets without altering the\ncoverage guarantee. Along the way, we prove an intriguing result that R\\\"uger's\ncombination rules (eg: twice the median of dependent p-values is a p-value) can\nbe strictly improved with randomization. When deployed in online settings, we\nshow how the exponential weighted majority algorithm can be employed in order\nto learn a good weighting over time. We then combine this method with adaptive\nconformal inference to deliver a simple conformal online model aggregation\n(COMA) method for nonexchangeable data.", "field": "Statistics", "categories": "stat.ME"}, {"arxiv_id": "2401.09381", "title": "Modelling clusters in network time series with an application to\n  presidential elections in the USA", "abstract": "Network time series are becoming increasingly relevant in the study of\ndynamic processes characterised by a known or inferred underlying network\nstructure. Generalised Network Autoregressive (GNAR) models provide a\nparsimonious framework for exploiting the underlying network, even in the\nhigh-dimensional setting. We extend the GNAR framework by introducing the\n$\\textit{community}$-$\\alpha$ GNAR model that exploits prior knowledge and/or\nexogenous variables for identifying and modelling dynamic interactions across\ncommunities in the underlying network. We further analyse the dynamics of\n$\\textit{Red, Blue}$ and $\\textit{Swing}$ states throughout presidential\nelections in the USA. Our analysis shows that dynamics differ among the\nstate-wise clusters.", "field": "Statistics", "categories": "stat.ME,stat.AP,62M10, 62P10"}, {"arxiv_id": "2401.09401", "title": "PERMUTOOLS: A MATLAB Package for Multivariate Permutation Testing", "abstract": "Statistical hypothesis testing and effect size measurement are routine parts\nof quantitative research. Advancements in computer processing power have\ngreatly improved the capability of statistical inference through the\navailability of resampling methods. However, many of the statistical practices\nused today are based on traditional, parametric methods that rely on\nassumptions about the underlying population. These assumptions may not always\nbe valid, leading to inaccurate results and misleading interpretations.\nPermutation testing, on the other hand, generates the sampling distribution\nempirically by permuting the observed data, providing distribution-free\nhypothesis testing. Furthermore, this approach lends itself to a powerful\nmethod for multiple comparison correction - known as max correction - which is\nless prone to type II errors than conventional correction methods. Parametric\nmethods have also traditionally been utilized for estimating the confidence\ninterval of various test statistics and effect size measures. However, these\ntoo can be estimated empirically using permutation or bootstrapping techniques.\nWhilst resampling methods are generally considered preferable, many popular\nprogramming languages and statistical software packages lack efficient\nimplementations. Here, we introduce PERMUTOOLS, a MATLAB package for\nmultivariate permutation testing and effect size measurement.", "field": "Statistics", "categories": "stat.ME,q-bio.QM,stat.CO"}]}