{"embeddings": [[15.621382713317871, 11.154074668884277], [15.816493034362793, 11.4685697555542], [15.358053207397461, 10.696805953979492], [16.108461380004883, 10.976778984069824], [13.230367660522461, 12.269624710083008], [13.789108276367188, 11.935131072998047], [15.503626823425293, 11.866011619567871], [16.510147094726562, 8.72782039642334], [15.936856269836426, 10.55573558807373], [16.216838836669922, 12.396333694458008], [16.361202239990234, 12.486787796020508], [16.36620330810547, 12.287811279296875], [14.766615867614746, 10.886679649353027], [14.723870277404785, 11.400971412658691], [14.788095474243164, 10.721933364868164], [14.304425239562988, 10.40871810913086], [12.850563049316406, 11.62231159210205], [16.055395126342773, 10.563982963562012], [15.548789024353027, 9.27962875366211], [15.753654479980469, 9.5244779586792], [16.851327896118164, 10.636336326599121], [17.993227005004883, 10.661276817321777], [12.618861198425293, 11.612649917602539], [16.097442626953125, 10.299263000488281], [14.677430152893066, 12.606690406799316], [12.74781322479248, 12.049263954162598], [15.06293773651123, 9.795234680175781], [16.086639404296875, 9.95876693725586], [14.048263549804688, 11.190381050109863], [16.476789474487305, 10.574499130249023], [13.670961380004883, 10.483515739440918], [15.031370162963867, 12.320679664611816], [13.172847747802734, 11.601622581481934], [16.69994354248047, 11.097465515136719], [13.88962459564209, 10.568255424499512], [14.281784057617188, 8.349021911621094], [14.387160301208496, 7.99936056137085], [18.354637145996094, 10.520763397216797], [12.562539100646973, 11.695931434631348], [12.962722778320312, 11.80823802947998], [15.15955638885498, 10.387681007385254], [15.521801948547363, 7.870575428009033], [16.217601776123047, 10.025564193725586], [16.82780647277832, 11.136728286743164], [13.684439659118652, 8.99842357635498], [13.713750839233398, 11.48856258392334], [17.740615844726562, 11.437560081481934], [16.82886505126953, 10.325281143188477], [17.479141235351562, 10.991735458374023], [16.879728317260742, 10.93712043762207], [14.542197227478027, 12.147229194641113], [17.51146125793457, 11.369117736816406], [14.469841003417969, 11.751462936401367], [14.503519058227539, 12.540756225585938], [16.918601989746094, 10.562759399414062], [13.950389862060547, 11.811201095581055], [13.308079719543457, 11.376890182495117], [16.464733123779297, 11.149565696716309], [18.02043342590332, 10.80152416229248], [14.017291069030762, 10.63084888458252], [16.34918975830078, 8.362611770629883], [14.37054443359375, 12.521185874938965], [13.88625431060791, 12.047338485717773], [15.475095748901367, 10.80966567993164], [13.427165985107422, 11.57120418548584], [18.047338485717773, 11.037181854248047], [16.264511108398438, 10.996567726135254], [16.923967361450195, 9.097188949584961], [14.011507034301758, 8.159156799316406], [15.380131721496582, 9.681092262268066], [15.01057243347168, 8.90778923034668], [14.73080062866211, 9.457528114318848], [15.79371452331543, 8.557312965393066], [16.68098258972168, 10.645903587341309], [13.214888572692871, 10.538341522216797], [17.878232955932617, 10.176468849182129], [15.016176223754883, 10.5626802444458], [14.128825187683105, 8.319599151611328], [12.739469528198242, 11.296948432922363], [15.280098915100098, 12.021611213684082], [16.46991539001465, 10.210883140563965], [16.62983512878418, 8.771970748901367], [18.268857955932617, 10.188484191894531], [16.197195053100586, 8.984268188476562], [18.38499641418457, 10.21320915222168], [15.876214027404785, 10.296860694885254], [17.228639602661133, 10.85154914855957], [17.33005714416504, 10.752065658569336], [15.24854850769043, 10.310341835021973], [14.750478744506836, 10.497270584106445], [18.167970657348633, 10.485979080200195], [17.029613494873047, 9.661319732666016], [12.643534660339355, 11.993695259094238], [17.583904266357422, 9.98287296295166], [17.9470272064209, 11.299745559692383], [13.953241348266602, 10.89472770690918], [17.692859649658203, 10.234139442443848], [14.247447967529297, 9.418607711791992], [16.55152130126953, 10.555245399475098], [14.023283004760742, 9.581332206726074], [15.098062515258789, 8.659100532531738], [16.161590576171875, 10.988981246948242], [13.269617080688477, 11.576546669006348], [15.0556640625, 8.285661697387695], [16.843124389648438, 11.91396427154541], [13.302160263061523, 10.311546325683594], [15.904970169067383, 7.893375396728516], [15.487065315246582, 11.813801765441895], [13.888589859008789, 11.353830337524414], [14.37635326385498, 8.588420867919922], [13.746996879577637, 12.039355278015137], [14.778539657592773, 8.621196746826172], [13.395233154296875, 9.817931175231934], [15.034675598144531, 8.609817504882812], [14.247770309448242, 8.261370658874512], [15.435004234313965, 8.953493118286133], [15.110954284667969, 8.147985458374023], [15.581934928894043, 10.21142578125], [18.358789443969727, 10.409029960632324], [13.215487480163574, 10.48173999786377], [15.87906551361084, 7.92711877822876], [15.538324356079102, 7.901020050048828], [15.35752010345459, 10.729903221130371], [17.063552856445312, 9.81872844696045], [12.472943305969238, 11.754003524780273], [13.59777545928955, 11.128296852111816], [18.0477294921875, 10.91817569732666], [16.231821060180664, 8.597037315368652], [15.390843391418457, 8.117913246154785], [16.182838439941406, 11.483851432800293], [17.313961029052734, 10.904253005981445], [17.72240447998047, 10.998129844665527], [13.602734565734863, 11.75539493560791], [12.659465789794922, 11.644021034240723], [15.571206092834473, 11.170524597167969], [17.651079177856445, 10.929913520812988], [18.129331588745117, 10.984956741333008], [12.89610481262207, 10.607539176940918], [13.876036643981934, 11.082643508911133], [17.965808868408203, 10.56484603881836], [17.421106338500977, 10.399020195007324], [17.815641403198242, 10.105866432189941], [14.314474105834961, 8.28307819366455], [15.543387413024902, 9.703615188598633], [15.173685073852539, 11.238457679748535], [14.978649139404297, 9.649073600769043], [14.180232048034668, 8.053892135620117], [14.146889686584473, 10.141481399536133], [14.92967700958252, 8.347853660583496], [15.669779777526855, 8.531045913696289], [15.11330509185791, 9.486736297607422], [17.63080596923828, 10.758036613464355], [17.81263542175293, 10.502031326293945], [15.358335494995117, 11.664484977722168], [14.699630737304688, 7.906805515289307], [14.352144241333008, 10.979375839233398], [15.750665664672852, 10.008464813232422], [14.122798919677734, 9.839510917663574], [14.450961112976074, 12.508642196655273], [15.080925941467285, 10.519989013671875], [15.6600341796875, 7.697052955627441], [16.56557273864746, 11.10438346862793], [14.811235427856445, 9.46139144897461], [15.22028923034668, 8.640480995178223], [13.46495532989502, 12.256841659545898], [15.41757583618164, 10.456232070922852], [13.74661922454834, 9.235237121582031], [16.11285400390625, 11.918432235717773], [13.869651794433594, 10.524847030639648], [12.920462608337402, 11.009101867675781], [15.415952682495117, 11.391019821166992], [15.581660270690918, 9.101963996887207], [16.297929763793945, 11.013731002807617], [15.946114540100098, 9.4428129196167], [15.874061584472656, 8.623499870300293], [13.219034194946289, 12.336838722229004], [14.268651008605957, 11.725541114807129], [15.915032386779785, 12.486299514770508], [15.664656639099121, 8.910898208618164], [16.721031188964844, 9.550009727478027], [14.406063079833984, 10.142417907714844], [15.612269401550293, 7.775167942047119], [16.975496292114258, 9.596665382385254], [17.252269744873047, 9.628972053527832], [14.47036075592041, 8.348572731018066], [14.195821762084961, 11.085892677307129], [15.25977611541748, 8.557760238647461], [16.24285125732422, 12.602307319641113], [12.56424617767334, 11.453661918640137], [14.337141990661621, 10.048103332519531], [14.300931930541992, 8.436711311340332], [15.817009925842285, 10.258363723754883], [15.702848434448242, 8.29281997680664], [16.528886795043945, 11.2910737991333], [15.821356773376465, 9.583767890930176], [14.303391456604004, 10.335576057434082], [16.341753005981445, 8.548524856567383], [15.772258758544922, 11.631895065307617], [17.133575439453125, 9.81039810180664], [15.2047119140625, 11.797608375549316], [14.948890686035156, 10.652276039123535], [15.147239685058594, 7.972273826599121], [16.465124130249023, 9.68704891204834], [13.368494987487793, 10.541205406188965], [16.742340087890625, 8.68685245513916], [15.103897094726562, 8.325145721435547], [13.531120300292969, 10.706377029418945], [14.994856834411621, 12.123101234436035], [17.310592651367188, 11.74283504486084], [15.084024429321289, 11.672664642333984], [12.612751007080078, 12.201391220092773], [16.75205421447754, 8.840792655944824], [16.91547393798828, 10.021474838256836], [18.158525466918945, 10.525474548339844], [17.52113151550293, 11.510920524597168], [15.481527328491211, 10.77893352508545], [15.512439727783203, 10.348305702209473], [15.575047492980957, 9.709373474121094], [17.603666305541992, 9.790411949157715], [12.645854949951172, 12.063614845275879], [18.249616622924805, 10.597784042358398], [16.845867156982422, 10.250314712524414], [16.586763381958008, 9.768168449401855], [15.460659980773926, 12.746561050415039], [14.570062637329102, 9.713350296020508], [14.946812629699707, 9.202593803405762], [16.936479568481445, 10.74929141998291], [12.96833610534668, 11.890870094299316], [17.8399658203125, 11.329962730407715], [16.856340408325195, 12.02733039855957], [15.12980842590332, 8.766839027404785], [16.607332229614258, 11.298096656799316], [16.298969268798828, 10.362869262695312], [14.485618591308594, 9.022851943969727], [15.607343673706055, 7.646025657653809], [13.469213485717773, 10.956377983093262], [12.673394203186035, 11.828008651733398], [16.389421463012695, 9.219087600708008], [18.207889556884766, 10.182638168334961], [13.756004333496094, 10.35805606842041], [14.871058464050293, 11.290910720825195], [13.095819473266602, 10.200583457946777], [14.731950759887695, 12.751175880432129], [14.914511680603027, 11.997302055358887], [17.57994842529297, 11.625244140625], [17.6180362701416, 10.210447311401367], [16.53868865966797, 12.259361267089844], [15.345019340515137, 8.522163391113281], [16.18739891052246, 12.665571212768555], [17.032560348510742, 9.069125175476074], [13.253687858581543, 9.896464347839355], [17.84068489074707, 10.962329864501953], [17.012876510620117, 10.437185287475586], [13.45103931427002, 10.773460388183594], [15.672335624694824, 12.681839942932129], [13.09721565246582, 10.500113487243652], [13.571392059326172, 10.635000228881836], [13.937820434570312, 8.624008178710938], [14.298823356628418, 12.356210708618164], [14.021947860717773, 12.633673667907715], [15.983670234680176, 12.032118797302246], [16.167234420776367, 8.140447616577148], [12.594277381896973, 11.729331970214844], [15.285818099975586, 12.519917488098145], [14.089887619018555, 8.566402435302734], [13.5254487991333, 11.890690803527832], [14.619244575500488, 8.61435317993164], [14.88525104522705, 12.761821746826172], [15.540205955505371, 9.968218803405762], [15.010029792785645, 11.432548522949219], [13.620933532714844, 9.064682006835938], [13.04120922088623, 11.87845516204834], [17.451786041259766, 9.750202178955078], [15.416678428649902, 11.305058479309082], [14.552828788757324, 7.804127216339111], [17.000577926635742, 10.909841537475586], [17.1729793548584, 9.552974700927734], [17.543182373046875, 10.655427932739258], [17.840604782104492, 10.69172477722168], [13.885360717773438, 10.868850708007812], [17.64337730407715, 10.318849563598633], [15.330881118774414, 11.13511848449707], [15.594099998474121, 12.309341430664062], [15.366554260253906, 7.563791751861572], [14.58880615234375, 9.934545516967773], [13.755566596984863, 10.575540542602539], [14.339747428894043, 12.057723999023438], [15.199726104736328, 8.798100471496582], [15.207317352294922, 10.095388412475586], [15.821125030517578, 9.008269309997559], [14.528947830200195, 12.707174301147461], [15.274513244628906, 7.920266628265381], [17.613285064697266, 9.988195419311523], [14.303878784179688, 7.9268670082092285], [13.661702156066895, 9.171632766723633], [14.745097160339355, 9.383605003356934], [16.379070281982422, 8.629457473754883]], "keys": ["2401.13672", "2401.13677", "2401.13680", "2401.13689", "2401.13691", "2401.13692", "2401.13693", "2401.13697", "2401.13699", "2401.13700", "2401.13702", "2401.13704", "2401.13708", "2401.13712", "2401.13713", "2401.13714", "2401.13715", "2401.13716", "2401.13719", "2401.13721", "2401.13722", "2401.13726", "2401.13743", "2401.13744", "2401.13747", "2401.13748", "2401.13751", "2401.13752", "2401.13754", "2401.13756", "2401.13761", "2401.13770", "2401.13779", "2401.13782", "2401.13784", "2401.13785", "2401.13786", "2401.13789", "2401.13790", "2401.13792", "2401.13794", "2401.13795", "2401.13796", "2401.13799", "2401.13800", "2401.13801", "2401.13802", "2401.13803", "2401.13804", "2401.13805", "2401.13807", "2401.13810", "2401.13815", "2401.13819", "2401.13822", "2401.13823", "2401.13827", "2401.13832", "2401.13835", "2401.13836", "2401.13837", "2401.13839", "2401.13842", "2401.13843", "2401.13848", "2401.13849", "2401.13850", "2401.13851", "2401.13853", "2401.13854", "2401.13856", "2401.13858", "2401.13865", "2401.13867", "2401.13868", "2401.13870", "2401.13872", "2401.13877", "2401.13882", "2401.13883", "2401.13887", "2401.13888", "2401.13891", "2401.13898", "2401.13903", "2401.13904", "2401.13905", "2401.13907", "2401.13912", "2401.13913", "2401.13919", "2401.13920", "2401.13922", "2401.13923", "2401.13924", "2401.13926", "2401.13927", "2401.13928", "2401.13929", "2401.13931", "2401.13934", "2401.13935", "2401.13936", "2401.13937", "2401.13940", "2401.13941", "2401.13942", "2401.13945", "2401.13947", "2401.13950", "2401.13952", "2401.13956", "2401.13957", "2401.13961", "2401.13964", "2401.13965", "2401.13967", "2401.13968", "2401.13970", "2401.13973", "2401.13974", "2401.13976", "2401.13977", "2401.13979", "2401.13980", "2401.13985", "2401.13986", "2401.13987", "2401.13992", "2401.13996", "2401.14000", "2401.14003", "2401.14005", "2401.14008", "2401.14009", "2401.14010", "2401.14011", "2401.14013", "2401.14014", "2401.14016", "2401.14019", "2401.14021", "2401.14024", "2401.14027", "2401.14028", "2401.14031", "2401.14032", "2401.14033", "2401.14034", "2401.14036", "2401.14038", "2401.14040", "2401.14043", "2401.14047", "2401.14051", "2401.14055", "2401.14056", "2401.14057", "2401.14060", "2401.14065", "2401.14066", "2401.14067", "2401.14069", "2401.14074", "2401.14076", "2401.14077", "2401.14078", "2401.14079", "2401.14081", "2401.14085", "2401.14086", "2401.14088", "2401.14090", "2401.14093", "2401.14095", "2401.14098", "2401.14100", "2401.14106", "2401.14107", "2401.14109", "2401.14110", "2401.14111", "2401.14112", "2401.14113", "2401.14115", "2401.14117", "2401.14121", "2401.14126", "2401.14129", "2401.14131", "2401.14132", "2401.14135", "2401.14136", "2401.14141", "2401.14142", "2401.14147", "2401.14148", "2401.14149", "2401.14151", "2401.14153", "2401.14155", "2401.14159", "2401.14160", "2401.14163", "2401.14166", "2401.14168", "2401.14169", "2401.14174", "2401.14176", "2401.14183", "2401.14184", "2401.14185", "2401.14192", "2401.14194", "2401.14196", "2401.14199", "2401.14210", "2401.14211", "2401.14212", "2401.14214", "2401.14215", "2401.14226", "2401.14228", "2401.14231", "2401.14232", "2401.14236", "2401.14240", "2401.14241", "2401.14242", "2401.14244", "2401.14250", "2401.14252", "2401.14255", "2401.14256", "2401.14257", "2401.14263", "2401.14265", "2401.14267", "2401.14268", "2401.14270", "2401.14272", "2401.14276", "2401.14277", "2401.14278", "2401.14279", "2401.14280", "2401.14284", "2401.14285", "2401.14286", "2401.14289", "2401.14292", "2401.14295", "2401.14296", "2401.14297", "2401.14303", "2401.14304", "2401.14310", "2401.14314", "2401.14317", "2401.14319", "2401.14320", "2401.14322", "2401.14323", "2401.14324", "2401.14325", "2401.14332", "2401.14336", "2401.14341", "2401.14343", "2401.14347", "2401.14349", "2401.14350", "2401.14351", "2401.14352", "2401.14354", "2401.14360", "2401.14361", "2401.14362", "2401.14367", "2401.14371", "2401.14373", "2401.14375", "2401.14377", "2401.14379", "2401.14381", "2401.14382", "2401.14383", "2401.14387", "2401.14388", "2401.14391", "2401.14394", "2401.14398", "2401.14400", "2401.14401", "2401.14403", "2401.14404", "2401.14405"], "additional_info": [{"arxiv_id": "2401.13672", "title": "Transforming Agriculture with Intelligent Data Management and Insights", "abstract": "Modern agriculture faces grand challenges to meet increased demands for food,\nfuel, feed, and fiber with population growth under the constraints of climate\nchange and dwindling natural resources. Data innovation is urgently required to\nsecure and improve the productivity, sustainability, and resilience of our\nagroecosystems. As various sensors and Internet of Things (IoT) instrumentation\nbecome more available, affordable, reliable, and stable, it has become possible\nto conduct data collection, integration, and analysis at multiple temporal and\nspatial scales, in real-time, and with high resolutions. At the same time, the\nsheer amount of data poses a great challenge to data storage and analysis, and\nthe \\textit{de facto} data management and analysis practices adopted by\nscientists have become increasingly inefficient. Additionally, the data\ngenerated from different disciplines, such as genomics, phenomics, environment,\nagronomy, and socioeconomic, can be highly heterogeneous. That is, datasets\nacross disciplines often do not share the same ontology, modality, or format.\nAll of the above make it necessary to design a new data management\ninfrastructure that implements the principles of Findable, Accessible,\nInteroperable, and Reusable (FAIR). In this paper, we propose Agriculture Data\nManagement and Analytics (ADMA), which satisfies the FAIR principles. Our new\ndata management infrastructure is intelligent by supporting semantic data\nmanagement across disciplines, interactive by providing various data\nmanagement/analysis portals such as web GUI, command line, and API, scalable by\nutilizing the power of high-performance computing (HPC), extensible by allowing\nusers to load their own data analysis tools, trackable by keeping track of\ndifferent operations on each file, and open by using a rich set of mature open\nsource technologies.", "field": "Computer Science", "categories": "cs.DB,cs.AI,cs.IR"}, {"arxiv_id": "2401.13677", "title": "Process Mining for Unstructured Data: Challenges and Research Directions", "abstract": "The application of process mining for unstructured data might significantly\nelevate novel insights into disciplines where unstructured data is a common\ndata format. To efficiently analyze unstructured data by process mining and to\nconvey confidence into the analysis result, requires bridging multiple\nchallenges. The purpose of this paper is to discuss these challenges, present\ninitial solutions and describe future research directions. We hope that this\narticle lays the foundations for future collaboration on this topic.", "field": "Computer Science", "categories": "cs.DB,cs.AI,cs.LG"}, {"arxiv_id": "2401.13680", "title": "A parallel algorithm for automated labeling of large time series", "abstract": "This article presents the PaSTiLa algorithm for automated labeling of large\ntime series on a cluster with GPUs. The method automatically selects snippet\nlength values based on the new proposed criterion and allows to search for\npatterns with high performance. Experiments showed high accuracy of pattern\nsearch and the advantage of the method compared to analogues.", "field": "Computer Science", "categories": "cs.DB"}, {"arxiv_id": "2401.13689", "title": "Trusting AI in High-stake Decision Making", "abstract": "The use of artificial intelligence models has recently grown common; we may\nuse them to write lines of code for us, summarize readings, draft emails, or\neven illustrate images. But when it comes to important decisions we need to\nmake, such as choosing between job offers or implementing certain economic\npolicies, our level of confidence and trust in AI falls. This raises an\nintriguing point of exploration which I tackle in this paper - What would need\nto happen for people to trust artificial intelligence for important decisions?\nIn this paper, I elaborate on how trust in AI for high-stake decisions would be\naccomplished if the technology was anthropomorphized because its\nanthropomorphism would overcome psychological barriers that are necessary to\novercome for us to trust AI for important decisions.", "field": "Computer Science", "categories": "cs.HC,I.2.0"}, {"arxiv_id": "2401.13691", "title": "PQCMC: Post-Quantum Cryptography McEliece-Chen Implicit Certificate\n  Scheme", "abstract": "In recent years, the elliptic curve Qu-Vanstone (ECQV) implicit certificate\nscheme has found application in security credential management systems (SCMS)\nand secure vehicle-to-everything (V2X) communication to issue pseudonymous\ncertificates. However, the vulnerability of elliptic-curve cryptography (ECC)\nto polynomial-time attacks posed by quantum computing raises concerns. In order\nto enhance resistance against quantum computing threats, various post-quantum\ncryptography methods have been adopted as standard (e.g. Dilithium) or\ncandidate standard methods (e.g. McEliece cryptography), but state of the art\nhas proven to be challenging to implement implicit certificates using\nlattice-based cryptography methods. Therefore, this study proposes a\npost-quantum cryptography McEliece-Chen (PQCMC) based on an efficient random\ninvertible matrix generation method to issue pseudonymous certificates with\nless computation time. The study provides mathematical models to validate the\nkey expansion process for implicit certificates. Furthermore, comprehensive\nsecurity evaluations and discussions are conducted to demonstrate that distinct\nimplicit certificates can be linked to the same end entity. In experiments, a\ncomparison is conducted between the certificate length and computation time to\nevaluate the performance of the proposed PQCMC. This study demonstrates the\nviability of the implicit certificate scheme based on PQC as a means of\ncountering quantum computing threats.", "field": "Computer Science", "categories": "cs.CR,cs.NI"}, {"arxiv_id": "2401.13692", "title": "Local Privacy-preserving Mechanisms and Applications in Machine Learning", "abstract": "The emergence and evolution of Local Differential Privacy (LDP) and its\nvarious adaptations play a pivotal role in tackling privacy issues related to\nthe vast amounts of data generated by intelligent devices, which are crucial\nfor data-informed decision-making in the realm of crowdsensing. Utilizing these\nextensive datasets can provide critical insights but also introduces\nsubstantial privacy concerns for the individuals involved. LDP, noted for its\ndecentralized framework, excels in providing strong privacy protection for\nindividual users during the stages of data collection and processing. The core\nprinciple of LDP lies in its technique of altering each user's data locally at\nthe client end before it is sent to the server, thus preventing privacy\nviolations at both stages. There are many LDP variances in the privacy research\ncommunity aimed to improve the utility-privacy tradeoff. On the other hand, one\nof the major applications of the privacy-preserving mechanisms is machine\nlearning. In this paper, we firstly delves into a comprehensive analysis of LDP\nand its variances, focusing on their various models, the diverse range of its\nadaptations, and the underlying structure of privacy mechanisms; then we\ndiscuss the state-of-art privacy mechanisms applications in machine learning.", "field": "Computer Science", "categories": "cs.CR"}, {"arxiv_id": "2401.13693", "title": "Challenge design roadmap", "abstract": "Challenges can be seen as a type of game that motivates participants to solve\nserious tasks. As a result, competition organizers must develop effective game\nrules. However, these rules have multiple objectives beyond making the game\nenjoyable for participants. These objectives may include solving real-world\nproblems, advancing scientific or technical areas, making scientific\ndiscoveries, and educating the public. In many ways, creating a challenge is\nsimilar to launching a product. It requires the same level of excitement and\nrigorous testing, and the goal is to attract ''customers'' in the form of\nparticipants. The process begins with a solid plan, such as a competition\nproposal that will eventually be submitted to an international conference and\nsubjected to peer review. Although peer review does not guarantee quality, it\ndoes force organizers to consider the impact of their challenge, identify\npotential oversights, and generally improve its quality. This chapter provides\nguidelines for creating a strong plan for a challenge. The material draws on\nthe preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and\nTailor 3 , as well as the NeurIPS proposal template, which some of the authors\ncontributed to.", "field": "Computer Science", "categories": "cs.OH,cs.AI,cs.HC"}, {"arxiv_id": "2401.13697", "title": "Toward Robust Multimodal Learning using Multimodal Foundational Models", "abstract": "Existing multimodal sentiment analysis tasks are highly rely on the\nassumption that the training and test sets are complete multimodal data, while\nthis assumption can be difficult to hold: the multimodal data are often\nincomplete in real-world scenarios. Therefore, a robust multimodal model in\nscenarios with randomly missing modalities is highly preferred. Recently,\nCLIP-based multimodal foundational models have demonstrated impressive\nperformance on numerous multimodal tasks by learning the aligned cross-modal\nsemantics of image and text pairs, but the multimodal foundational models are\nalso unable to directly address scenarios involving modality absence. To\nalleviate this issue, we propose a simple and effective framework, namely TRML,\nToward Robust Multimodal Learning using Multimodal Foundational Models. TRML\nemploys generated virtual modalities to replace missing modalities, and aligns\nthe semantic spaces between the generated and missing modalities. Concretely,\nwe design a missing modality inference module to generate virtual modaliites\nand replace missing modalities. We also design a semantic matching learning\nmodule to align semantic spaces generated and missing modalities. Under the\nprompt of complete modality, our model captures the semantics of missing\nmodalities by leveraging the aligned cross-modal semantic space. Experiments\ndemonstrate the superiority of our approach on three multimodal sentiment\nanalysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.CL"}, {"arxiv_id": "2401.13699", "title": "Generative AI-Driven Human Digital Twin in IoT-Healthcare: A\n  Comprehensive Survey", "abstract": "The Internet of things (IoT) can significantly enhance the quality of human\nlife, specifically in healthcare, attracting extensive attentions to\nIoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as\nan innovative paradigm that can comprehensively characterize the replication of\nthe individual human body in the digital world and reflect its physical status\nin real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the\napplication of healthcare monitoring by acting as a versatile and vivid human\ndigital testbed, simulating the outcomes and guiding the practical treatments.\nHowever, successfully establishing HDT requires high-fidelity virtual modeling\nand strong information interactions but possibly with scarce, biased and noisy\ndata. Fortunately, a recent popular technology called generative artificial\nintelligence (GAI) may be a promising solution because it can leverage advanced\nAI algorithms to automatically create, manipulate, and modify valuable while\ndiverse data. This survey particularly focuses on the implementation of\nGAI-driven HDT in IoT-healthcare. We start by introducing the background of\nIoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the\nfundamental techniques and present the overall framework of GAI-driven HDT.\nAfter that, we explore the realization of GAI-driven HDT in detail, including\nGAI-enabled data acquisition, communication, data management, digital modeling,\nand data analysis. Besides, we discuss typical IoT-healthcare applications that\ncan be revolutionized by GAI-driven HDT, namely personalized health monitoring\nand diagnosis, personalized prescription, and personalized rehabilitation.\nFinally, we conclude this survey by highlighting some future research\ndirections.", "field": "Computer Science", "categories": "cs.HC,cs.AI,cs.LG"}, {"arxiv_id": "2401.13700", "title": "Towards Automated Readable Proofs of Ruler and Compass Constructions", "abstract": "Although there are several systems that successfully generate construction\nsteps for ruler and compass construction problems, none of them provides\nreadable synthetic correctness proofs for generated constructions. In the\npresent work, we demonstrate how our triangle construction solver ArgoTriCS can\ncooperate with automated theorem provers for first order logic and coherent\nlogic so that it generates construction correctness proofs, that are both\nhuman-readable and formal (can be checked by interactive theorem provers such\nas Coq or Isabelle/HOL). These proofs currently rely on many high-level lemmas\nand our goal is to have them all formally shown from the basic axioms of\ngeometry.", "field": "Computer Science", "categories": "cs.LO,cs.AI,F.4.1"}, {"arxiv_id": "2401.13702", "title": "Open Source Prover in the Attic", "abstract": "The well known JGEX program became open source a few years ago, but\nseemingly, further development of the program can only be done without the\noriginal authors. In our project, we are looking at whether it is possible to\ncontinue such a large project as a newcomer without the involvement of the\noriginal authors. Is there a way to internationalize, fix bugs, improve the\ncode base, add new features? In other words, to save a relic found in the attic\nand polish it into a useful everyday tool.", "field": "Computer Science", "categories": "cs.PL,cs.MS,cs.SC,cs.SE"}, {"arxiv_id": "2401.13704", "title": "Using Java Geometry Expert as Guide in the Preparations for Math\n  Contests", "abstract": "We give an insight into Java Geometry Expert (JGEX) in use in a school\ncontext, focusing on the Austrian school system. JGEX can offer great support\nin some classroom situations, especially for solving mathematical competition\ntasks. Also, we discuss some limitations of the program.", "field": "Computer Science", "categories": "cs.CY,cs.AI,cs.CG,cs.SC"}, {"arxiv_id": "2401.13708", "title": "Accelerating hyperbolic t-SNE", "abstract": "The need to understand the structure of hierarchical or high-dimensional data\nis present in a variety of fields. Hyperbolic spaces have proven to be an\nimportant tool for embedding computations and analysis tasks as their\nnon-linear nature lends itself well to tree or graph data. Subsequently, they\nhave also been used in the visualization of high-dimensional data, where they\nexhibit increased embedding performance. However, none of the existing\ndimensionality reduction methods for embedding into hyperbolic spaces scale\nwell with the size of the input data. That is because the embeddings are\ncomputed via iterative optimization schemes and the computation cost of every\niteration is quadratic in the size of the input. Furthermore, due to the\nnon-linear nature of hyperbolic spaces, Euclidean acceleration structures\ncannot directly be translated to the hyperbolic setting. This paper introduces\nthe first acceleration structure for hyperbolic embeddings, building upon a\npolar quadtree. We compare our approach with existing methods and demonstrate\nthat it computes embeddings of similar quality in significantly less time.\nImplementation and scripts for the experiments can be found at\nhttps://graphics.tudelft.nl/accelerating-hyperbolic-tsne.", "field": "Computer Science", "categories": "cs.HC,cs.AI,cs.LG,q-bio.QM,stat.ML"}, {"arxiv_id": "2401.13712", "title": "Engineering Yeast Cells to Facilitate Information Exchange", "abstract": "Although continuous advances in theoretical modelling of Molecular\nCommunications (MC) are observed, there is still an insuperable gap between\ntheory and experimental testbeds, especially at the microscale. In this paper,\nthe development of the first testbed incorporating engineered yeast cells is\nreported. Different from the existing literature, eukaryotic yeast cells are\nconsidered for both the sender and the receiver, with {\\alpha}-factor molecules\nfacilitating the information transfer. The use of such cells is motivated\nmainly by the well understood biological mechanism of yeast mating, together\nwith their genetic amenability. In addition, recent advances in yeast\nbiosensing establish yeast as a suitable detector and a neat interface to\nin-body sensor networks. The system under consideration is presented first, and\nthe mathematical models of the underlying biological processes leading to an\nend-to-end (E2E) system are given. The experimental setup is then described and\nused to obtain experimental results which validate the developed mathematical\nmodels. Beyond that, the ability of the system to effectively generate output\npulses in response to repeated stimuli is demonstrated, reporting one event per\ntwo hours. However, fast RNA fluctuations indicate cell responses in less than\nthree minutes, demonstrating the potential for much higher rates in the future.", "field": "Computer Science", "categories": "cs.ET,cs.IT,math.IT,q-bio.MN,94-08, 94-05"}, {"arxiv_id": "2401.13713", "title": "EMP: Effective Multidimensional Persistence for Graph Representation\n  Learning", "abstract": "Topological data analysis (TDA) is gaining prominence across a wide spectrum\nof machine learning tasks that spans from manifold learning to graph\nclassification. A pivotal technique within TDA is persistent homology (PH),\nwhich furnishes an exclusive topological imprint of data by tracing the\nevolution of latent structures as a scale parameter changes. Present PH tools\nare confined to analyzing data through a single filter parameter. However, many\nscenarios necessitate the consideration of multiple relevant parameters to\nattain finer insights into the data. We address this issue by introducing the\nEffective Multidimensional Persistence (EMP) framework. This framework empowers\nthe exploration of data by simultaneously varying multiple scale parameters.\nThe framework integrates descriptor functions into the analysis process,\nyielding a highly expressive data summary. It seamlessly integrates established\nsingle PH summaries into multidimensional counterparts like EMP Landscapes,\nSilhouettes, Images, and Surfaces. These summaries represent data's\nmultidimensional aspects as matrices and arrays, aligning effectively with\ndiverse ML models. We provide theoretical guarantees and stability proofs for\nEMP summaries. We demonstrate EMP's utility in graph classification tasks,\nshowing its effectiveness. Results reveal that EMP enhances various single PH\ndescriptors, outperforming cutting-edge methods on multiple benchmark datasets.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CG"}, {"arxiv_id": "2401.13714", "title": "Value-Driven Mixed-Precision Quantization for Patch-Based Inference on\n  Microcontrollers", "abstract": "Deploying neural networks on microcontroller units (MCUs) presents\nsubstantial challenges due to their constrained computation and memory\nresources. Previous researches have explored patch-based inference as a\nstrategy to conserve memory without sacrificing model accuracy. However, this\ntechnique suffers from severe redundant computation overhead, leading to a\nsubstantial increase in execution latency. A feasible solution to address this\nissue is mixed-precision quantization, but it faces the challenges of accuracy\ndegradation and a time-consuming search time. In this paper, we propose\nQuantMCU, a novel patch-based inference method that utilizes value-driven\nmixed-precision quantization to reduce redundant computation. We first utilize\nvalue-driven patch classification (VDPC) to maintain the model accuracy. VDPC\nclassifies patches into two classes based on whether they contain outlier\nvalues. For patches containing outlier values, we apply 8-bit quantization to\nthe feature maps on the dataflow branches that follow. In addition, for patches\nwithout outlier values, we utilize value-driven quantization search (VDQS) on\nthe feature maps of their following dataflow branches to reduce search time.\nSpecifically, VDQS introduces a novel quantization search metric that takes\ninto account both computation and accuracy, and it employs entropy as an\naccuracy representation to avoid additional training. VDQS also adopts an\niterative approach to determine the bitwidth of each feature map to further\naccelerate the search process. Experimental results on real-world MCU devices\nshow that QuantMCU can reduce computation by 2.2x on average while maintaining\ncomparable model accuracy compared to the state-of-the-art patch-based\ninference methods.", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.13715", "title": "A tabu search-based LED selection approach safeguarding visible light\n  communication systems", "abstract": "In this paper, we investigate the secrecy performance of a single-input\nsingle-output visible light communication (VLC) channel in the presence of an\neavesdropper. The studied VLC system comprises distributed light-emitting\ndiodes (LEDs) and multiple randomly located users (UEs) within an indoor\nenvironment. A sum secrecy rate maximization problem is formulated to enhance\nconfidential transmission by selecting the optimal LED for each UE. To address\nthe non-convex and non-continuous nature of this problem, we propose a tabu\nsearch-based algorithm that prevents entrapment in local optima by organizing\nthe trial vectors from previous iterations. Furthermore, we develop three\nstraightforward LED selection strategies that reduce computational complexity\nby employing fixed criteria to choose one LED for each UE. We also examine the\nconvergence and complexity analysis of the proposed algorithm and strategies.\nThe results demonstrate that the secrecy performance of our proposed algorithm\nis very close to the global optimal value and surpasses that of the developed\nstrategies.", "field": "Computer Science", "categories": "cs.CR,cs.ET"}, {"arxiv_id": "2401.13716", "title": "Can I trust my fake data -- A comprehensive quality assessment framework\n  for synthetic tabular data in healthcare", "abstract": "Ensuring safe adoption of AI tools in healthcare hinges on access to\nsufficient data for training, testing and validation. In response to privacy\nconcerns and regulatory requirements, using synthetic data has been suggested.\nSynthetic data is created by training a generator on real data to produce a\ndataset with similar statistical properties. Competing metrics with differing\ntaxonomies for quality evaluation have been suggested, resulting in a complex\nlandscape. Optimising quality entails balancing considerations that make the\ndata fit for use, yet relevant dimensions are left out of existing frameworks.\nWe performed a comprehensive literature review on the use of quality evaluation\nmetrics on SD within the scope of tabular healthcare data and SD made using\ndeep generative methods. Based on this and the collective team experiences, we\ndeveloped a conceptual framework for quality assurance. The applicability was\nbenchmarked against a practical case from the Dutch National Cancer Registry.\nWe present a conceptual framework for quality assurance of SD for AI\napplications in healthcare that aligns diverging taxonomies, expands on common\nquality dimensions to include the dimensions of Fairness and Carbon footprint,\nand proposes stages necessary to support real-life applications. Building trust\nin synthetic data by increasing transparency and reducing the safety risk will\naccelerate the development and uptake of trustworthy AI tools for the benefit\nof patients. Despite the growing emphasis on algorithmic fairness and carbon\nfootprint, these metrics were scarce in the literature review. The overwhelming\nfocus was on statistical similarity using distance metrics while sequential\nlogic detection was scarce. A consensus-backed framework that includes all\nrelevant quality dimensions can provide assurance for safe and responsible\nreal-life applications of SD.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13719", "title": "Inference Attacks Against Face Recognition Model without Classification\n  Layers", "abstract": "Face recognition (FR) has been applied to nearly every aspect of daily life,\nbut it is always accompanied by the underlying risk of leaking private\ninformation. At present, almost all attack models against FR rely heavily on\nthe presence of a classification layer. However, in practice, the FR model can\nobtain complex features of the input via the model backbone, and then compare\nit with the target for inference, which does not explicitly involve the outputs\nof the classification layer adopting logit or other losses. In this work, we\nadvocate a novel inference attack composed of two stages for practical FR\nmodels without a classification layer. The first stage is the membership\ninference attack. Specifically, We analyze the distances between the\nintermediate features and batch normalization (BN) parameters. The results\nindicate that this distance is a critical metric for membership inference. We\nthus design a simple but effective attack model that can determine whether a\nface image is from the training dataset or not. The second stage is the model\ninversion attack, where sensitive private data is reconstructed using a\npre-trained generative adversarial network (GAN) guided by the attack model in\nthe first stage. To the best of our knowledge, the proposed attack model is the\nvery first in the literature developed for FR models without a classification\nlayer. We illustrate the application of the proposed attack model in the\nestablishment of privacy-preserving FR techniques.", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.13721", "title": "Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in\n  Regression", "abstract": "Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model\nfrom a labeled source domain to an unlabeled target domain for regression\ntasks. Recent successful works in UDAR mostly focus on subspace alignment,\ninvolving the alignment of a selected subspace within the entire feature space.\nThis contrasts with the feature alignment methods used for classification,\nwhich aim at aligning the entire feature space and have proven effective but\nare less so in regression settings. Specifically, while classification aims to\nidentify separate clusters across the entire embedding dimension, regression\ninduces less structure in the data representation, necessitating additional\nguidance for efficient alignment. In this paper, we propose an effective method\nfor UDAR by incorporating guidance from uncertainty. Our approach serves a dual\npurpose: providing a measure of confidence in predictions and acting as a\nregularization of the embedding space. Specifically, we leverage the Deep\nEvidential Learning framework, which outputs both predictions and uncertainties\nfor each input sample. We propose aligning the parameters of higher-order\nevidential distributions between the source and target domains using\ntraditional alignment methods at the feature or posterior level. Additionally,\nwe propose to augment the feature space representation by mixing source samples\nwith pseudo-labeled target samples based on label similarity. This cross-domain\nmixing strategy produces more realistic samples than random mixing and\nintroduces higher uncertainty, facilitating further alignment. We demonstrate\nthe effectiveness of our approach on four benchmarks for UDAR, on which we\noutperform existing methods.", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.13722", "title": "Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion\n  Monitoring", "abstract": "This research project aims to tackle the growing mental health challenges in\ntoday's digital age. It employs a modified pre-trained BERT model to detect\ndepressive text within social media and users' web browsing data, achieving an\nimpressive 93% test accuracy. Simultaneously, the project aims to incorporate\nphysiological signals from wearable devices, such as smartwatches and EEG\nsensors, to provide long-term tracking and prognosis of mood disorders and\nemotional states. This comprehensive approach holds promise for enhancing early\ndetection of depression and advancing overall mental health outcomes.", "field": "Computer Science", "categories": "cs.HC,cs.AI"}, {"arxiv_id": "2401.13726", "title": "Supporting Sensemaking of Large Language Model Outputs at Scale", "abstract": "Large language models (LLMs) are capable of generating multiple responses to\na single prompt, yet little effort has been expended to help end-users or\nsystem designers make use of this capability. In this paper, we explore how to\npresent many LLM responses at once. We design five features, which include both\npre-existing and novel methods for computing similarities and differences\nacross textual documents, as well as how to render their outputs. We report on\na controlled user study (n=24) and eight case studies evaluating these features\nand how they support users in different tasks. We find that the features\nsupport a wide variety of sensemaking tasks and even make tasks previously\nconsidered to be too difficult by our participants now tractable. Finally, we\npresent design guidelines to inform future explorations of new LLM interfaces.", "field": "Computer Science", "categories": "cs.HC,cs.LG"}, {"arxiv_id": "2401.13743", "title": "Intermittency versus Path Loss in RIS-aided THz Communication: A Data\n  Significance Approach", "abstract": "The transition to Terahertz (THz) frequencies, providing an ultra-wide\nbandwidth, is a key driver for future wireless communication networks. However,\nthe specific properties of the THz channel, such as severe path loss and\nvulnerability to blockage, pose a significant challenge in balancing data rate\nand reliability. This work considers reconfigurable intelligent surface\n(RIS)-aided THz communication, where the effective exploitation of a strong,\nbut intermittent line-of-sight (LOS) path versus a reliable, yet weaker\nRIS-path is studied. We introduce a mixed-criticality superposition coding\nscheme that addresses this tradeoff from a data significance perspective. The\nresults show that the proposed scheme enables reliable transmission for a\nportion of high-criticality data without significantly impacting the overall\nachievable sum rate and queuing delay. Additionally, we gain insights into how\nthe LOS blockage probability and the channel gain of the RIS-link influence the\nrate performance of our scheme.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.13744", "title": "Conformal Prediction Sets Improve Human Decision Making", "abstract": "In response to everyday queries, humans explicitly signal uncertainty and\noffer alternative answers when they are unsure. Machine learning models that\noutput calibrated prediction sets through conformal prediction mimic this human\nbehaviour; larger sets signal greater uncertainty while providing alternatives.\nIn this work, we study the usefulness of conformal prediction sets as an aid\nfor human decision making by conducting a pre-registered randomized controlled\ntrial with conformal prediction sets provided to human subjects. With\nstatistical significance, we find that when humans are given conformal\nprediction sets their accuracy on tasks improves compared to fixed-size\nprediction sets with the same coverage guarantee. The results show that\nquantifying model uncertainty with conformal prediction is helpful for\nhuman-in-the-loop decision making and human-AI teams.", "field": "Computer Science", "categories": "cs.LG,cs.HC,stat.ML"}, {"arxiv_id": "2401.13747", "title": "Searching in trees with monotonic query times", "abstract": "We consider the following generalization of binary search in sorted arrays to\ntree domains. In each step of the search, an algorithm is querying a vertex\n$q$, and as a reply, it receives an answer, which either states that $q$ is the\ndesired target, or it gives the neighbor of $q$ that is closer to the target\nthan $q$. A further generalization assumes that a vertex-weight function\n$\\omega$ gives the query costs, i.e., the cost of querying $q$ is $\\omega(q)$.\nThe goal is to find an adaptive search strategy requiring the minimum cost in\nthe worst case. This problem is NP-complete for general weight functions and\none of the challenging open questions is whether there exists a polynomial-time\nconstant factor approximation algorithm for an arbitrary tree? In this work, we\nprove that there exist a constant-factor approximation algorithm for trees with\na monotonic cost function, i.e., when the tree has a vertex $v$ such that the\nweights of the subsequent vertices on the path from $v$ to any leaf give a\nmonotonic (non-increasing or non-decreasing) sequence $S$. This gives a\nconstant factor approximation algorithm for trees with cost functions such that\neach such sequence $S$ has a fixed number of monotonic segments. Finally, we\ncombine several earlier results to show that the problem is NP-complete when\nthe number of monotonic segments in $S$ is at least $4$.", "field": "Computer Science", "categories": "cs.DS,cs.DM,68R10, 68W25,G.2.2; F.2.2"}, {"arxiv_id": "2401.13748", "title": "Log-Log Domain Sum-Product Algorithm for Information Reconciliation in\n  Continuous-Variable Quantum Key Distribution", "abstract": "In this paper, we present a novel log-log domain sum-product algorithm (SPA)\nfor decoding low-density parity-check (LDPC) codes in continuous-variable\nquantum key distribution (CV-QKD) systems. This algorithm reduces the\nfractional bit width of decoder messages, leading to a smaller memory footprint\nand a lower resource consumption in hardware implementation. We also provide\npractical insights for fixed-point arithmetic and compare our algorithm with\nthe conventional SPA in terms of performance and complexity. Our results show\nthat our algorithm achieves comparable or better decoding accuracy than the\nconventional SPA while saving at least $25\\%$ of the fractional bit width.", "field": "Computer Science", "categories": "cs.IT,math.IT,quant-ph"}, {"arxiv_id": "2401.13751", "title": "A Systematic Approach to Robustness Modelling for Deep Convolutional\n  Neural Networks", "abstract": "Convolutional neural networks have shown to be widely applicable to a large\nnumber of fields when large amounts of labelled data are available. The recent\ntrend has been to use models with increasingly larger sets of tunable\nparameters to increase model accuracy, reduce model loss, or create more\nadversarially robust models -- goals that are often at odds with one another.\nIn particular, recent theoretical work raises questions about the ability for\neven larger models to generalize to data outside of the controlled train and\ntest sets. As such, we examine the role of the number of hidden layers in the\nResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a\nvariety of parameters including the size of the model, the floating point\nprecision, and the noise level of both the training data and the model output.\nTo encapsulate the model's predictive power and computational cost, we provide\na method that uses induced failures to model the probability of failure as a\nfunction of time and relate that to a novel metric that allows us to quickly\ndetermine whether or not the cost of training a model outweighs the cost of\nattacking it. Using this approach, we are able to approximate the expected\nfailure rate using a small number of specially crafted samples rather than\nincreasingly larger benchmark datasets. We demonstrate the efficacy of this\ntechnique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit\nfloating-point numbers, various data pre-processing techniques, and several\nattacks on five configurations of the ResNet model. Then, using empirical\nmeasurements, we examine the various trade-offs between cost, robustness,\nlatency, and reliability to find that larger models do not significantly aid in\nadversarial robustness despite costing significantly more to train.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CV,stat.ML"}, {"arxiv_id": "2401.13752", "title": "Explaining Image Classifiers", "abstract": "We focus on explaining image classifiers, taking the work of Mothilal et al.\n[2021] (MMTS) as our point of departure. We observe that, although MMTS claim\nto be using the definition of explanation proposed by Halpern [2016], they do\nnot quite do so. Roughly speaking, Halpern's definition has a necessity clause\nand a sufficiency clause. MMTS replace the necessity clause by a requirement\nthat, as we show, implies it. Halpern's definition also allows agents to\nrestrict the set of options considered. While these difference may seem minor,\nas we show, they can have a nontrivial impact on explanations. We also show\nthat, essentially without change, Halpern's definition can handle two issues\nthat have proved difficult for other approaches: explanations of absence (when,\nfor example, an image classifier for tumors outputs \"no tumor\") and\nexplanations of rare events (such as tumors).", "field": "Computer Science", "categories": "cs.AI"}, {"arxiv_id": "2401.13754", "title": "Multi-Function Multi-Way Analog Technology for Sustainable Machine\n  Intelligence Computation", "abstract": "Numerical computation is essential to many areas of artificial intelligence\n(AI), whose computing demands continue to grow dramatically, yet their\ncontinued scaling is jeopardized by the slowdown in Moore's law. Multi-function\nmulti-way analog (MFMWA) technology, a computing architecture comprising arrays\nof memristors supporting in-memory computation of matrix operations, can offer\ntremendous improvements in computation and energy, but at the expense of\ninherent unpredictability and noise. We devise novel randomized algorithms\ntailored to MFMWA architectures that mitigate the detrimental impact of\nimperfect analog computations while realizing their potential benefits across\nvarious areas of AI, such as applications in computer vision. Through analysis,\nmeasurements from analog devices, and simulations of larger systems, we\ndemonstrate orders of magnitude reduction in both computation and energy with\naccuracy similar to digital computers.", "field": "Computer Science", "categories": "math.NA,cs.ET,cs.NA,65F10, C3, G1,G.1.3"}, {"arxiv_id": "2401.13756", "title": "NLICE: Synthetic Medical Record Generation for Effective Primary\n  Healthcare Differential Diagnosis", "abstract": "This paper offers a systematic method for creating medical knowledge-grounded\npatient records for use in activities involving differential diagnosis.\nAdditionally, an assessment of machine learning models that can differentiate\nbetween various conditions based on given symptoms is also provided. We use a\npublic disease-symptom data source called SymCat in combination with Synthea to\nconstruct the patients records. In order to increase the expressive nature of\nthe synthetic data, we use a medically-standardized symptom modeling method\ncalled NLICE to augment the synthetic data with additional contextual\ninformation for each condition. In addition, Naive Bayes and Random Forest\nmodels are evaluated and compared on the synthetic data. The paper shows how to\nsuccessfully construct SymCat-based and NLICE-based datasets. We also show\nresults for the effectiveness of using the datasets to train predictive disease\nmodels. The SymCat-based dataset is able to train a Naive Bayes and Random\nForest model yielding a 58.8% and 57.1% Top-1 accuracy score, respectively. In\ncontrast, the NLICE-based dataset improves the results, with a Top-1 accuracy\nof 82.0% and Top-5 accuracy values of more than 90% for both models. Our\nproposed data generation approach solves a major barrier to the application of\nartificial intelligence methods in the healthcare domain. Our novel NLICE\nsymptom modeling approach addresses the incomplete and insufficient information\nproblem in the current binary symptom representation approach. The NLICE code\nis open sourced at https://github.com/guozhuoran918/NLICE.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13761", "title": "Experimental validation of ultra-shortened 3D finite element\n  electromagnetic modeling of three-core armored cables at power frequency", "abstract": "Due to recent advances, the numerical analysis of submarine three-core\narmored cables can nowadays be developed through the finite element method\n(FEM) in a small slice of the cable. This strongly reduces the computational\nburden and simulation time. However, the performance of this ultra-shortened\n3D-FEM model is still to be fully assessed with experimental measurements. This\npaper focuses on this validation for an extensive variety of situations through\nthe experimental measurements available in the specialized literature for up to\n10 actual cables. In particular, it deals not only with relevant calculations\nat power frequency, like the series resistance and inductive reactance or the\ninduced sheath current, but also with other aspects never analyzed before\nthrough 3D-FEM simulations, such as the zero sequence impedance, the magnetic\nfield distribution around the power cable, as well as side effects due to the\nnonlinear properties of the armor wires. All this considering different\narmoring and sheath bonding configurations. Results show a very good agreement\nbetween measured and computed values, presenting the ultra-shortened 3D-FEM\nmodel as a suitable tool for the analysis and design of three-core armored\ncables, and opening the possibility to reduce the need of extensive\nexperimental tests in the design stage of new cables.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13770", "title": "AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard\n  Combinatorial Problems", "abstract": "This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS)\nbased Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving\nchallenging combinatorial problems. Despite the tremendous success of CnC\nsolvers in solving a variety of hard combinatorial problems, the lookahead\ncubing techniques at the heart of CnC have not evolved much for many years.\nPart of the reason is the sheer difficulty of coming up with new cubing\ntechniques that are both low-cost and effective in partitioning input formulas\ninto sub-formulas, such that the overall runtime is minimized.\n  Lookahead cubing techniques used by current state-of-the-art CnC solvers,\nsuch as March, keep their cubing costs low by constraining the search for the\noptimal splitting variables. By contrast, our key innovation is a\ndeductively-driven MCTS-based lookahead cubing technique, that performs a\ndeeper heuristic search to find effective cubes, while keeping the cubing cost\nlow. We perform an extensive comparison of AlphaMapleSAT against the March CnC\nsolver on challenging combinatorial problems such as the minimum Kochen-Specker\nand Ramsey problems. We also perform ablation studies to verify the efficacy of\nthe MCTS heuristic search for the cubing problem. Results show up to 2.3x\nspeedup in parallel (and up to 27x in sequential) elapsed real time.", "field": "Computer Science", "categories": "cs.AI,math.CO"}, {"arxiv_id": "2401.13779", "title": "Faster Convergence with Less Communication: Broadcast-Based Subgraph\n  Sampling for Decentralized Learning over Wireless Networks", "abstract": "Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely\nadopted algorithm for decentralized training of machine learning models across\nnetworked agents. A crucial part of D-SGD is the consensus-based model\naveraging, which heavily relies on information exchange and fusion among the\nnodes. Specifically, for consensus averaging over wireless networks,\ncommunication coordination is necessary to determine when and how a node can\naccess the channel and transmit (or receive) information to (or from) its\nneighbors. In this work, we propose $\\texttt{BASS}$, a broadcast-based subgraph\nsampling method designed to accelerate the convergence of D-SGD while\nconsidering the actual communication cost per iteration. $\\texttt{BASS}$\ncreates a set of mixing matrix candidates that represent sparser subgraphs of\nthe base topology. In each consensus iteration, one mixing matrix is sampled,\nleading to a specific scheduling decision that activates multiple\ncollision-free subsets of nodes. The sampling occurs in a probabilistic manner,\nand the elements of the mixing matrices, along with their sampling\nprobabilities, are jointly optimized. Simulation results demonstrate that\n$\\texttt{BASS}$ enables faster convergence with fewer transmission slots\ncompared to existing link-based scheduling methods. In conclusion, the inherent\nbroadcasting nature of wireless channels offers intrinsic advantages in\naccelerating the convergence of decentralized optimization and learning.", "field": "Computer Science", "categories": "cs.IT,cs.DC,cs.LG,eess.SP,math.IT"}, {"arxiv_id": "2401.13782", "title": "Tweets to Citations: Unveiling the Impact of Social Media Influencers on\n  AI Research Visibility", "abstract": "As the number of accepted papers at AI and ML conferences reaches into the\nthousands, it has become unclear how researchers access and read research\npublications. In this paper, we investigate the role of social media\ninfluencers in enhancing the visibility of machine learning research,\nparticularly the citation counts of papers they share. We have compiled a\ncomprehensive dataset of over 8,000 papers, spanning tweets from December 2018\nto October 2023, alongside 1:1 matched controls based on publication year,\nvenue, and abstract topics. Our analysis reveals a significant increase in\ncitations for papers endorsed by these influencers, with median citation counts\n2-3 times higher than those of the control group. Additionally, the study\ndelves into the geographic, gender, and institutional diversity of highlighted\nauthors. These findings highlight the expanding influence of social media in\nscholarly communication and underscore the importance of an evolving ecosystem\nin today's digital academic landscape.", "field": "Computer Science", "categories": "cs.DL,cs.AI,cs.CL,cs.CV,cs.LG,cs.SI"}, {"arxiv_id": "2401.13784", "title": "On the Predictive Capability of Dynamic Mode Decomposition for Nonlinear\n  Periodic Systems with Focus on Orbital Mechanics", "abstract": "This paper discusses the predictive capability of Dynamic Mode Decomposition\n(DMD) in the context of orbital mechanics. The focus is specifically on the\nHankel variant of DMD which uses a stacked set of time-delayed observations for\nsystem identification and subsequent prediction. A theory on the minimum number\nof time delays required for accurate reconstruction of periodic trajectories of\nnonlinear systems is presented and corroborated using experimental analysis. In\naddition, the window size for training and prediction regions, respectively, is\npresented. The need for a meticulous approach while using DMD is emphasized by\ndrawing comparisons between its performance on two candidate satellites, the\nISS and MOLNIYA-3-50.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13785", "title": "S2TPVFormer: Spatio-Temporal Tri-Perspective View for temporally\n  coherent 3D Semantic Occupancy Prediction", "abstract": "Holistic understanding and reasoning in 3D scenes play a vital role in the\nsuccess of autonomous driving systems. The evolution of 3D semantic occupancy\nprediction as a pretraining task for autonomous driving and robotic downstream\ntasks captures finer 3D details compared to methods like 3D detection. Existing\napproaches predominantly focus on spatial cues, often overlooking temporal\ncues. Query-based methods tend to converge on computationally intensive Voxel\nrepresentation for encoding 3D scene information. This study introduces\nS2TPVFormer, an extension of TPVFormer, utilizing a spatiotemporal transformer\narchitecture for coherent 3D semantic occupancy prediction. Emphasizing the\nimportance of spatiotemporal cues in 3D scene perception, particularly in 3D\nsemantic occupancy prediction, our work explores the less-explored realm of\ntemporal cues. Leveraging Tri-Perspective View (TPV) representation, our\nspatiotemporal encoder generates temporally rich embeddings, improving\nprediction coherence while maintaining computational efficiency. To achieve\nthis, we propose a novel Temporal Cross-View Hybrid Attention (TCVHA)\nmechanism, facilitating effective spatiotemporal information exchange across\nTPV views. Experimental evaluations on the nuScenes dataset demonstrate a\nsubstantial 3.1% improvement in mean Intersection over Union (mIoU) for 3D\nSemantic Occupancy compared to TPVFormer, confirming the effectiveness of the\nproposed S2TPVFormer in enhancing 3D scene perception.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13786", "title": "FoVA-Depth: Field-of-View Agnostic Depth Estimation for Cross-Dataset\n  Generalization", "abstract": "Wide field-of-view (FoV) cameras efficiently capture large portions of the\nscene, which makes them attractive in multiple domains, such as automotive and\nrobotics. For such applications, estimating depth from multiple images is a\ncritical task, and therefore, a large amount of ground truth (GT) data is\navailable. Unfortunately, most of the GT data is for pinhole cameras, making it\nimpossible to properly train depth estimation models for large-FoV cameras. We\npropose the first method to train a stereo depth estimation model on the widely\navailable pinhole data, and to generalize it to data captured with larger FoVs.\nOur intuition is simple: We warp the training data to a canonical, large-FoV\nrepresentation and augment it to allow a single network to reason about diverse\ntypes of distortions that otherwise would prevent generalization. We show\nstrong generalization ability of our approach on both indoor and outdoor\ndatasets, which was not possible with previous methods.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13789", "title": "A Unified Approach to Emotion Detection and Task-Oriented Dialogue\n  Modeling", "abstract": "In current text-based task-oriented dialogue (TOD) systems, user emotion\ndetection (ED) is often overlooked or is typically treated as a separate and\nindependent task, requiring additional training. In contrast, our work\ndemonstrates that seamlessly unifying ED and TOD modeling brings about mutual\nbenefits, and is therefore an alternative to be considered. Our method consists\nin augmenting SimpleToD, an end-to-end TOD system, by extending belief state\ntracking to include ED, relying on a single language model. We evaluate our\napproach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ\nannotated with emotions. Our results reveal a general increase in performance\nfor ED and task results. Our findings also indicate that user emotions provide\nuseful contextual conditioning for system responses, and can be leveraged to\nfurther refine responses in terms of empathy.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13790", "title": "Orthogonal Time-Frequency-Space (OTFS) and Related Signaling", "abstract": "The principle of orthogonal time-frequency-space (OTFS) signaling is firstly\nanalyzed, followed by explaining that OTFS embeds another signaling scheme\nreferred to as orthogonal short-time Fourier (OSTF). Then, the relationship\namong OTFS, OSTF, orthogonal frequency-division multiplexing (OFDM) and\nsingle-carrier frequency-division multiple-access (SC-FDMA) is explored,\ndemonstrating that OSTF/OTFS are fundamentally the extensions of OFDM/SC-FDMA\nfrom one-dimensional (1D) signaling to two-dimensional (2D) signaling. Hence,\nthe characteristics and performance of OSTF/OTFS schemes can be perceived from\nthe well-understood OFDM/SC-FDMA schemes. Accordingly, the advantages and\ndisadvantages of OSTF/OTFS are discussed. Furthermore, from the principles of\nOFDM/SC-FDMA, the multiuser multiplexing in OSTF/OTFS systems is analyzed with\nrespect to uplink and downlink, respectively. Added on this, a range of\ngeneralized multiplexing schemes are presented, whose characteristics are\nbriefly analyzed.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.13792", "title": "Probabilistic Mobility Load Balancing for Multi-band 5G and Beyond\n  Networks", "abstract": "The ever-increasing demand for data services and the proliferation of user\nequipment (UE) have resulted in a significant rise in the volume of mobile\ntraffic. Moreover, in multi-band networks, non-uniform traffic distribution\namong different operational bands can lead to congestion, which can adversely\nimpact the user's quality of experience. Load balancing is a critical aspect of\nnetwork optimization, where it ensures that the traffic is evenly distributed\namong different bands, avoiding congestion and ensuring better user experience.\nTraditional load balancing approaches rely only on the band channel quality as\na load indicator and to move UEs between bands, which disregards the UE's\ndemands and the band resource, and hence, leading to a suboptimal balancing and\nutilization of resources. To address this challenge, we propose an event-based\nalgorithm, in which we model the load balancing problem as a multi-objective\nstochastic optimization, and assign UEs to bands in a probabilistic manner. The\ngoal is to evenly distribute traffic across available bands according to their\nresources, while maintaining minimal number of inter-frequency handovers to\navoid the signaling overhead and the interruption time. Simulation results show\nthat the proposed algorithm enhances the network's performance and outperforms\ntraditional load balancing approaches in terms of throughput and interruption\ntime.", "field": "Computer Science", "categories": "cs.NI"}, {"arxiv_id": "2401.13794", "title": "Traffic Pattern Classification in Smart Cities Using Deep Recurrent\n  Neural Network", "abstract": "This paper examines the use of deep recurrent neural networks to classify\ntraffic patterns in smart cities. We propose a novel approach to traffic\npattern classification based on deep recurrent neural networks, which can\neffectively capture traffic patterns' dynamic and sequential features. The\nproposed model combines convolutional and recurrent layers to extract features\nfrom traffic pattern data and a SoftMax layer to classify traffic patterns.\nExperimental results show that the proposed model outperforms existing methods\nregarding accuracy, precision, recall, and F1 score. Furthermore, we provide an\nin depth analysis of the results and discuss the implications of the proposed\nmodel for smart cities. The results show that the proposed model can accurately\nclassify traffic patterns in smart cities with a precision of as high as 95%.\nThe proposed model is evaluated on a real world traffic pattern dataset and\ncompared with existing classification methods.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13795", "title": "Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent\n  Diffusion Models for Virtual Try-All", "abstract": "As online shopping is growing, the ability for buyers to virtually visualize\nproducts in their settings-a phenomenon we define as \"Virtual Try-All\"-has\nbecome crucial. Recent diffusion models inherently contain a world model,\nrendering them suitable for this task within an inpainting context. However,\ntraditional image-conditioned diffusion models often fail to capture the\nfine-grained details of products. In contrast, personalization-driven models\nsuch as DreamPaint are good at preserving the item's details but they are not\noptimized for real-time applications. We present \"Diffuse to Choose,\" a novel\ndiffusion-based image-conditioned inpainting model that efficiently balances\nfast inference with the retention of high-fidelity details in a given reference\nitem while ensuring accurate semantic manipulations in the given scene content.\nOur approach is based on incorporating fine-grained features from the reference\nimage directly into the latent feature maps of the main diffusion model,\nalongside with a perceptual loss to further preserve the reference item's\ndetails. We conduct extensive testing on both in-house and publicly available\ndatasets, and show that Diffuse to Choose is superior to existing zero-shot\ndiffusion inpainting methods as well as few-shot diffusion personalization\nalgorithms like DreamPaint.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13796", "title": "Don't Push the Button! Exploring Data Leakage Risks in Machine Learning\n  and Transfer Learning", "abstract": "Machine Learning (ML) has revolutionized various domains, offering predictive\ncapabilities in several areas. However, with the increasing accessibility of ML\ntools, many practitioners, lacking deep ML expertise, adopt a \"push the button\"\napproach, utilizing user-friendly interfaces without a thorough understanding\nof underlying algorithms. While this approach provides convenience, it raises\nconcerns about the reliability of outcomes, leading to challenges such as\nincorrect performance evaluation. This paper addresses a critical issue in ML,\nknown as data leakage, where unintended information contaminates the training\ndata, impacting model performance evaluation. Users, due to a lack of\nunderstanding, may inadvertently overlook crucial steps, leading to optimistic\nperformance estimates that may not hold in real-world scenarios. The\ndiscrepancy between evaluated and actual performance on new data is a\nsignificant concern. In particular, this paper categorizes data leakage in ML,\ndiscussing how certain conditions can propagate through the ML workflow.\nFurthermore, it explores the connection between data leakage and the specific\ntask being addressed, investigates its occurrence in Transfer Learning, and\ncompares standard inductive ML with transductive ML frameworks. The conclusion\nsummarizes key findings, emphasizing the importance of addressing data leakage\nfor robust and reliable ML applications.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13799", "title": "Who Changed the Destiny of Rural Students, and How?: Unpacking\n  ICT-Mediated Remote Education in Rural China", "abstract": "The proliferation of Information and Communication Technologies (ICTs) has\nshown great promise in addressing educational challenges facing rural areas.\nHowever, the complex rural context poses significant challenges to the\neffective utilization of these technologies. This paper examines the empirical\nintegration of live-streaming-based remote classrooms (LSRC) through a\nqualitative study in rural China. Our findings suggest that while LSRC enables\nrural students equal access to high-quality educational resources, its\npractical integration faces numerous challenges. In particular, we emphasize\nthe crucial role of local teachers in addressing these challenges, ultimately\nachieving the desired improvement of students' learning outcomes. We also\nexamine the impact of LSRC on the original rural education ecosystem. Building\nupon our findings, we call for a reconsideration of interaction paradigms and\nevaluation systems of ICT-mediated rural education, emphasizing the\nsignificance of rural teachers. We conclude by discussing the implications for\nfuture ICT-mediated technology interventions in rural settings.", "field": "Computer Science", "categories": "cs.CY,cs.HC"}, {"arxiv_id": "2401.13800", "title": "Multi-Object Navigation in real environments using hybrid policies", "abstract": "Navigation has been classically solved in robotics through the combination of\nSLAM and planning. More recently, beyond waypoint planning, problems involving\nsignificant components of (visual) high-level reasoning have been explored in\nsimulated environments, mostly addressed with large-scale machine learning, in\nparticular RL, offline-RL or imitation learning. These methods require the\nagent to learn various skills like local planning, mapping objects and querying\nthe learned spatial representations. In contrast to simpler tasks like waypoint\nplanning (PointGoal), for these more complex tasks the current state-of-the-art\nmodels have been thoroughly evaluated in simulation but, to our best knowledge,\nnot yet in real environments.\n  In this work we focus on sim2real transfer. We target the challenging\nMulti-Object Navigation (Multi-ON) task and port it to a physical environment\ncontaining real replicas of the originally virtual Multi-ON objects. We\nintroduce a hybrid navigation method, which decomposes the problem into two\ndifferent skills: (1) waypoint navigation is addressed with classical SLAM\ncombined with a symbolic planner, whereas (2) exploration, semantic mapping and\ngoal retrieval are dealt with deep neural networks trained with a combination\nof supervised learning and RL. We show the advantages of this approach compared\nto end-to-end methods both in simulation and a real environment and outperform\nthe SOTA for this task.", "field": "Computer Science", "categories": "cs.RO,cs.AI"}, {"arxiv_id": "2401.13801", "title": "Exploring Adversarial Threat Models in Cyber Physical Battery Systems", "abstract": "Technological advancements like the Internet of Things (IoT) have facilitated\ndata exchange across various platforms. This data exchange across various\nplatforms has transformed the traditional battery system into a cyber physical\nsystem. Such connectivity makes modern cyber physical battery systems\nvulnerable to cyber threats where a cyber attacker can manipulate sensing and\nactuation signals to bring the battery system into an unsafe operating\ncondition. Hence, it is essential to build resilience in modern cyber physical\nbattery systems (CPBS) under cyber attacks. The first step of building such\nresilience is to analyze potential adversarial behavior, that is, how the\nadversaries can inject attacks into the battery systems. However, it has been\nfound that in this under-explored area of battery cyber physical security, such\nan adversarial threat model has not been studied in a systematic manner. In\nthis study, we address this gap and explore adversarial attack generation\npolicies based on optimal control framework. The framework is developed by\nperforming theoretical analysis, which is subsequently supported by evaluation\nwith experimental data generated from a commercial battery cell.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13802", "title": "Investigating the Efficacy of Large Language Models for Code Clone\n  Detection", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in various\nnatural language processing and software engineering tasks, such as code\ngeneration. The LLMs are mainly utilized in the prompt-based zero/few-shot\nparadigm to guide the model in accomplishing the task. %\\textbf{Goal:}\nGPT-based models are one of the popular ones studied for tasks such as code\ncomment generation or test generation. These tasks are `generative' tasks.\nHowever, there is limited research on the usage of LLMs for `non-generative'\ntasks such as classification using the prompt-based paradigm. In this\npreliminary exploratory study, we investigated the applicability of LLMs for\nCode Clone Detection (CCD), a non-generative task. %\\textbf{Method:} By\nbuilding a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we\nfirst investigated two different prompts using ChatGPT to detect\n\\textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a\nzero-shot setting. We \\textcolor{black}{then} conducted an analysis to\nunderstand the strengths and weaknesses of ChatGPT in CCD. %\\textbf{Results:}\nChatGPT surpasses the baselines in cross-language CCD\n\\textcolor{black}{attaining an F1-score of 0.877 } and achieves comparable\nperformance to fully fine-tuned models for mono-lingual CCD,\n\\textcolor{black}{with an F1-score of 0.878}. Also, the\n\\textcolor{black}{prompt and the} difficulty level of the problems has an\nimpact on the performance of ChatGPT. \\textcolor{black}{Finally,} we provide\ninsights and future directions based on our initial analysis", "field": "Computer Science", "categories": "cs.SE,cs.AI,cs.CL,cs.LG"}, {"arxiv_id": "2401.13803", "title": "Synergizing Human Expertise and AI Efficiency with Language Model for\n  Microscopy Operation and Automated Experiment Design", "abstract": "With the advent of large language models (LLMs), in both the open source and\nproprietary domains, attention is turning to how to exploit such artificial\nintelligence (AI) systems in assisting complex scientific tasks, such as\nmaterial synthesis, characterization, analysis and discovery. Here, we explore\nthe utility of LLM, particularly ChatGPT4, in combination with application\nprogram interfaces (APIs) in tasks of experimental design, programming\nworkflows, and data analysis in scanning probe microscopy, using both in-house\ndeveloped API and API given by a commercial vendor for instrument control. We\nfind that the LLM can be especially useful in converting ideations of\nexperimental workflows to executable code on microscope APIs. Beyond code\ngeneration, we find that the GPT4 is capable of analyzing microscopy images in\na generic sense. At the same time, we find that GPT4 suffers from inability to\nextend beyond basic analyses or more in-depth technical experimental design. We\nargue that a LLM specifically fine-tuned for individual scientific domains can\npotentially be a better language interface for converting scientific ideations\nfrom human experts to executable workflows, such a synergy between human\nexpertise and LLM efficiency in experimentation can open new door for\naccelerating scientific research, enabling effective experimental protocols\narchive and sharing in scientific community.", "field": "Computer Science", "categories": "cs.HC,cond-mat.mtrl-sci"}, {"arxiv_id": "2401.13804", "title": "Exploring Parent's Needs for Children-Centered AI to Support\n  Preschoolers' Storytelling and Reading Activities", "abstract": "Interactive storytelling is vital for preschooler development. While\nchildren's interactive partners have traditionally been their parents and\nteachers, recent advances in artificial intelligence (AI) have sparked a surge\nof AI-based storytelling technologies. As these technologies become\nincreasingly ubiquitous in preschoolers' lives, questions arise regarding how\nthey function in practical storytelling scenarios and, in particular, how\nparents, the most critical stakeholders, experience and perceive these\ntechnologies. This paper investigates these questions through a qualitative\nstudy with 17 parents of children aged 3-6. Our findings suggest that even\nthough AI-based storytelling technologies provide more immersive and engaging\ninteraction, they still cannot meet parents' expectations due to a series of\ninteractive, functional, and algorithmic challenges. We elaborate on these\nchallenges and discuss the possible implications of future AI-based\nstorytelling technologies for preschoolers. We conclude by highlighting the\ndesign implications for future AI-based storytelling technologies.", "field": "Computer Science", "categories": "cs.HC,cs.CY"}, {"arxiv_id": "2401.13805", "title": "Longitudinal Sentiment Topic Modelling of Reddit Posts", "abstract": "In this study, we analyze texts of Reddit posts written by students of four\nmajor Canadian universities. We gauge the emotional tone and uncover prevailing\nthemes and discussions through longitudinal topic modeling of posts textual\ndata. Our study focuses on four years, 2020-2023, covering COVID-19 pandemic\nand after pandemic years. Our results highlight a gradual uptick in discussions\nrelated to mental health.", "field": "Computer Science", "categories": "cs.SI,cs.IR,I.2.7"}, {"arxiv_id": "2401.13807", "title": "Depth-Optimal Addressing of 2D Qubit Array with 1D Controls Based on\n  Exact Binary Matrix Factorization", "abstract": "Reducing control complexity is essential for achieving large-scale quantum\ncomputing, particularly on platforms operating in cryogenic environments.\nWiring each qubit to a room-temperature control poses a challenge, as this\napproach would surpass the thermal budget in the foreseeable future. An\nessential tradeoff becomes evident: reducing control knobs compromises the\nability to independently address each qubit. Recent progress in neutral\natom-based platforms suggests that rectangular addressing may strike a balance\nbetween control granularity and flexibility for 2D qubit arrays. This scheme\nallows addressing qubits on the intersections of a set of rows and columns each\ntime. While quadratically reducing controls, it may necessitate more depth. We\nformulate the depth-optimal rectangular addressing problem as exact binary\nmatrix factorization, an NP-hard problem also appearing in communication\ncomplexity and combinatorial optimization. We introduce a satisfiability modulo\ntheories-based solver for this problem, and a heuristic, row packing,\nperforming close to the optimal solver on various benchmarks. Furthermore, we\ndiscuss rectangular addressing in the context of fault-tolerant quantum\ncomputing, leveraging a natural two-level structure.", "field": "Computer Science", "categories": "cs.ET,quant-ph"}, {"arxiv_id": "2401.13810", "title": "Automated Root Causing of Cloud Incidents using In-Context Learning with\n  GPT-4", "abstract": "Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis\nprocess for cloud services, requiring on-call engineers to identify the primary\nissues and implement corrective actions to prevent future recurrences.\nImproving the incident RCA process is vital for minimizing service downtime,\ncustomer impact and manual toil. Recent advances in artificial intelligence\nhave introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which\nhave proven effective in tackling various AIOps problems, ranging from code\nauthoring to incident management. Nonetheless, the GPT-4 model's immense size\npresents challenges when trying to fine-tune it on user data because of the\nsignificant GPU resource demand and the necessity for continuous model\nfine-tuning with the emergence of new data. To address the high cost of\nfine-tuning LLM, we propose an in-context learning approach for automated root\ncausing, which eliminates the need for fine-tuning. We conduct extensive study\nover 100,000 production incidents, comparing several large language models\nusing multiple metrics. The results reveal that our in-context learning\napproach outperforms the previous fine-tuned large language models such as\nGPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\%\nimprovement over the zero-shot model. Moreover, human evaluation involving\nactual incident owners demonstrates its superiority over the fine-tuned model,\nachieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in\nreadability. The impressive results demonstrate the viability of utilizing a\nvanilla GPT model for the RCA task, thereby avoiding the high computational and\nmaintenance costs associated with a fine-tuned model.", "field": "Computer Science", "categories": "cs.CL,cs.SE"}, {"arxiv_id": "2401.13815", "title": "SoK: Game-Theoretic Cybersecurity: Assumptions, Models, Gaps, and\n  Bridges", "abstract": "The discipline of game theory was introduced in the context of economics, and\nhas been applied to study cyber attacker and defender behaviors. While\nadaptions have been made to accommodate features in the cyber domain, these\nstudies are inherently limited by the root of game theory in economic systems\nwhere players (i.e., agents) may be selfish but not malicious. In this SoK, we\nsystematize the major cybersecurity problems that have been studied with the\ngame-theoretic approach, the assumptions that have been made, the models and\nsolution concepts that have been proposed. The systematization leads to a\ncharacterization of the technical gaps that must be addressed in order to make\ngame-theoretic cybersecurity models truly useful. We explore bridges to address\nthem.", "field": "Computer Science", "categories": "cs.GT,cs.CR"}, {"arxiv_id": "2401.13819", "title": "Separating $k$-Median from the Supplier Version", "abstract": "Given a metric space $(V, d)$ along with an integer $k$, the $k$-Median\nproblem asks to open $k$ centers $C \\subseteq V$ to minimize $\\sum_{v \\in V}\nd(v, C)$, where $d(v, C) := \\min_{c \\in C} d(v, c)$. While the best-known\napproximation ratio of $2.613$ holds for the more general supplier version\nwhere an additional set $F \\subseteq V$ is given with the restriction $C\n\\subseteq F$, the best known hardness for these two versions are $1+1/e \\approx\n1.36$ and $1+2/e \\approx 1.73$ respectively, using the same reduction from Max\n$k$-Coverage. We prove the following two results separating them.\n  First, we show a $1.546$-parameterized approximation algorithm that runs in\ntime $f(k) n^{O(1)}$. Since $1+2/e$ is proved to be the optimal approximation\nratio for the supplier version in the parameterized setting, this result\nseparates the original $k$-Median from the supplier version.\n  Next, we prove a $1.416$-hardness for polynomial-time algorithms assuming the\nUnique Games Conjecture. This is achieved via a new fine-grained hardness of\nMax-$k$-Coverage for small set sizes.\n  Our upper bound and lower bound are derived from almost the same expression,\nwith the only difference coming from the well-known separation between the\npowers of LP and SDP on (hypergraph) vertex cover.", "field": "Computer Science", "categories": "cs.DS"}, {"arxiv_id": "2401.13822", "title": "Navigating Dataset Documentations in AI: A Large-Scale Analysis of\n  Dataset Cards on Hugging Face", "abstract": "Advances in machine learning are closely tied to the creation of datasets.\nWhile data documentation is widely recognized as essential to the reliability,\nreproducibility, and transparency of ML, we lack a systematic empirical\nunderstanding of current dataset documentation practices. To shed light on this\nquestion, here we take Hugging Face -- one of the largest platforms for sharing\nand collaborating on ML models and datasets -- as a prominent case study. By\nanalyzing all 7,433 dataset documentation on Hugging Face, our investigation\nprovides an overview of the Hugging Face dataset ecosystem and insights into\ndataset documentation practices, yielding 5 main findings: (1) The dataset card\ncompletion rate shows marked heterogeneity correlated with dataset popularity.\n(2) A granular examination of each section within the dataset card reveals that\nthe practitioners seem to prioritize Dataset Description and Dataset Structure\nsections, while the Considerations for Using the Data section receives the\nlowest proportion of content. (3) By analyzing the subsections within each\nsection and utilizing topic modeling to identify key topics, we uncover what is\ndiscussed in each section, and underscore significant themes encompassing both\ntechnical and social impacts, as well as limitations within the Considerations\nfor Using the Data section. (4) Our findings also highlight the need for\nimproved accessibility and reproducibility of datasets in the Usage sections.\n(5) In addition, our human annotation evaluation emphasizes the pivotal role of\ncomprehensive dataset content in shaping individuals' perceptions of a dataset\ncard's overall quality. Overall, our study offers a unique perspective on\nanalyzing dataset documentation through large-scale data science analysis and\nunderlines the need for more thorough dataset documentation in machine learning\nresearch.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13823", "title": "Robustness in Fairness against Edge-level Perturbations in GNN-based\n  Recommendation", "abstract": "Efforts in the recommendation community are shifting from the sole emphasis\non utility to considering beyond-utility factors, such as fairness and\nrobustness. Robustness of recommendation models is typically linked to their\nability to maintain the original utility when subjected to attacks. Limited\nresearch has explored the robustness of a recommendation model in terms of\nfairness, e.g., the parity in performance across groups, under attack\nscenarios. In this paper, we aim to assess the robustness of graph-based\nrecommender systems concerning fairness, when exposed to attacks based on\nedge-level perturbations. To this end, we considered four different fairness\noperationalizations, including both consumer and provider perspectives.\nExperiments on three datasets shed light on the impact of perturbations on the\ntargeted fairness notion, uncovering key shortcomings in existing evaluation\nprotocols for robustness. As an example, we observed perturbations affect\nconsumer fairness on a higher extent than provider fairness, with alarming\nunfairness for the former. Source code:\nhttps://github.com/jackmedda/CPFairRobust", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13827", "title": "Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink\n  in Markovian IoT Models", "abstract": "The age of information (AoI) is used to measure the freshness of the data. In\nIoT networks, the traditional resource management schemes rely on a message\nexchange between the devices and the base station (BS) before communication\nwhich causes high AoI, high energy consumption, and low reliability. Unmanned\naerial vehicles (UAVs) as flying BSs have many advantages in minimizing the\nAoI, energy-saving, and throughput improvement. In this paper, we present a\nnovel learning-based framework that estimates the traffic arrival of IoT\ndevices based on Markovian events. The learning proceeds to optimize the\ntrajectory of multiple UAVs and their scheduling policy. First, the BS predicts\nthe future traffic of the devices. We compare two traffic predictors: the\nforward algorithm (FA) and the long short-term memory (LSTM). Afterward, we\npropose a deep reinforcement learning (DRL) approach to optimize the optimal\npolicy of each UAV. Finally, we manipulate the optimum reward function for the\nproposed DRL approach. Simulation results show that the proposed algorithm\noutperforms the random-walk (RW) baseline model regarding the AoI, scheduling\naccuracy, and transmission power.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.NI"}, {"arxiv_id": "2401.13832", "title": "Algorithmically Curated Lies: How Search Engines Handle Misinformation\n  about US Biolabs in Ukraine", "abstract": "The growing volume of online content prompts the need for adopting\nalgorithmic systems of information curation. These systems range from web\nsearch engines to recommender systems and are integral for helping users stay\ninformed about important societal developments. However, unlike journalistic\nediting the algorithmic information curation systems (AICSs) are known to be\nsubject to different forms of malperformance which make them vulnerable to\npossible manipulation. The risk of manipulation is particularly prominent in\nthe case when AICSs have to deal with information about false claims that\nunderpin propaganda campaigns of authoritarian regimes. Using as a case study\nof the Russian disinformation campaign concerning the US biolabs in Ukraine, we\ninvestigate how one of the most commonly used forms of AICSs - i.e. web search\nengines - curate misinformation-related content. For this aim, we conduct\nvirtual agent-based algorithm audits of Google, Bing, and Yandex search outputs\nin June 2022. Our findings highlight the troubling performance of search\nengines. Even though some search engines, like Google, were less likely to\nreturn misinformation results, across all languages and locations, the three\nsearch engines still mentioned or promoted a considerable share of false\ncontent (33% on Google; 44% on Bing, and 70% on Yandex). We also find\nsignificant disparities in misinformation exposure based on the language of\nsearch, with all search engines presenting a higher number of false stories in\nRussian. Location matters as well with users from Germany being more likely to\nbe exposed to search results promoting false information. These observations\nstress the possibility of AICSs being vulnerable to manipulation, in particular\nin the case of the unfolding propaganda campaigns, and underline the importance\nof monitoring performance of these systems to prevent it.", "field": "Computer Science", "categories": "cs.IR,cs.CY"}, {"arxiv_id": "2401.13835", "title": "The Calibration Gap between Model and Human Confidence in Large Language\n  Models", "abstract": "For large language models (LLMs) to be trusted by humans they need to be\nwell-calibrated in the sense that they can accurately assess and communicate\nhow likely it is that their predictions are correct. Recent work has focused on\nthe quality of internal LLM confidence assessments, but the question remains of\nhow well LLMs can communicate this internal model confidence to human users.\nThis paper explores the disparity between external human confidence in an LLM's\nresponses and the internal confidence of the model. Through experiments\ninvolving multiple-choice questions, we systematically examine human users'\nability to discern the reliability of LLM outputs. Our study focuses on two key\nareas: (1) assessing users' perception of true LLM confidence and (2)\ninvestigating the impact of tailored explanations on this perception. The\nresearch highlights that default explanations from LLMs often lead to user\noverestimation of both the model's confidence and its' accuracy. By modifying\nthe explanations to more accurately reflect the LLM's internal confidence, we\nobserve a significant shift in user perception, aligning it more closely with\nthe model's actual confidence levels. This adjustment in explanatory approach\ndemonstrates potential for enhancing user trust and accuracy in assessing LLM\noutputs. The findings underscore the importance of transparent communication of\nconfidence levels in LLMs, particularly in high-stakes applications where\nunderstanding the reliability of AI-generated information is essential.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CL,cs.HC"}, {"arxiv_id": "2401.13836", "title": "Machine learning for industrial sensing and control: A survey and\n  practical perspective", "abstract": "With the rise of deep learning, there has been renewed interest within the\nprocess industries to utilize data on large-scale nonlinear sensing and control\nproblems. We identify key statistical and machine learning techniques that have\nseen practical success in the process industries. To do so, we start with\nhybrid modeling to provide a methodological framework underlying core\napplication areas: soft sensing, process optimization, and control. Soft\nsensing contains a wealth of industrial applications of statistical and machine\nlearning methods. We quantitatively identify research trends, allowing insight\ninto the most successful techniques in practice.\n  We consider two distinct flavors for data-driven optimization and control:\nhybrid modeling in conjunction with mathematical programming techniques and\nreinforcement learning. Throughout these application areas, we discuss their\nrespective industrial requirements and challenges.\n  A common challenge is the interpretability and efficiency of purely\ndata-driven methods. This suggests a need to carefully balance deep learning\ntechniques with domain knowledge. As a result, we highlight ways prior\nknowledge may be integrated into industrial machine learning applications. The\ntreatment of methods, problems, and applications presented here is poised to\ninform and inspire practitioners and researchers to develop impactful\ndata-driven sensing, optimization, and control solutions in the process\nindustries.", "field": "Computer Science", "categories": "eess.SY,cs.LG,cs.SY"}, {"arxiv_id": "2401.13837", "title": "Democratizing Fine-grained Visual Recognition with Large Language Models", "abstract": "Identifying subordinate-level categories from images is a longstanding task\nin computer vision and is referred to as fine-grained visual recognition\n(FGVR). It has tremendous significance in real-world applications since an\naverage layperson does not excel at differentiating species of birds or\nmushrooms due to subtle differences among the species. A major bottleneck in\ndeveloping FGVR systems is caused by the need of high-quality paired expert\nannotations. To circumvent the need of expert knowledge we propose Fine-grained\nSemantic Category Reasoning (FineR) that internally leverages the world\nknowledge of large language models (LLMs) as a proxy in order to reason about\nfine-grained category names. In detail, to bridge the modality gap between\nimages and LLM, we extract part-level visual attributes from images as text and\nfeed that information to a LLM. Based on the visual attributes and its internal\nworld knowledge the LLM reasons about the subordinate-level category names. Our\ntraining-free FineR outperforms several state-of-the-art FGVR and language and\nvision assistant models and shows promise in working in the wild and in new\ndomains where gathering expert annotation is arduous.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13839", "title": "Edge-coloring sparse graphs with $\u0394$ colors in quasilinear time", "abstract": "In this paper we show that every graph $G$ of bounded maximum average degree\n${\\rm mad}(G)$ and with maximum degree $\\Delta$ can be edge-colored using the\noptimal number of $\\Delta$ colors in quasilinear expected time, whenever\n$\\Delta\\ge 2{\\rm mad}(G)$. The maximum average degree is within a\nmultiplicative constant of other popular graph sparsity parameters like\narboricity, degeneracy or maximum density. Our algorithm extends previous\nresults of Chrobak and Nishizeki [J. Algorithms, 1990] and Bhattacharya, Costa,\nPanski and Solomon [arXiv, 2023].", "field": "Computer Science", "categories": "cs.DS"}, {"arxiv_id": "2401.13842", "title": "Tight Competitive and Variance Analyses of Matching Policies in Gig\n  Platforms", "abstract": "In this paper, we propose an online-matching-based model to tackle the two\nfundamental issues, matching and pricing, existing in a wide range of\nreal-world gig platforms, including ride-hailing (matching riders and drivers),\ncrowdsourcing markets (pairing workers and tasks), and online recommendations\n(offering items to customers). Our model assumes the arriving distributions of\ndynamic agents (e.g., riders, workers, and buyers) are accessible in advance,\nand they can change over time, which is referred to as \\emph{Known\nHeterogeneous Distributions} (KHD).\n  In this paper, we initiate variance analysis for online matching algorithms\nunder KHD. Unlike the popular competitive-ratio (CR) metric, the variance of\nonline algorithms' performance is rarely studied due to inherent technical\nchallenges, though it is well linked to robustness. We focus on two natural\nparameterized sampling policies, denoted by $\\mathsf{ATT}(\\gamma)$ and\n$\\mathsf{SAMP}(\\gamma)$, which appear as foundational bedrock in online\nalgorithm design. We offer rigorous competitive ratio (CR) and variance\nanalyses for both policies. Specifically, we show that $\\mathsf{ATT}(\\gamma)$\nwith $\\gamma \\in [0,1/2]$ achieves a CR of $\\gamma$ and a variance of $\\gamma\n\\cdot (1-\\gamma) \\cdot B$ on the total number of matches with $B$ being the\ntotal matching capacity. In contrast, $\\mathsf{SAMP}(\\gamma)$ with $\\gamma \\in\n[0,1]$ accomplishes a CR of $\\gamma (1-\\gamma)$ and a variance of $\\bar{\\gamma}\n(1-\\bar{\\gamma})\\cdot B$ with $\\bar{\\gamma}=\\min(\\gamma,1/2)$. All CR and\nvariance analyses are tight and unconditional of any benchmark. As a byproduct,\nwe prove that $\\mathsf{ATT}(\\gamma=1/2)$ achieves an optimal CR of $1/2$.", "field": "Computer Science", "categories": "cs.DS"}, {"arxiv_id": "2401.13843", "title": "Enumerating the k-fold configurations in multi-class classification\n  problems", "abstract": "K-fold cross-validation is a widely used tool for assessing classifier\nperformance. The reproducibility crisis faced by artificial intelligence partly\nresults from the irreproducibility of reported k-fold cross-validation-based\nperformance scores. Recently, we introduced numerical techniques to test the\nconsistency of claimed performance scores and experimental setups. In a crucial\nuse case, the method relies on the combinatorial enumeration of all k-fold\nconfigurations, for which we proposed an algorithm in the binary classification\ncase.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13848", "title": "A V2X-based Privacy Preserving Federated Measuring and Learning System", "abstract": "Future autonomous vehicles (AVs) will use a variety of sensors that generate\na vast amount of data. Naturally, this data not only serves self-driving\nalgorithms; but can also assist other vehicles or the infrastructure in\nreal-time decision-making. Consequently, vehicles shall exchange their\nmeasurement data over Vehicle-to-Everything (V2X) technologies. Moreover,\npredicting the state of the road network might be beneficial too. With such a\nprediction, we might mitigate road congestion, balance parking lot usage, or\noptimize the traffic flow. That would decrease transportation costs as well as\nreduce its environmental impact.\n  In this paper, we propose a federated measurement and learning system that\nprovides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V)\ncommunication while also operating a federated learning (FL) scheme over the\nVehicle-to-Network (V2N) link to create a predictive model of the\ntransportation network. As we are yet to have real-world AV data, we model it\nwith a non-IID (independent and identically distributed) dataset to evaluate\nthe capabilities of the proposed system in terms of performance and privacy.\nResults indicate that the proposed FL scheme improves learning performance and\nprevents eavesdropping at the aggregator server side.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CR,stat.ML,68T07, 68T42, 68P27, 68P25,I.2.6; I.2.11"}, {"arxiv_id": "2401.13849", "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery\n  and Guidance", "abstract": "Large Language Models (LLMs) have recently showcased remarkable reasoning\nabilities. However, larger models often surpass their smaller counterparts in\nreasoning tasks, posing the challenge of effectively transferring these\ncapabilities from larger models. Existing approaches heavily rely on extensive\nfine-tuning data or continuous interactions with a superior teacher LLM during\ninference. We introduce a principle-based teacher-student framework called\n``Teaching via Principle Discovery'' (TPD) to address these limitations.\nInspired by human learning mechanisms, TPD mimics the interaction between a\nteacher and a student using a principle-based approach. The teacher LLM\ngenerates problem-solving instructions and corrective principles based on the\nstudent LLM's errors. These principles guide the refinement of instructions and\nthe selection of instructive examples from a validation set. This enables the\nstudent model to learn from both the teacher's guidance and its own mistakes.\nOnce the student model begins making inferences, TPD requires no further\nintervention from the teacher LLM or humans. Through extensive experiments\nacross eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared\nto standard chain-of-thought prompting, TPD significantly improves the student\nmodel's performance, achieving $6.2\\%$ improvement on average.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13850", "title": "PADTHAI-MM: A Principled Approach for Designing Trustable,\n  Human-centered AI systems using the MAST Methodology", "abstract": "Designing for AI trustworthiness is challenging, with a lack of practical\nguidance despite extensive literature on trust. The Multisource AI Scorecard\nTable (MAST), a checklist rating system, addresses this gap in designing and\nevaluating AI-enabled decision support systems. We propose the Principled\nApproach for Designing Trustable Human-centered AI systems using MAST\nMethodology (PADTHAI-MM), a nine-step framework what we demonstrate through the\niterative design of a text analysis platform called the REporting Assistant for\nDefense and Intelligence Tasks (READIT). We designed two versions of READIT,\nhigh-MAST including AI context and explanations, and low-MAST resembling a\n\"black box\" type system. Participant feedback and state-of-the-art AI knowledge\nwas integrated in the design process, leading to a redesigned prototype tested\nby participants in an intelligence reporting task. Results show that\nMAST-guided design can improve trust perceptions, and that MAST criteria can be\nlinked to performance, process, and purpose information, providing a practical\nand theory-informed basis for AI system design.", "field": "Computer Science", "categories": "cs.CY"}, {"arxiv_id": "2401.13851", "title": "Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice\n  cloning to Indic Languages", "abstract": "In this paper, we describe the TTS models developed by NVIDIA for the\nMMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024\nChallenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by\ntraining additionally on 5 minutes of target speaker data. In Track 3, we\nutilize P-Flow to perform zero-shot TTS by training on the challenge dataset as\nwell as external datasets. We use HiFi-GAN vocoders for all submissions.\nRAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on\nTrack 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS)\nof 3.62.", "field": "Computer Science", "categories": "cs.SD,cs.LG,eess.AS"}, {"arxiv_id": "2401.13853", "title": "Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception", "abstract": "Conventional cameras employed in autonomous vehicle (AV) systems support many\nperception tasks, but are challenged by low-light or high dynamic range scenes,\nadverse weather, and fast motion. Novel sensors, such as event and thermal\ncameras, offer capabilities with the potential to address these scenarios, but\nthey remain to be fully exploited. This paper introduces the Novel Sensors for\nAutonomous Vehicle Perception (NSAVP) dataset to facilitate future research on\nthis topic. The dataset was captured with a platform including stereo event,\nthermal, monochrome, and RGB cameras as well as a high precision navigation\nsystem providing ground truth poses. The data was collected by repeatedly\ndriving two ~8 km routes and includes varied lighting conditions and opposing\nviewpoint perspectives. We provide benchmarking experiments on the task of\nplace recognition to demonstrate challenges and opportunities for novel sensors\nto enhance critical AV perception tasks. To our knowledge, the NSAVP dataset is\nthe first to include stereo thermal cameras together with stereo event and\nmonochrome cameras. The dataset and supporting software suite is available at:\nhttps://umautobots.github.io/nsavp", "field": "Computer Science", "categories": "cs.RO,cs.CV"}, {"arxiv_id": "2401.13854", "title": "Embedding Attack Project (Work Report)", "abstract": "This report summarizes all the MIA experiments (Membership Inference Attacks)\nof the Embedding Attack Project, including threat models, experimental setup,\nexperimental results, findings and discussion. Current results cover the\nevaluation of two main MIA strategies (loss-based and embedding-based MIAs) on\n6 AI models ranging from Computer Vision to Language Modelling. There are two\nongoing experiments on MIA defense and neighborhood-comparison embedding\nattacks. These are ongoing projects.\n  The current work on MIA and PIA can be summarized into six conclusions: (1)\nAmount of overfitting is directly proportional to model's vulnerability; (2)\nearly embedding layers in the model are less susceptible to privacy leaks; (3)\nDeeper model layers contain more membership information; (4) Models are more\nvulnerable to MIA if both embeddings and corresponding training labels are\ncompromised; (5) it is possible to use pseudo-labels to increase the MIA\nsuccess; and (6) although MIA and PIA success rates are proportional, reducing\nthe MIA does not necessarily reduce the PIA.", "field": "Computer Science", "categories": "cs.LG,cs.CR"}, {"arxiv_id": "2401.13856", "title": "LAA-Net: Localized Artifact Attention Network for High-Quality Deepfakes\n  Detection", "abstract": "This paper introduces a novel approach for high-quality deepfake detection\ncalled Localized Artifact Attention Network (LAA-Net). Existing methods for\nhigh-quality deepfake detection are mainly based on a supervised binary\nclassifier coupled with an implicit attention mechanism. As a result, they do\nnot generalize well to unseen manipulations. To handle this issue, two main\ncontributions are made. First, an explicit attention mechanism within a\nmulti-task learning framework is proposed. By combining heatmap-based and\nself-consistency attention strategies, LAA-Net is forced to focus on a few\nsmall artifact-prone vulnerable regions. Second, an Enhanced Feature Pyramid\nNetwork (E-FPN) is proposed as a simple and effective mechanism for spreading\ndiscriminative low-level features into the final feature output, with the\nadvantage of limiting redundancy. Experiments performed on several benchmarks\nshow the superiority of our approach in terms of Area Under the Curve (AUC) and\nAverage Precision (AP). The code will be released soon.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13858", "title": "Inverse Molecular Design with Multi-Conditional Diffusion Guidance", "abstract": "Inverse molecular design with diffusion models holds great potential for\nadvancements in material and drug discovery. Despite success in unconditional\nmolecule generation, integrating multiple properties such as synthetic score\nand gas permeability as condition constraints into diffusion models remains\nunexplored. We introduce multi-conditional diffusion guidance. The proposed\nTransformer-based denoising model has a condition encoder that learns the\nrepresentations of numerical and categorical conditions. The denoising model,\nconsisting of a structure encoder-decoder, is trained for denoising under the\nrepresentation of conditions. The diffusion process becomes graph-dependent to\naccurately estimate graph-related noise in molecules, unlike the previous\nmodels that focus solely on the marginal distributions of atoms or bonds. We\nextensively validate our model for multi-conditional polymer and small molecule\ngeneration. Results demonstrate our superiority across metrics from\ndistribution learning to condition control for molecular properties. An inverse\npolymer design task for gas separation with feedback from domain experts\nfurther demonstrates its practical utility.", "field": "Computer Science", "categories": "cs.LG,q-bio.BM"}, {"arxiv_id": "2401.13865", "title": "Appearance Debiased Gaze Estimation via Stochastic Subject-Wise\n  Adversarial Learning", "abstract": "Recently, appearance-based gaze estimation has been attracting attention in\ncomputer vision, and remarkable improvements have been achieved using various\ndeep learning techniques. Despite such progress, most methods aim to infer gaze\nvectors from images directly, which causes overfitting to person-specific\nappearance factors. In this paper, we address these challenges and propose a\nnovel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE),\nwhich trains a network to generalize the appearance of subjects. We design a\nFace generalization Network (Fgen-Net) using a face-to-gaze encoder and face\nidentity classifier and a proposed adversarial loss. The proposed loss\ngeneralizes face appearance factors so that the identity classifier inferences\na uniform probability distribution. In addition, the Fgen-Net is trained by a\nlearning mechanism that optimizes the network by reselecting a subset of\nsubjects at every training step to avoid overfitting. Our experimental results\nverify the robustness of the method in that it yields state-of-the-art\nperformance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets,\nrespectively. Furthermore, we demonstrate the positive generalization effect by\nconducting further experiments using face images involving different styles\ngenerated from the generative model.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13867", "title": "Unmasking and Quantifying Racial Bias of Large Language Models in\n  Medical Report Generation", "abstract": "Large language models like GPT-3.5-turbo and GPT-4 hold promise for\nhealthcare professionals, but they may inadvertently inherit biases during\ntheir training, potentially affecting their utility in medical applications.\nDespite few attempts in the past, the precise impact and extent of these biases\nremain uncertain. Through both qualitative and quantitative analyses, we find\nthat these models tend to project higher costs and longer hospitalizations for\nWhite populations and exhibit optimistic views in challenging medical scenarios\nwith much higher survival rates. These biases, which mirror real-world\nhealthcare disparities, are evident in the generation of patient backgrounds,\nthe association of specific diseases with certain races, and disparities in\ntreatment recommendations, etc. Our findings underscore the critical need for\nfuture research to address and mitigate biases in language models, especially\nin critical healthcare applications, to ensure fair and accurate outcomes for\nall patients.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13868", "title": "Shell topology optimization based on level set method", "abstract": "This paper proposes a level set-based method for optimizing shell structures\nwith large design changes in shape and topology. Conventional shell\noptimization methods, whether parametric or nonparametric, often only allow\nlimited design changes in shape. In the proposed method, the shell structure is\ndefined as the isosurface of a level set function. The level set function is\niteratively updated based on the shape sensitivity on the surface mesh.\nTherefore, the proposed method can represent an arbitrary manifold surface\nwhile dealing with topological changes, for example, from a spherical surface\nto a toroidal surface. We applied the proposed method to the mean compliance\nminimization problems of 3D shell structural designs for dome, bending plate\nand cantilever beam examples to demonstrate its efficacy of the proposed\nmethod.", "field": "Computer Science", "categories": "cs.CE"}, {"arxiv_id": "2401.13870", "title": "Integrating Large Language Models into Recommendation via Mutual\n  Augmentation and Adaptive Aggregation", "abstract": "Conventional recommendation methods have achieved notable advancements by\nharnessing collaborative or sequential information from user behavior.\nRecently, large language models (LLMs) have gained prominence for their\ncapabilities in understanding and reasoning over textual semantics, and have\nfound utility in various domains, including recommendation. Conventional\nrecommendation methods and LLMs each have their strengths and weaknesses. While\nconventional methods excel at mining collaborative information and modeling\nsequential behavior, they struggle with data sparsity and the long-tail\nproblem. LLMs, on the other hand, are proficient at utilizing rich textual\ncontexts but face challenges in mining collaborative or sequential information.\nDespite their individual successes, there is a significant gap in leveraging\ntheir combined potential to enhance recommendation performance.\n  In this paper, we introduce a general and model-agnostic framework known as\n\\textbf{L}arge \\textbf{la}nguage model with \\textbf{m}utual augmentation and\n\\textbf{a}daptive aggregation for \\textbf{Rec}ommendation (\\textbf{Llama4Rec}).\nLlama4Rec synergistically combines conventional and LLM-based recommendation\nmodels. Llama4Rec proposes data augmentation and prompt augmentation strategies\ntailored to enhance the conventional model and LLM respectively. An adaptive\naggregation module is adopted to combine the predictions of both kinds of\nmodels to refine the final recommendation results. Empirical studies on three\nreal-world datasets validate the superiority of Llama4Rec, demonstrating its\nconsistent outperformance of baseline methods and significant improvements in\nrecommendation performance.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13872", "title": "Edge Conditional Node Update Graph Neural Network for Multi-variate Time\n  Series Anomaly Detection", "abstract": "With the rapid advancement in cyber-physical systems, the increasing number\nof sensors has significantly complicated manual monitoring of system states.\nConsequently, graph-based time-series anomaly detection methods have gained\nattention due to their ability to explicitly represent relationships between\nsensors. However, these methods often apply a uniform source node\nrepresentation across all connected target nodes, even when updating different\ntarget node representations. Moreover, the graph attention mechanism, commonly\nused to infer unknown graph structures, could constrain the diversity of source\nnode representations. In this paper, we introduce the Edge Conditional\nNode-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge\nconditional node update module, dynamically transforms source node\nrepresentations based on connected edges to represent target nodes aptly. We\nvalidate performance on three real-world datasets: SWaT, WADI, and PSM. Our\nmodel demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively,\ncompared to best F1 baseline models.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13877", "title": "AscDAMs: Advanced SLAM-based channel detection and mapping system", "abstract": "Obtaining high-resolution, accurate channel topography and deposit conditions\nis the prior challenge for the study of channelized debris flow. Currently,\nwide-used mapping technologies including satellite imaging and drone\nphotogrammetry struggle to precisely observe channel interior conditions of\nmountainous long-deep gullies, particularly those in the Wenchuan Earthquake\nregion. SLAM is an emerging tech for 3D mapping; however, extremely rugged\nenvironment in long-deep gullies poses two major challenges even for the\nstate-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation\nof sensors. These issues result in large deviation and lots of noise for SLAM\nresults. To improve SLAM mapping in such environments, we propose an advanced\nSLAM-based channel detection and mapping system, namely AscDAMs. It features\nthree main enhancements to post-process SLAM results: (1) The digital\northophoto map aided deviation correction algorithm greatly eliminates the\nsystematic error; (2) The point cloud smoothing algorithm substantially\ndiminishes noises; (3) The cross section extraction algorithm enables the\nquantitative assessment of channel deposits and their changes. Two field\nexperiments were conducted in Chutou Gully, Wenchuan County in China in\nFebruary and November 2023, representing observations before and after the\nrainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM\nresults, promoting SLAM for mapping the specially challenging environment. The\nproposed method compensates for the insufficiencies of existing technologies in\ndetecting debris flow channel interiors including detailed channel morphology,\nerosion patterns, deposit distinction, volume estimation and change detection.\nIt serves to enhance the study of full-scale debris flow mechanisms, long-term\npost-seismic evolution, and hazard assessment.", "field": "Computer Science", "categories": "cs.CV,cs.RO"}, {"arxiv_id": "2401.13882", "title": "Robust Transmission Design for RIS-Assisted Integrated Sensing and\n  Communication Systems", "abstract": "As a critical technology for next-generation communication networks,\nintegrated sensing and communication (ISAC) aims to achieve the harmonious\ncoexistence of communication and sensing. The degrees-of-freedom (DoF) of ISAC\nis limited due to multiple performance metrics used for communication and\nsensing. Reconfigurable Intelligent Surfaces (RIS) composed of metamaterials\ncan enhance the DoF in the spatial domain of ISAC systems. However, the\navailability of perfect Channel State Information (CSI) is a prerequisite for\nthe gain brought by RIS, which is not realistic in practical environments.\nTherefore, under the imperfect CSI condition, we propose a decomposition-based\nlarge deviation inequality approach to eliminate the impact of CSI error on\ncommunication rate and sensing Cram\\'er-Rao bound (CRB). Then, an alternating\noptimization (AO) algorithm based on semi-definite relaxation (SDR) and\ngradient extrapolated majorization-maximization (GEMM) is proposed to solve the\ntransmit beamforming and discrete RIS beamforming problems. We also analyze the\ncomplexity and convergence of the proposed algorithm. Simulation results show\nthat the proposed algorithms can effectively eliminate the influence of CSI\nerror and have good convergence performance. Notably, when CSI error exists,\nthe gain brought by RIS will decrease with the increase of the number of RIS\nelements. Finally, we summarize and outline future research directions.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.13883", "title": "Domain-Independent Dynamic Programming", "abstract": "For combinatorial optimization problems, model-based paradigms such as\nmixed-integer programming (MIP) and constraint programming (CP) aim to decouple\nmodeling and solving a problem: the `holy grail' of declarative problem\nsolving. We propose domain-independent dynamic programming (DIDP), a new\nmodel-based paradigm based on dynamic programming (DP). While DP is not new, it\nhas typically been implemented as a problem-specific method. We introduce\nDynamic Programming Description Language (DyPDL), a formalism to define DP\nmodels based on a state transition system, inspired by AI planning. We show\nthat heuristic search algorithms can be used to solve DyPDL models and propose\nseven DIDP solvers. We experimentally compare our DIDP solvers with commercial\nMIP and CP solvers (solving MIP and CP models, respectively) on common\nbenchmark instances of eleven combinatorial optimization problem classes. We\nshow that DIDP outperforms MIP in nine problem classes, CP also in nine problem\nclasses, and both MIP and CP in seven.", "field": "Computer Science", "categories": "cs.AI"}, {"arxiv_id": "2401.13887", "title": "A comparative study of zero-shot inference with large language models\n  and supervised modeling in breast cancer pathology classification", "abstract": "Although supervised machine learning is popular for information extraction\nfrom clinical notes, creating large annotated datasets requires extensive\ndomain expertise and is time-consuming. Meanwhile, large language models (LLMs)\nhave demonstrated promising transfer learning capability. In this study, we\nexplored whether recent LLMs can reduce the need for large-scale data\nannotations. We curated a manually-labeled dataset of 769 breast cancer\npathology reports, labeled with 13 categories, to compare zero-shot\nclassification capability of the GPT-4 model and the GPT-3.5 model with\nsupervised classification performance of three model architectures: random\nforests classifier, long short-term memory networks with attention (LSTM-Att),\nand the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either\nsignificantly better than or as well as the best supervised model, the LSTM-Att\nmodel (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance\nbetween labels, the differences were more prominent. Frequent sources of GPT-4\nerrors included inferences from multiple samples and complex task design. On\ncomplex tasks where large annotated datasets cannot be easily collected, LLMs\ncan reduce the burden of large-scale data labeling. However, if the use of LLMs\nis prohibitive, the use of simpler supervised models with large annotated\ndatasets can provide comparable results. LLMs demonstrated the potential to\nspeed up the execution of clinical NLP studies by reducing the need for\ncurating large annotated datasets. This may result in an increase in the\nutilization of NLP-based variables and outcomes in observational clinical\nstudies.", "field": "Computer Science", "categories": "cs.CL,cs.LG"}, {"arxiv_id": "2401.13888", "title": "Knowledge Graph Supported Benchmark and Video Captioning for Basketball", "abstract": "Despite the recent emergence of video captioning models, how to generate the\ntext description with specific entity names and fine-grained actions is far\nfrom being solved, which however has great applications such as basketball live\ntext broadcast. In this paper, a new multimodal knowledge supported basketball\nbenchmark for video captioning is proposed. Specifically, we construct a\nMultimodal Basketball Game Knowledge Graph (MbgKG) to provide knowledge beyond\nvideos. Then, a Multimodal Basketball Game Video Captioning (MbgVC) dataset\nthat contains 9 types of fine-grained shooting events and 286 players'\nknowledge (i.e., images and names) is constructed based on MbgKG. We develop a\nnovel framework in the encoder-decoder form named Entity-Aware Captioner (EAC)\nfor basketball live text broadcast. The temporal information in video is\nencoded by introducing the bi-directional GRU (Bi-GRU) module. And the\nmulti-head self-attention module is utilized to model the relationships among\nthe players and select the key players. Besides, we propose a new performance\nevaluation metric named Game Description Score (GDS), which measures not only\nthe linguistic performance but also the accuracy of the names prediction.\nExtensive experiments on MbgVC dataset demonstrate that EAC effectively\nleverages external knowledge and outperforms advanced video captioning models.\nThe proposed benchmark and corresponding codes will be publicly available soon.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13891", "title": "Text to speech synthesis", "abstract": "Text-to-speech (TTS) synthesis is a technology that converts written text\ninto spoken words, enabling a natural and accessible means of communication.\nThis abstract explores the key aspects of TTS synthesis, encompassing its\nunderlying technologies, applications, and implications for various sectors.\nThe technology utilizes advanced algorithms and linguistic models to convert\ntextual information into life like speech, allowing for enhanced user\nexperiences in diverse contexts such as accessibility tools, navigation\nsystems, and virtual assistants. The abstract delves into the challenges and\nadvancements in TTS synthesis, including considerations for naturalness,\nmultilingual support, and emotional expression in synthesized speech.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.13898", "title": "Cross-Modal Prototype based Multimodal Federated Learning under Severely\n  Missing Modality", "abstract": "Multimodal federated learning (MFL) has emerged as a decentralized machine\nlearning paradigm, allowing multiple clients with different modalities to\ncollaborate on training a machine learning model across diverse data sources\nwithout sharing their private data. However, challenges, such as data\nheterogeneity and severely missing modalities, pose crucial hindrances to the\nrobustness of MFL, significantly impacting the performance of global model. The\nabsence of a modality introduces misalignment during the local training phase,\nstemming from zero-filling in the case of clients with missing modalities.\nConsequently, achieving robust generalization in global model becomes\nimperative, especially when dealing with clients that have incomplete data. In\nthis paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a\nnovel approach for MFL under severely missing modalities by conducting the\ncomplete prototypes to provide diverse modality knowledge in modality-shared\nlevel with the cross-modal regularization and modality-specific level with\ncross-modal contrastive mechanism. Additionally, our approach introduces the\ncross-modal alignment to provide regularization for modality-specific features,\nthereby enhancing overall performance, particularly in scenarios involving\nseverely missing modalities. Through extensive experiments on three multimodal\ndatasets, we demonstrate the effectiveness of MFCPL in mitigating these\nchallenges and improving the overall performance.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13903", "title": "Alternative Interfaces for Human-initiated Natural Language\n  Communication and Robot-initiated Haptic Feedback: Towards Better Situational\n  Awareness in Human-Robot Collaboration", "abstract": "This article presents an implementation of a natural-language speech\ninterface and a haptic feedback interface that enables a human supervisor to\nprovide guidance to, request information, and receive status updates from a\nSpot robot. We provide insights gained during preliminary user testing of the\ninterface in a realistic robot exploration scenario.", "field": "Computer Science", "categories": "cs.RO,cs.HC"}, {"arxiv_id": "2401.13904", "title": "Empowering Machines to Think Like Chemists: Unveiling Molecular\n  Structure-Polarity Relationships with Hierarchical Symbolic Regression", "abstract": "Thin-layer chromatography (TLC) is a crucial technique in molecular polarity\nanalysis. Despite its importance, the interpretability of predictive models for\nTLC, especially those driven by artificial intelligence, remains a challenge.\nCurrent approaches, utilizing either high-dimensional molecular fingerprints or\ndomain-knowledge-driven feature engineering, often face a dilemma between\nexpressiveness and interpretability. To bridge this gap, we introduce\nUnsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical\nneural networks and symbolic regression. UHiSR automatically distills\nchemical-intuitive polarity indices, and discovers interpretable equations that\nlink molecular structure to chromatographic behavior.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.DB,stat.AP"}, {"arxiv_id": "2401.13905", "title": "Dynamic embedded topic models and change-point detection for exploring\n  literary-historical hypotheses", "abstract": "We present a novel combination of dynamic embedded topic models and\nchange-point detection to explore diachronic change of lexical semantic\nmodality in classical and early Christian Latin. We demonstrate several methods\nfor finding and characterizing patterns in the output, and relating them to\ntraditional scholarship in Comparative Literature and Classics. This simple\napproach to unsupervised models of semantic change can be applied to any\nsuitable corpus, and we conclude with future directions and refinements aiming\nto allow noisier, less-curated materials to meet that threshold.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13907", "title": "No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data\n  Artifacts", "abstract": "Researchers recently found out that sometimes language models achieve high\naccuracy on benchmark data set, but they can not generalize very well with even\nlittle changes to the original data set. This is sometimes due to data\nartifacts, model is learning the spurious correlation between tokens and\nlabels, instead of the semantics and logic. In this work, we analyzed SNLI data\nand visualized such spurious correlations. We proposed an adaptive up-sampling\nalgorithm to correct the data artifacts, which is simple and effective, and\ndoes not need human edits or annotation. We did an experiment applying the\nalgorithm to fix the data artifacts in SNLI data and the model trained with\ncorrected data performed significantly better than the model trained with raw\nSNLI data, overall, as well as on the subset we corrected.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13912", "title": "A Survey of Deep Learning and Foundation Models for Time Series\n  Forecasting", "abstract": "Deep Learning has been successfully applied to many application domains, yet\nits advantages have been slow to emerge for time series forecasting. For\nexample, in the well-known Makridakis (M) Competitions, hybrids of traditional\nstatistical or machine learning techniques have only recently become the top\nperformers. With the recent architectural advances in deep learning being\napplied to time series forecasting (e.g., encoder-decoders with attention,\ntransformers, and graph neural networks), deep learning has begun to show\nsignificant advantages. Still, in the area of pandemic prediction, there remain\nchallenges for deep learning models: the time series is not long enough for\neffective training, unawareness of accumulated scientific knowledge, and\ninterpretability of the model. To this end, the development of foundation\nmodels (large deep learning models with extensive pre-training) allows models\nto understand patterns and acquire knowledge that can be applied to new related\nproblems before extensive training data becomes available. Furthermore, there\nis a vast amount of knowledge available that deep learning models can tap into,\nincluding Knowledge Graphs and Large Language Models fine-tuned with scientific\ndomain knowledge. There is ongoing research examining how to utilize or inject\nsuch knowledge into deep learning models. In this survey, several\nstate-of-the-art modeling techniques are reviewed, and suggestions for further\nwork are provided.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13913", "title": "Spectral Clustering for Discrete Distributions", "abstract": "Discrete distribution clustering (D2C) was often solved by Wasserstein\nbarycenter methods. These methods are under a common assumption that clusters\ncan be well represented by barycenters, which may not hold in many real\napplications. In this work, we propose a simple yet effective framework based\non spectral clustering and distribution affinity measures (e.g., maximum mean\ndiscrepancy and Wasserstein distance) for D2C. To improve the scalability, we\npropose to use linear optimal transport to construct affinity matrices\nefficiently on large datasets. We provide theoretical guarantees for the\nsuccess of the proposed methods in clustering distributions. Experiments on\nsynthetic and real data show that our methods outperform the baselines largely\nin terms of both clustering accuracy and computational efficiency.", "field": "Computer Science", "categories": "cs.LG,cs.AI,stat.ML"}, {"arxiv_id": "2401.13919", "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal\n  Models", "abstract": "The advancement of large language models (LLMs) leads to a new era marked by\nthe development of autonomous applications in the real world, which drives\ninnovation in the creation of advanced web-based agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe propose a new evaluation protocol for web agents to address the challenges\nof automatic evaluation of open-ended web agent tasks, leveraging the robust\nmultimodal comprehension capabilities of GPT-4V. We create a new benchmark by\ngathering real-world tasks from 15 widely used websites to evaluate our agents.\nWe show that WebVoyager achieves a 55.7% task success rate, significantly\nsurpassing the performance of both GPT-4 (All Tools) and the WebVoyager\n(text-only) setups, underscoring the exceptional capability of WebVoyager in\npractical applications. We found that our proposed automatic evaluation\nachieves 85.3% agreement with human judgment, paving the way for further\ndevelopment of web agents in a real-world setting.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13920", "title": "LocMoE: A Low-overhead MoE for Large Language Model Training", "abstract": "The Mixtures-of-Experts (MoE) model is a widespread distributed and\nintegrated learning method for large language models (LLM), which is favored\ndue to its ability to sparsify and expand models efficiently. However, the\nperformance of MoE is limited by load imbalance and high latency of All-To-All\ncommunication, along with relatively redundant computation owing to large\nexpert capacity. Load imbalance may result from existing routing policies that\nconsistently tend to select certain experts. The frequent inter-node\ncommunication in the All-To-All procedure also significantly prolongs the\ntraining time. To alleviate the above performance problems, we propose a novel\nrouting strategy that combines load balance and locality by converting partial\ninter-node communication to that of intra-node. Notably, we elucidate that\nthere is a minimum threshold for expert capacity, calculated through the\nmaximal angular deviation between the gating weights of the experts and the\nassigned tokens. We port these modifications on the PanGu-Sigma model based on\nthe MindSpore framework with multi-level routing and conduct experiments on\nAscend clusters. The experiment results demonstrate that the proposed LocMoE\nreduces training time per epoch by 12.68% to 22.24% compared to classical\nrouters, such as hash router and switch router, without impacting the model\naccuracy.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CL"}, {"arxiv_id": "2401.13922", "title": "Simplified Successive Cancellation List Decoding of PAC Codes", "abstract": "Polar codes are the first class of structured channel codes that achieve the\nsymmetric capacity of binary channels with efficient encoding and decoding. In\n2019, Arikan proposed a new polar coding scheme referred to as\npolarization-adjusted convolutional (PAC)} codes. In contrast to polar codes,\nPAC codes precode the information word using a convolutional code prior to\npolar encoding. This results in material coding gain over polar code under Fano\nsequential decoding as well as successive cancellation list (SCL) decoding.\nGiven the advantages of SCL decoding over Fano decoding in certain scenarios\nsuch as low-SNR regime or where a constraint on the worst case decoding latency\nexists, in this paper, we focus on SCL decoding and present a simplified SCL\n(SSCL) decoding algorithm for PAC codes. SSCL decoding of PAC codes reduces the\ndecoding latency by identifying special nodes in the decoding tree and\nprocessing them at the intermediate stages of the graph. Our simulation results\nshow that the performance of PAC codes under SSCL decoding is almost similar to\nthe SCL decoding while having lower decoding latency.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13923", "title": "Towards 3D Molecule-Text Interpretation in Language Models", "abstract": "Language Models (LMs) have greatly influenced diverse domains. However, their\ninherent limitation in comprehending 3D molecular structures has considerably\nconstrained their potential in the biomolecular domain. To bridge this gap, we\nfocus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular\nLanguage Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze\n3D molecules by equipping the LM with a 3D molecular encoder. This integration\nis achieved by a 3D molecule-text projector, bridging the 3D molecular\nencoder's representation space and the LM's input space. Moreover, to enhance\n3D-MoLM's ability of cross-modal molecular understanding and instruction\nfollowing, we meticulously curated a 3D molecule-centric instruction tuning\ndataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric\ninstruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder\nand LM. It significantly surpasses existing baselines on downstream tasks,\nincluding molecule-text retrieval, molecule captioning, and more challenging\nopen-text molecular QA tasks, especially focusing on 3D-dependent properties.", "field": "Computer Science", "categories": "cs.LG,cs.IR,q-bio.BM"}, {"arxiv_id": "2401.13924", "title": "ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis", "abstract": "In recent years, large language models (LLMs), such as ChatGPT, have been\npivotal in advancing various artificial intelligence applications, including\nnatural language processing and software engineering. A promising yet\nunderexplored area is utilizing LLMs in software testing, particularly in\nblack-box testing. This paper explores the test cases devised by ChatGPT in\ncomparison to those created by human participants. In this study, ChatGPT\n(GPT-4) and four participants each created black-box test cases for three\napplications based on specifications written by the authors. The goal was to\nevaluate the real-world applicability of the proposed test cases, identify\npotential shortcomings, and comprehend how ChatGPT could enhance human testing\nstrategies. ChatGPT can generate test cases that generally match or slightly\nsurpass those created by human participants in terms of test viewpoint\ncoverage. Additionally, our experiments demonstrated that when ChatGPT\ncooperates with humans, it can cover considerably more test viewpoints than\neach can achieve alone, suggesting that collaboration between humans and\nChatGPT may be more effective than human pairs working together. Nevertheless,\nwe noticed that the test cases generated by ChatGPT have certain issues that\nrequire addressing before use.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.13926", "title": "Iterative Methods in GPU-Resident Linear Solvers for Nonlinear\n  Constrained Optimization", "abstract": "Linear solvers are major computational bottlenecks in a wide range of\ndecision support and optimization computations. The challenges become even more\npronounced on heterogeneous hardware, where traditional sparse numerical linear\nalgebra methods are often inefficient. For example, methods for solving\nill-conditioned linear systems have relied on conditional branching, which\ndegrades performance on hardware accelerators such as graphical processing\nunits (GPUs). To improve the efficiency of solving ill-conditioned systems, our\ncomputational strategy separates computations that are efficient on GPUs from\nthose that need to run on traditional central processing units (CPUs). Our\nstrategy maximizes the reuse of expensive CPU computations. Iterative methods,\nwhich thus far have not been broadly used for ill-conditioned linear systems,\nplay an important role in our approach. In particular, we extend ideas from [1]\nto implement iterative refinement using inexact LU factors and flexible\ngeneralized minimal residual (FGMRES), with the aim of efficient performance on\nGPUs. We focus on solutions that are effective within broader application\ncontexts, and discuss how early performance tests could be improved to be more\npredictive of the performance in a realistic environment", "field": "Computer Science", "categories": "cs.CE,cs.NA,cs.SY,eess.SY,math.NA,65F05, 65F10, 65F50, 65K10, 65Y05, 65Y10, 90C51"}, {"arxiv_id": "2401.13927", "title": "Adaptive Text Watermark for Large Language Models", "abstract": "The advancement of Large Language Models (LLMs) has led to increasing\nconcerns about the misuse of AI-generated text, and watermarking for\nLLM-generated text has emerged as a potential solution. However, it is\nchallenging to generate high-quality watermarked text while maintaining strong\nsecurity, robustness, and the ability to detect watermarks without prior\nknowledge of the prompt or model. This paper proposes an adaptive watermarking\nstrategy to address this problem. To improve the text quality and maintain\nrobustness, we adaptively add watermarking to token distributions with high\nentropy measured using an auxiliary model and keep the low entropy token\ndistributions untouched. For the sake of security and to further minimize the\nwatermark's impact on text quality, instead of using a fixed green/red list\ngenerated from a random secret key, which can be vulnerable to decryption and\nforgery, we adaptively scale up the output logits in proportion based on the\nsemantic embedding of previously generated text using a well designed semantic\nmapping model. Our experiments involving various LLMs demonstrate that our\napproach achieves comparable robustness performance to existing watermark\nmethods. Additionally, the text generated by our method has perplexity\ncomparable to that of \\emph{un-watermarked} LLMs while maintaining security\neven under various attacks.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13928", "title": "Image based Crop Monitoring Technologies in Protected Horticulture: A\n  Review", "abstract": "Future food security is a major concern of the 21st century with the growing\nglobal population and climate changes. In addressing these challenges,\nprotected cropping ensures food production year-round and increases crop\nproduction per land area by controlling environment conditions. Maintaining the\ngrowth and health of crops in these facilities is essential to ensure optimum\nfood production. However, this is a laborious work and is currently done\nmanually. Image-based non-destructive plant phenotyping is an emerging research\narea that reduces the skilled labour cost while enhancing the monitoring of\ncrop growth, health, and identifying phenotype-genotype relations for plant\nbreeding. With the proliferations of protected infrastructures and targeted\nplants, different technologies and sensor setups are needed for image-based\ncrop monitoring. Conveyor-type plant-to-sensor systems, bench-top or\ngantry-based systems are commonly found in research facilities focussing on\nphenotyping of small, relatively short, or movable model plants. This review\nexamines the literature on crop monitoring and phenotyping platforms in both\nfield and protected facilities and explains different camera technologies and\ntheir ability to extract different plant traits. The review highlights the\nfuture research directions of image-based monitoring of commercial scale\nprotected crops where crops can be relatively tall or vertically supported\nunder semi controlled environments, which presents new challenges and is rarely\ncovered in the literature.", "field": "Computer Science", "categories": "cs.CE"}, {"arxiv_id": "2401.13929", "title": "Reinforcement Learning with Hidden Markov Models for Discovering\n  Decision-Making Dynamics", "abstract": "Major depressive disorder (MDD) presents challenges in diagnosis and\ntreatment due to its complex and heterogeneous nature. Emerging evidence\nindicates that reward processing abnormalities may serve as a behavioral marker\nfor MDD. To measure reward processing, patients perform computer-based\nbehavioral tasks that involve making choices or responding to stimulants that\nare associated with different outcomes. Reinforcement learning (RL) models are\nfitted to extract parameters that measure various aspects of reward processing\nto characterize how patients make decisions in behavioral tasks. Recent\nfindings suggest the inadequacy of characterizing reward learning solely based\non a single RL model; instead, there may be a switching of decision-making\nprocesses between multiple strategies. An important scientific question is how\nthe dynamics of learning strategies in decision-making affect the reward\nlearning ability of individuals with MDD. Motivated by the probabilistic reward\ntask (PRT) within the EMBARC study, we propose a novel RL-HMM framework for\nanalyzing reward-based decision-making. Our model accommodates learning\nstrategy switching between two distinct approaches under a hidden Markov model\n(HMM): subjects making decisions based on the RL model or opting for random\nchoices. We account for continuous RL state space and allow time-varying\ntransition probabilities in the HMM. We introduce a computationally efficient\nEM algorithm for parameter estimation and employ a nonparametric bootstrap for\ninference. We apply our approach to the EMBARC study to show that MDD patients\nare less engaged in RL compared to the healthy controls, and engagement is\nassociated with brain activities in the negative affect circuitry during an\nemotional conflict task.", "field": "Computer Science", "categories": "cs.LG,stat.AP,stat.ME,stat.ML"}, {"arxiv_id": "2401.13931", "title": "Precise Robotic Weed Spot-Spraying for Reduced Herbicide Usage and\n  Improved Environmental Outcomes -- A Real-World Case Study", "abstract": "Precise robotic weed control plays an essential role in precision\nagriculture. It can help significantly reduce the environmental impact of\nherbicides while reducing weed management costs for farmers. In this paper, we\ndemonstrate that a custom-designed robotic spot spraying tool based on computer\nvision and deep learning can significantly reduce herbicide usage on sugarcane\nfarms. We present results from field trials that compare robotic spot spraying\nagainst industry-standard broadcast spraying, by measuring the weed control\nefficacy, the reduction in herbicide usage, and the water quality improvements\nin irrigation runoff. The average results across 25 hectares of field trials\nshow that spot spraying on sugarcane farms is 97% as effective as broadcast\nspraying and reduces herbicide usage by 35%, proportionally to the weed\ndensity. For specific trial strips with lower weed pressure, spot spraying\nreduced herbicide usage by up to 65%. Water quality measurements of\nirrigation-induced runoff, three to six days after spraying, showed reductions\nin the mean concentration and mean load of herbicides of 39% and 54%,\nrespectively, compared to broadcast spraying. These promising results reveal\nthe capability of spot spraying technology to reduce herbicide usage on\nsugarcane farms without impacting weed control and potentially providing\nsustained water quality benefits.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13934", "title": "MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for\n  Deformable MR-CT Registration", "abstract": "Deformable image registration is an essential approach for medical image\nanalysis.This paper introduces MambaMorph, an innovative multi-modality\ndeformable registration network, specifically designed for Magnetic Resonance\n(MR) and Computed Tomography (CT) image alignment. MambaMorph stands out with\nits Mamba-based registration module and a contrastive feature learning\napproach, addressing the prevalent challenges in multi-modality registration.\nThe network leverages Mamba blocks for efficient long-range modeling and\nhigh-dimensional data processing, coupled with a feature extractor that learns\nfine-grained features for enhanced registration accuracy. Experimental results\nshowcase MambaMorph's superior performance over existing methods in MR-CT\nregistration, underlining its potential in clinical applications. This work\nunderscores the significance of feature learning in multi-modality registration\nand positions MambaMorph as a trailblazing solution in this field. The code for\nMambaMorph is available at: https://github.com/Guo-Stone/MambaMorph.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13935", "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse", "abstract": "Counterfactuals and counterfactual reasoning underpin numerous techniques for\nauditing and understanding artificial intelligence (AI) systems. The\ntraditional paradigm for counterfactual reasoning in this literature is the\ninterventional counterfactual, where hypothetical interventions are imagined\nand simulated. For this reason, the starting point for causal reasoning about\nlegal protections and demographic data in AI is an imagined intervention on a\nlegally-protected characteristic, such as ethnicity, race, gender, disability,\nage, etc. We ask, for example, what would have happened had your race been\ndifferent? An inherent limitation of this paradigm is that some demographic\ninterventions -- like interventions on race -- may not translate into the\nformalisms of interventional counterfactuals. In this work, we explore a new\nparadigm based instead on the backtracking counterfactual, where rather than\nimagine hypothetical interventions on legally-protected characteristics, we\nimagine alternate initial conditions while holding these characteristics fixed.\nWe ask instead, what would explain a counterfactual outcome for you as you\nactually are or could be? This alternate framework allows us to address many of\nthe same social concerns, but to do so while asking fundamentally different\nquestions that do not rely on demographic interventions.", "field": "Computer Science", "categories": "cs.AI,cs.CY,stat.ML"}, {"arxiv_id": "2401.13936", "title": "Learning-based sensing and computing decision for data freshness in edge\n  computing-enabled networks", "abstract": "As the demand on artificial intelligence (AI)-based applications increases,\nthe freshness of sensed data becomes crucial in the wireless sensor networks.\nSince those applications require a large amount of computation for processing\nthe sensed data, it is essential to offload the computation load to the edge\ncomputing (EC) server. In this paper, we propose the sensing and computing\ndecision (SCD) algorithms for data freshness in the EC-enabled wireless sensor\nnetworks. We define the {\\eta}-coverage probability to show the probability of\nmaintaining fresh data for more than {\\eta} ratio of the network, where the\nspatial-temporal correlation of information is considered. We then propose the\nprobability-based SCD for the single pre-charged sensor case with providing the\noptimal point after deriving the {\\eta}-coverage probability. We also propose\nthe reinforcement learning (RL)- based SCD by training the SCD policy of\nsensors for both the single pre-charged and multiple energy harvesting (EH)\nsensor cases, to make a real-time decision based on its observation. Our\nsimulation results verify the performance of the proposed algorithms under\nvarious environment settings, and show that the RL-based SCD algorithm achieves\nhigher performance compared to baseline algorithms for both the single\npre-charged sensor and multiple EH sensor cases.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13937", "title": "Self-supervised Video Object Segmentation with Distillation Learning of\n  Deformable Attention", "abstract": "Video object segmentation is a fundamental research problem in computer\nvision. Recent techniques have often applied attention mechanism to object\nrepresentation learning from video sequences. However, due to temporal changes\nin the video data, attention maps may not well align with the objects of\ninterest across video frames, causing accumulated errors in long-term video\nprocessing. In addition, existing techniques have utilised complex\narchitectures, requiring highly computational complexity and hence limiting the\nability to integrate video object segmentation into low-powered devices. To\naddress these issues, we propose a new method for self-supervised video object\nsegmentation based on distillation learning of deformable attention.\nSpecifically, we devise a lightweight architecture for video object\nsegmentation that is effectively adapted to temporal changes. This is enabled\nby deformable attention mechanism, where the keys and values capturing the\nmemory of a video sequence in the attention module have flexible locations\nupdated across frames. The learnt object representations are thus adaptive to\nboth the spatial and temporal dimensions. We train the proposed architecture in\na self-supervised fashion through a new knowledge distillation paradigm where\ndeformable attention maps are integrated into the distillation loss. We\nqualitatively and quantitatively evaluate our method and compare it with\nexisting methods on benchmark datasets including DAVIS 2016/2017 and\nYouTube-VOS 2018/2019. Experimental results verify the superiority of our\nmethod via its achieved state-of-the-art performance and optimal memory usage.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13940", "title": "How Are Paid and Volunteer Open Source Developers Different? A Study of\n  the Rust Project", "abstract": "It is now commonplace for organizations to pay developers to work on specific\nopen source software (OSS) projects to pursue their business goals. Such paid\ndevelopers work alongside voluntary contributors, but given the different\nmotivations of these two groups of developers, conflict may arise, which may\npose a threat to a project's sustainability. This paper presents an empirical\nstudy of paid developers and volunteers in Rust, a popular open source\nprogramming language project. Rust is a particularly interesting case given\nconsiderable concerns about corporate participation. We compare volunteers and\npaid developers through contribution characteristics and long-term\nparticipation, and solicit volunteers' perceptions on paid developers. We find\nthat core paid developers tend to contribute more frequently; commits\ncontributed by one-time paid developers have bigger sizes; peripheral paid\ndevelopers implement more features; and being paid plays a positive role in\nbecoming a long-term contributor. We also find that volunteers do have some\nprejudices against paid developers. This study suggests that the dichotomous\nview of paid vs. volunteer developers is too simplistic and that further\nsubgroups can be identified. Companies should become more sensitive to how they\nengage with OSS communities, in certain ways as suggested by this study.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.13941", "title": "AC-Driven Series Elastic Electrohydraulic Actuator for Stable and Smooth\n  Displacement Output", "abstract": "Soft electrohydraulic actuators known as HASEL actuators have attracted\nwidespread research interest due to their outstanding dynamic performance and\nhigh output power. However, the displacement of electrohydraulic actuators\nusually declines with time under constant DC voltage, which hampers its\nprospective application. A mathematical model is firstly established to not\nonly explain the decrease in displacement under DC voltage but also predict the\nrelatively stable displacement with oscillation under AC square wave voltage.\nThe mathematical model is validated since the actual displacement confirms the\ntrend observed by our model. To smooth the displacement oscillation introduced\nby AC voltage, a serial elastic component is incorporated to form a SE-HASEL\nactuator. A feedback control with a proportion-integration algorithm enables\nthe SE-HASEL actuator to eliminate the obstinate displacement hysteresis. Our\nresults revealed that, through our methodology, the SE-HASEL actuator can give\nstable and smooth displacement and is capable of absorbing external impact\ndisturbance simultaneously. A rotary joint based on the SE-HASEL actuator is\ndeveloped to reflect its possibility to generate a common rotary motion for\nwide robotic applications. More importantly, this paper also proposes a highly\naccurate needle biopsy robot that can be utilized in MRI-guide surgical\nprocedures. Overall, we have achieved AC-driven series elastic electrohydraulic\nactuators that can exhibit stable and smooth displacement output.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13942", "title": "StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion\n  Models", "abstract": "The ability to fine-tune generative models for text-to-image generation tasks\nis crucial, particularly facing the complexity involved in accurately\ninterpreting and visualizing textual inputs. While LoRA is efficient for\nlanguage model adaptation, it often falls short in text-to-image tasks due to\nthe intricate demands of image generation, such as accommodating a broad\nspectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a\nspecialized fine-tuning approach tailored for text-to-image models. StyleInject\ncomprises multiple parallel low-rank parameter matrices, maintaining the\ndiversity of visual features. It dynamically adapts to varying styles by\nadjusting the variance of visual features based on the characteristics of the\ninput signal. This approach significantly minimizes the impact on the original\nmodel's text-image alignment capabilities while adeptly adapting to various\nstyles in transfer learning. StyleInject proves particularly effective in\nlearning from and enhancing a range of advanced, community-fine-tuned\ngenerative models. Our comprehensive experiments, including both small-sample\nand large-scale data fine-tuning as well as base model distillation, show that\nStyleInject surpasses traditional LoRA in both text-image semantic consistency\nand human preference evaluation, all while ensuring greater parameter\nefficiency.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13945", "title": "General Automatic Solution Generation of Social Problems", "abstract": "Given the escalating intricacy and multifaceted nature of contemporary social\nsystems, manually generating solutions to address pertinent social issues has\nbecome a formidable task. In response to this challenge, the rapid development\nof artificial intelligence has spurred the exploration of computational\nmethodologies aimed at automatically generating solutions. However, current\nmethods for auto-generation of solutions mainly concentrate on local social\nregulations that pertain to specific scenarios. Here, we report an automatic\nsocial operating system (ASOS) designed for general social solution generation,\nwhich is built upon agent-based models, enabling both global and local analyses\nand regulations of social problems across spatial and temporal dimensions. ASOS\nadopts a hypergraph with extensible social semantics for a comprehensive and\nstructured representation of social dynamics. It also incorporates a\ngeneralized protocol for standardized hypergraph operations and a symbolic\nhybrid framework that delivers interpretable solutions, yielding a balance\nbetween regulatory efficacy and function viability. To demonstrate the\neffectiveness of ASOS, we apply it to the domain of averting extreme events\nwithin international oil futures markets. By generating a new trading role\nsupplemented by new mechanisms, ASOS can adeptly discern precarious market\nconditions and make front-running interventions for non-profit purposes. This\nstudy demonstrates that ASOS provides an efficient and systematic approach for\ngenerating solutions for enhancing our society.", "field": "Computer Science", "categories": "cs.CY,cs.AI,cs.CE,cs.MA"}, {"arxiv_id": "2401.13947", "title": "Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy\n  Trading", "abstract": "Utilizing distributed renewable and energy storage resources in local\ndistribution networks via peer-to-peer (P2P) energy trading has long been\ntouted as a solution to improve energy systems' resilience and sustainability.\nConsumers and prosumers (those who have energy generation resources), however,\ndo not have the expertise to engage in repeated P2P trading, and the\nzero-marginal costs of renewables present challenges in determining fair market\nprices. To address these issues, we propose multi-agent reinforcement learning\n(MARL) frameworks to help automate consumers' bidding and management of their\nsolar PV and energy storage resources, under a specific P2P clearing mechanism\nthat utilizes the so-called supply-demand ratio. In addition, we show how the\nMARL frameworks can integrate physical network constraints to realize voltage\ncontrol, hence ensuring physical feasibility of the P2P energy trading and\npaving way for real-world implementations.", "field": "Computer Science", "categories": "eess.SY,cs.LG,cs.MA,cs.SY"}, {"arxiv_id": "2401.13950", "title": "AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding\n  for Multi-Object Tracking", "abstract": "Many multi-object tracking (MOT) approaches, which employ the Kalman Filter\nas a motion predictor, assume constant velocity and Gaussian-distributed\nfiltering noises. These assumptions render the Kalman Filter-based trackers\neffective in linear motion scenarios. However, these linear assumptions serve\nas a key limitation when estimating future object locations within scenarios\ninvolving non-linear motion and occlusions. To address this issue, we propose a\nmotion-based MOT approach with an adaptable motion predictor, called AM-SORT,\nwhich adapts to estimate non-linear uncertainties. AM-SORT is a novel extension\nof the SORT-series trackers that supersedes the Kalman Filter with the\ntransformer architecture as a motion predictor. We introduce a historical\ntrajectory embedding that empowers the transformer to extract spatio-temporal\nfeatures from a sequence of bounding boxes. AM-SORT achieves competitive\nperformance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1\nand 55.6 HOTA. We conduct extensive experiments to demonstrate the\neffectiveness of our method in predicting non-linear movement under occlusions.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13952", "title": "Randomized Response with Gradual Release of Privacy Budget", "abstract": "An algorithm is developed to gradually relax the Differential Privacy (DP)\nguarantee of a randomized response. The output from each relaxation maintains\nthe same probability distribution as a standard randomized response with the\nequivalent DP guarantee, ensuring identical utility as the standard approach.\nThe entire relaxation process is proven to have the same DP guarantee as the\nmost recent relaxed guarantee.\n  The DP relaxation algorithm is adaptable to any Local Differential Privacy\n(LDP) mechanisms relying on randomized response. It has been seamlessly\nintegrated into RAPPOR, an LDP crowdsourcing string-collecting tool, to\noptimize the utility of estimating the frequency of collected data.\nAdditionally, it facilitates the relaxation of the DP guarantee for mean\nestimation based on randomized response. Finally, numerical experiments have\nbeen conducted to validate the utility and DP guarantee of the algorithm.", "field": "Computer Science", "categories": "cs.CR"}, {"arxiv_id": "2401.13956", "title": "A New Image Quality Database for Multiple Industrial Processes", "abstract": "Recent years have witnessed a broader range of applications of image\nprocessing technologies in multiple industrial processes, such as smoke\ndetection, security monitoring, and workpiece inspection. Different kinds of\ndistortion types and levels must be introduced into an image during the\nprocesses of acquisition, compression, transmission, storage, and display,\nwhich might heavily degrade the image quality and thus strongly reduce the\nfinal display effect and clarity. To verify the reliability of existing image\nquality assessment methods, we establish a new industrial process image\ndatabase (IPID), which contains 3000 distorted images generated by applying\ndifferent levels of distortion types to each of the 50 source images. We\nconduct the subjective test on the aforementioned 3000 images to collect their\nsubjective quality ratings in a well-suited laboratory environment. Finally, we\nperform comparison experiments on IPID database to investigate the performance\nof some objective image quality assessment algorithms. The experimental results\nshow that the state-of-the-art image quality assessment methods have difficulty\nin predicting the quality of images that contain multiple distortion types.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13957", "title": "Automatic Tissue Traction with Haptics-Enabled Forceps for Minimally\n  Invasive Surgery", "abstract": "A common limitation of autonomous tissue manipulation in robotic minimally\ninvasive surgery (MIS) is the absence of force sensing and control at the tool\nlevel. Recently, our team has developed haptics-enabled forceps that can\nsimultaneously measure the grasping and pulling forces during tissue\nmanipulation. Based on this design, here we further present a method to\nautomate tissue traction with controlled grasping and pulling forces.\nSpecifically, the grasping stage relies on a controlled grasping force, while\nthe pulling stage is under the guidance of a controlled pulling force. Notably,\nduring the pulling process, the simultaneous control of both grasping and\npulling forces is also enabled for more precise tissue traction, achieved\nthrough force decoupling. The force controller is built upon a static model of\ntissue manipulation, considering the interaction between the haptics-enabled\nforceps and soft tissue. The efficacy of this force control approach is\nvalidated through a series of experiments comparing targeted, estimated, and\nactual reference forces. To verify the feasibility of the proposed method in\nsurgical applications, various tissue resections are conducted on ex vivo\ntissues employing a dual-arm robotic setup. Finally, we discuss the benefits of\nmulti-force control in tissue traction, evidenced through comparative analyses\nof various ex vivo tissue resections. The results affirm the feasibility of\nimplementing automatic tissue traction using micro-sized forceps with\nmulti-force control, suggesting its potential to promote autonomous MIS. A\nvideo demonstrating the experiments can be found at\nhttps://youtu.be/8fe8o8IFrjE.", "field": "Computer Science", "categories": "cs.RO,cs.HC,cs.SY,eess.SY"}, {"arxiv_id": "2401.13961", "title": "TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation\n  in VEM images", "abstract": "In this paper, we address a significant gap in the field of neuroimaging by\nintroducing the largest-to-date public benchmark, BvEM, designed specifically\nfor cortical blood vessel segmentation in Volume Electron Microscopy (VEM)\nimages. The intricate relationship between cerebral blood vessels and neural\nfunction underscores the vital role of vascular analysis in understanding brain\nhealth. While imaging techniques at macro and mesoscales have garnered\nsubstantial attention and resources, the microscale VEM imaging, capable of\nrevealing intricate vascular details, has lacked the necessary benchmarking\ninfrastructure. As researchers delve deeper into the microscale intricacies of\ncerebral vasculature, our BvEM benchmark represents a critical step toward\nunraveling the mysteries of neurovascular coupling and its impact on brain\nfunction and pathology. The BvEM dataset is based on VEM image volumes from\nthree mammal species: adult mouse, macaque, and human. We standardized the\nresolution, addressed imaging variations, and meticulously annotated blood\nvessels through semi-automatic, manual, and quality control processes, ensuring\nhigh-quality 3D segmentation. Furthermore, we developed a zero-shot cortical\nblood vessel segmentation method named TriSAM, which leverages the powerful\nsegmentation model SAM for 3D segmentation. To lift SAM from 2D segmentation to\n3D volume segmentation, TriSAM employs a multi-seed tracking framework,\nleveraging the reliability of certain image planes for tracking while using\nothers to identify potential turning points. This approach, consisting of\nTri-Plane selection, SAM-based tracking, and recursive redirection, effectively\nachieves long-term 3D blood vessel segmentation without model training or\nfine-tuning. Experimental results show that TriSAM achieved superior\nperformances on the BvEM benchmark across three species.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13964", "title": "An Extensible Framework for Open Heterogeneous Collaborative Perception", "abstract": "Collaborative perception aims to mitigate the limitations of single-agent\nperception, such as occlusions, by facilitating data exchange among multiple\nagents. However, most current works consider a homogeneous scenario where all\nagents use identity sensors and perception models. In reality, heterogeneous\nagent types may continually emerge and inevitably face a domain gap when\ncollaborating with existing agents. In this paper, we introduce a new open\nheterogeneous problem: how to accommodate continually emerging new\nheterogeneous agent types into collaborative perception, while ensuring high\nperception performance and low integration cost? To address this problem, we\npropose HEterogeneous ALliance (HEAL), a novel extensible collaborative\nperception framework. HEAL first establishes a unified feature space with\ninitial agents via a novel multi-scale foreground-aware Pyramid Fusion network.\nWhen heterogeneous new agents emerge with previously unseen modalities or\nmodels, we align them to the established unified space with an innovative\nbackward alignment. This step only involves individual training on the new\nagent type, thus presenting extremely low training costs and high\nextensibility. It also protects new agents' model details from disclosure since\nthe training can be conducted by the agent owner locally. To enrich agents'\ndata heterogeneity, we bring OPV2V-H, a new large-scale dataset with more\ndiverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets\nshow that HEAL surpasses SOTA methods in performance while reducing the\ntraining parameters by 91.5% when integrating 3 new agent types. Code and data\nare available at: https://github.com/yifanlu0227/HEAL.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13965", "title": "Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised\n  Domain Generalization", "abstract": "Beyond attaining domain generalization (DG), visual recognition models should\nalso be data-efficient during learning by leveraging limited labels. We study\nthe problem of Semi-Supervised Domain Generalization (SSDG) which is crucial\nfor real-world applications like automated healthcare. SSDG requires learning a\ncross-domain generalizable model when the given training data is only partially\nlabelled. Empirical investigations reveal that the DG methods tend to\nunderperform in SSDG settings, likely because they are unable to exploit the\nunlabelled data. Semi-supervised learning (SSL) shows improved but still\ninferior results compared to fully-supervised learning. A key challenge, faced\nby the best-performing SSL-based SSDG methods, is selecting accurate\npseudo-labels under multiple domain shifts and reducing overfitting to source\ndomains under limited labels. In this work, we propose new SSDG approach, which\nutilizes a novel uncertainty-guided pseudo-labelling with model averaging\n(UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to\nimprove pseudo-labelling selection, addressing poor model calibration under\nmulti-source unlabelled data. The UPL technique, enhanced by our novel model\naveraging (MA) strategy, mitigates overfitting to source domains with limited\nlabels. Extensive experiments on key representative DG datasets suggest that\nour method demonstrates effectiveness against existing methods. Our code and\nchosen labelled data seeds are available on GitHub:\nhttps://github.com/Adnan-Khan7/UPLM", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13967", "title": "Perceptual-oriented Learned Image Compression with Dynamic Kernel", "abstract": "In this paper, we extend our prior research named DKIC and propose the\nperceptual-oriented learned image compression method, PO-DKIC. Specifically,\nDKIC adopts a dynamic kernel-based dynamic residual block group to enhance the\ntransform coding and an asymmetric space-channel context entropy model to\nfacilitate the estimation of gaussian parameters. Based on DKIC, PO-DKIC\nintroduces PatchGAN and LPIPS loss to enhance visual quality. Furthermore, to\nmaximize the overall perceptual quality under a rate constraint, we formulate\nthis challenge into a constrained programming problem and use the Linear\nInteger Programming method for resolution. The experiments demonstrate that our\nproposed method can generate realistic images with richer textures and finer\ndetails when compared to state-of-the-art image compression techniques.", "field": "Computer Science", "categories": "cs.MM"}, {"arxiv_id": "2401.13968", "title": "Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks", "abstract": "A reliable long-term time-series forecaster is highly demanded in practice\nbut comes across many challenges such as low computational and memory\nfootprints as well as robustness against dynamic learning environments. This\npaper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic\nlong-term time-series forecasting tasks. MANTRA relies on the concept of fast\nand slow learners where a collection of fast learners learns different aspects\nof data distributions while adapting quickly to changes. A slow learner tailors\nsuitable representations to fast learners. Fast adaptations to dynamic\nenvironments are achieved using the universal representation transformer layers\nproducing task-adapted representations with a small number of parameters. Our\nexperiments using four datasets with different prediction lengths demonstrate\nthe advantage of our approach with at least $3\\%$ improvements over the\nbaseline algorithms for both multivariate and univariate settings. Source codes\nof MANTRA are publicly available in\n\\url{https://github.com/anwarmaxsum/MANTRA}.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13970", "title": "CUI@CHI 2024: Building Trust in CUIs-From Design to Deployment", "abstract": "Conversational user interfaces (CUIs) have become an everyday technology for\npeople the world over, as well as a booming area of research. Advances in voice\nsynthesis and the emergence of chatbots powered by large language models\n(LLMs), notably ChatGPT, have pushed CUIs to the forefront of human-computer\ninteraction (HCI) research and practice. Now that these technologies enable an\nelemental level of usability and user experience (UX), we must turn our\nattention to higher-order human factors: trust and reliance. In this workshop,\nwe aim to bring together a multidisciplinary group of researchers and\npractitioners invested in the next phase of CUI design. Through keynotes,\npresentations, and breakout sessions, we will share our knowledge, identify\ncutting-edge resources, and fortify an international network of CUI scholars.\nIn particular, we will engage with the complexity of trust and reliance as\nattitudes and behaviours that emerge when people interact with conversational\nagents.", "field": "Computer Science", "categories": "cs.HC"}, {"arxiv_id": "2401.13973", "title": "Optimal design of unimorph-type cantilevered piezoelectric energy\n  harvesters using level set-based topology optimization by considering\n  manufacturability", "abstract": "In this study, we proposed a design methodology for a piezoelectric\nenergy-harvesting device optimized for maximal power generation at a designated\nfrequency using topology optimization. The proposed methodology emphasizes the\ndesign of a unimorph-type piezoelectric energy harvester, wherein a\npiezoelectric film is affixed to a singular side of a silicon cantilever beam.\nBoth the substrate and the piezoelectric film components underwent concurrent\noptimization. Constraints were imposed to ensure that the resultant design is\namenable to microfabrication, with specific emphasis on the etchability of\npiezoelectric energy harvesters. Several numerical examples were provided to\nvalidate the efficacy of the proposed method. The results showed that the\nproposed method derives both the substrate and piezoelectric designs that\nmaximize the electromechanical coupling coefficient and allows the\neigenfrequency of the device and minimum output voltage to be set to the\ndesired values. Furthermore, the proposed method can provide solutions that\nsatisfy the cross-sectional shape, substrate-depend, and minimum output voltage\nconstraints. The solutions obtained by the proposed method are manufacturable\nin the field of microfabrication.", "field": "Computer Science", "categories": "cs.CE"}, {"arxiv_id": "2401.13974", "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation\n  Capabilities in Pretrained Diffusion Models", "abstract": "Recent text-to-image generation models have demonstrated incredible success\nin generating images that faithfully follow input prompts. However, the\nrequirement of using words to describe a desired concept provides limited\ncontrol over the appearance of the generated concepts. In this work, we address\nthis shortcoming by proposing an approach to enable personalization\ncapabilities in existing text-to-image diffusion models. We propose a novel\narchitecture (BootPIG) that allows a user to provide reference images of an\nobject in order to guide the appearance of a concept in the generated images.\n  The proposed BootPIG architecture makes minimal modifications to a pretrained\ntext-to-image diffusion model and utilizes a separate UNet model to steer the\ngenerations toward the desired appearance. We introduce a training procedure\nthat allows us to bootstrap personalization capabilities in the BootPIG\narchitecture using data generated from pretrained text-to-image models, LLM\nchat agents, and image segmentation models. In contrast to existing methods\nthat require several days of pretraining, the BootPIG architecture can be\ntrained in approximately 1 hour. Experiments on the DreamBooth dataset\ndemonstrate that BootPIG outperforms existing zero-shot methods while being\ncomparable with test-time finetuning approaches. Through a user study, we\nvalidate the preference for BootPIG generations over existing methods both in\nmaintaining fidelity to the reference object's appearance and aligning with\ntextual prompts.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.GR"}, {"arxiv_id": "2401.13976", "title": "Learning to Manipulate Artistic Images", "abstract": "Recent advancement in computer vision has significantly lowered the barriers\nto artistic creation. Exemplar-based image translation methods have attracted\nmuch attention due to flexibility and controllability. However, these methods\nhold assumptions regarding semantics or require semantic information as the\ninput, while accurate semantics is not easy to obtain in artistic images.\nBesides, these methods suffer from cross-domain artifacts due to training data\nprior and generate imprecise structure due to feature compression in the\nspatial domain. In this paper, we propose an arbitrary Style Image Manipulation\nNetwork (SIM-Net), which leverages semantic-free information as guidance and a\nregion transportation strategy in a self-supervised manner for image\ngeneration. Our method balances computational efficiency and high resolution to\na certain extent. Moreover, our method facilitates zero-shot style image\nmanipulation. Both qualitative and quantitative experiments demonstrate the\nsuperiority of our method over state-of-the-art methods.Code is available at\nhttps://github.com/SnailForce/SIM-Net.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13977", "title": "Evaluating the Determinants of Mode Choice Using Statistical and Machine\n  Learning Techniques in the Indian Megacity of Bengaluru", "abstract": "The decision making involved behind the mode choice is critical for\ntransportation planning. While statistical learning techniques like discrete\nchoice models have been used traditionally, machine learning (ML) models have\ngained traction recently among the transportation planners due to their higher\npredictive performance. However, the black box nature of ML models pose\nsignificant interpretability challenges, limiting their practical application\nin decision and policy making. This study utilised a dataset of $1350$\nhouseholds belonging to low and low-middle income bracket in the city of\nBengaluru to investigate mode choice decision making behaviour using\nMultinomial logit model and ML classifiers like decision trees, random forests,\nextreme gradient boosting and support vector machines. In terms of accuracy,\nrandom forest model performed the best ($0.788$ on training data and $0.605$ on\ntesting data) compared to all the other models. This research has adopted\nmodern interpretability techniques like feature importance and individual\nconditional expectation plots to explain the decision making behaviour using ML\nmodels. A higher travel costs significantly reduce the predicted probability of\nbus usage compared to other modes (a $0.66\\%$ and $0.34\\%$ reduction using\nRandom Forests and XGBoost model for $10\\%$ increase in travel cost). However,\nreducing travel time by $10\\%$ increases the preference for the metro ($0.16\\%$\nin Random Forests and 0.42% in XGBoost). This research augments the ongoing\nresearch on mode choice analysis using machine learning techniques, which would\nhelp in improving the understanding of the performance of these models with\nreal-world data in terms of both accuracy and interpretability.", "field": "Computer Science", "categories": "cs.LG,econ.GN,q-fin.EC"}, {"arxiv_id": "2401.13979", "title": "Leeroo Orchestrator: Elevating LLMs Performance Through Model\n  Integration", "abstract": "In this paper, we propose an architecture to harness the collective knowledge\nof multiple trained LLMs to create a new state-of-the-art. At the core of this\nframework is a LLM-based orchestrator that is adept at picking the right\nunderlying LLM experts for optimal task execution. Inspired by self-play in\nreinforcement learning, we created a loop of query generation, orchestration,\nand evaluation to generate training data for the orchestrator. Our evaluation\nfocused on the MMLU benchmark, employing models with 7B, 13B, and 34B\nparameters available on Hugging Face. The results demonstrate new\nstate-of-the-art open-source models: Our Leeroo orchestrator achieves\nperformance on par with the Mixtral model while incurring only two-thirds of\nits cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by\nover 5% at the same cost level, reaching an accuracy of 75.9%. Further\nenhancements were observed when integrating GPT4 into the underlying model\npool. The Leeroo orchestrator nearly matches GPT4's performance at half the\ncost and even exceeds GPT4's results with a 25% cost reduction. These findings\nillustrate the potential of our architecture in creating state-of-the-art and\ncost-effective LLMs by optimizing the synergy between multiple LLMs to achieve\nsuperior performance outcomes.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.13980", "title": "A Nearly Information Theoretically Secure Approach for Semantic\n  Communications over Wiretap Channel", "abstract": "This paper addresses the challenge of achieving information-theoretic\nsecurity in semantic communication (SeCom) over a wiretap channel, where a\nlegitimate receiver coexists with an eavesdropper experiencing a poorer channel\ncondition. Despite previous efforts to secure SeCom against eavesdroppers,\nachieving information-theoretic security in such schemes remains an open issue.\nIn this work, we propose a secure digital SeCom approach based on superposition\ncodes, aiming to attain nearly information-theoretic security. Our proposed\nmethod involves associating semantic information with satellite constellation\npoints within a double-layered constellation map, where cloud center\nconstellation points are randomly selected. By carefully allocating power\nbetween these two layers of constellation, we ensure that the symbol error\nprobability (SEP) of the eavesdropper decoding satellite constellation points\nis nearly equivalent to random guessing, while maintaining a low SEP for the\nlegitimate receiver to successfully decode the semantic information. Simulation\nresults showcase that the Peak Signal-to-Noise Ratio (PSNR) and Mean Squared\nError (MSE) for the eavesdropper's reconstructed data, using our proposed\nmethod, can range from decoding Gaussian-distributed random noise to\napproaching the variance of the data. This validates the ability of our method\nto achieve nearly information-theoretic security, demonstrating superior data\nsecurity compared to benchmark methods.", "field": "Computer Science", "categories": "cs.IT,eess.IV,math.IT"}, {"arxiv_id": "2401.13985", "title": "A new analysis of empirical interpolation methods and Chebyshev greedy\n  algorithms", "abstract": "We present new convergence estimates of generalized empirical interpolation\nmethods in terms of the entropy numbers of the parametrized function class. Our\nanalysis is transparent and leads to sharper convergence rates than the\nclassical analysis via the Kolmogorov n-width. In addition, we also derive\nnovel entropy-based convergence estimates of the Chebyshev greedy algorithm for\nsparse n-term nonlinear approximation of a target function. This also improves\nclassical convergence analysis when corresponding entropy numbers decay fast\nenough.", "field": "Computer Science", "categories": "math.NA,cs.NA,41A46, 41A65, 65J05, 65M12"}, {"arxiv_id": "2401.13986", "title": "Towards Consistent Natural-Language Explanations via\n  Explanation-Consistency Finetuning", "abstract": "Large language models (LLMs) often generate convincing, fluent explanations.\nHowever, different from humans, they often generate inconsistent explanations\non different inputs. For example, an LLM may generate the explanation \"all\nbirds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile\nanswer \"no\" to the related question \"Can penguins fly?\". Explanations should be\nconsistent across related examples so that they allow a human to simulate the\nLLM's decision process on multiple examples. We propose explanation-consistency\nfinetuning (EC-finetuning), a method that adapts LLMs to generate more\nconsistent natural-language explanations on related examples. EC-finetuning\ninvolves finetuning LLMs on synthetic data that is carefully constructed to\ncontain consistent explanations. Across a variety of question-answering\ndatasets in various domains, EC-finetuning yields a 10.0% relative explanation\nconsistency improvement on four finetuning datasets, and generalizes to seven\nout-of-distribution datasets not seen during finetuning (+4.5% relative). Code\nis available at https://github.com/yandachen/explanation-consistency-finetuning .", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.13987", "title": "Cross-Domain Few-Shot Learning via Adaptive Transformer Networks", "abstract": "Most few-shot learning works rely on the same domain assumption between the\nbase and the target tasks, hindering their practical applications. This paper\nproposes an adaptive transformer network (ADAPTER), a simple but effective\nsolution for cross-domain few-shot learning where there exist large domain\nshifts between the base task and the target task. ADAPTER is built upon the\nidea of bidirectional cross-attention to learn transferable features between\nthe two domains. The proposed architecture is trained with DINO to produce\ndiverse, and less biased features to avoid the supervision collapse problem.\nFurthermore, the label smoothing approach is proposed to improve the\nconsistency and reliability of the predictions by also considering the\npredicted labels of the close samples in the embedding space. The performance\nof ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it\noutperforms prior arts with significant margins.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13992", "title": "Diffusion-based Data Augmentation for Object Counting Problems", "abstract": "Crowd counting is an important problem in computer vision due to its wide\nrange of applications in image understanding. Currently, this problem is\ntypically addressed using deep learning approaches, such as Convolutional\nNeural Networks (CNNs) and Transformers. However, deep networks are data-driven\nand are prone to overfitting, especially when the available labeled crowd\ndataset is limited. To overcome this limitation, we have designed a pipeline\nthat utilizes a diffusion model to generate extensive training data. We are the\nfirst to generate images conditioned on a location dot map (a binary dot map\nthat specifies the location of human heads) with a diffusion model. We are also\nthe first to use these diverse synthetic data to augment the crowd counting\nmodels. Our proposed smoothed density map input for ControlNet significantly\nimproves ControlNet's performance in generating crowds in the correct\nlocations. Also, Our proposed counting loss for the diffusion model effectively\nminimizes the discrepancies between the location dot map and the crowd images\ngenerated. Additionally, our innovative guidance sampling further directs the\ndiffusion process toward regions where the generated crowd images align most\naccurately with the location dot map. Collectively, we have enhanced\nControlNet's ability to generate specified objects from a location dot map,\nwhich can be used for data augmentation in various counting problems. Moreover,\nour framework is versatile and can be easily adapted to all kinds of counting\nproblems. Extensive experiments demonstrate that our framework improves the\ncounting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS\ndatasets, showcasing its effectiveness.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13996", "title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent\n  Self-Evolution", "abstract": "This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy\nfor enhancing the adaptability and flexibility of AI agents through inter-task\nself-evolution. Unlike existing methods focused on intra-task learning, ICE\npromotes the transfer of knowledge between tasks for genuine self-evolution,\nsimilar to human experience learning. The strategy dynamically investigates\nplanning and execution trajectories, consolidates them into simplified\nworkflows and pipelines, and exploits them for improved task execution. Our\nexperiments on the XAgent framework demonstrate ICE's effectiveness, reducing\nAPI calls by as much as 80% and significantly decreasing the demand for the\nmodel's capability. Specifically, when combined with GPT-3.5, ICE's performance\nmatches that of raw GPT-4 across various agent tasks. We argue that this\nself-evolution approach represents a paradigm shift in agent design,\ncontributing to a more robust AI community and ecosystem, and moving a step\ncloser to full autonomy.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14000", "title": "Mapping the Design Space of Teachable Social Media Feed Experiences", "abstract": "Social media feeds are deeply personal spaces that reflect individual values\nand preferences. However, top-down, platform-wide content algorithms can reduce\nusers' sense of agency and fail to account for nuanced experiences and values.\nDrawing on the paradigm of interactive machine teaching (IMT), an interaction\nframework for non-expert algorithmic adaptation, we map out a design space for\nteachable social media feed experiences to empower agential, personalized feed\ncuration. To do so, we conducted a think-aloud study (N=24) featuring four\nsocial media platforms -- Instagram, Mastodon, TikTok, and Twitter -- to\nunderstand key signals users leveraged to determine the value of a post in\ntheir feed. We synthesized users' signals into taxonomies that, when combined\nwith user interviews, inform five design principles that extend IMT into the\nsocial media setting. We finally embodied our principles into three feed\ndesigns that we present as sensitizing concepts for teachable feed experiences\nmoving forward.", "field": "Computer Science", "categories": "cs.HC"}, {"arxiv_id": "2401.14003", "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on\n  Commonsense Knowledge Bases", "abstract": "Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has\nbeen explored as a way to acquire new commonsense knowledge based on reference\nknowledge in the original CSKBs and external prior knowledge. Despite the\nadvancement of Large Language Models (LLM) and prompt engineering techniques in\nvarious reasoning tasks, they still struggle to deal with CSKB reasoning. One\nof the problems is that it is hard for them to acquire explicit relational\nconstraints in CSKBs from only in-context exemplars, due to a lack of symbolic\nreasoning capabilities (Bengio et al., 2021). To this end, we proposed\n**ConstraintChecker**, a plugin over prompting techniques to provide and check\nexplicit constraints. When considering a new knowledge instance,\nConstraintChecker employs a rule-based module to produce a list of constraints,\nthen it uses a zero-shot learning module to check whether this knowledge\ninstance satisfies all constraints. The acquired constraint-checking result is\nthen aggregated with the output of the main prompting technique to produce the\nfinal output. Experimental results on CSKB Reasoning benchmarks demonstrate the\neffectiveness of our method by bringing consistent improvements over all\nprompting methods. Codes and data are available at\n\\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14005", "title": "Cyber-Twin: Digital Twin-boosted Autonomous Attack Detection for\n  Vehicular Ad-Hoc Networks", "abstract": "The rapid evolution of Vehicular Ad-hoc NETworks (VANETs) has ushered in a\ntransformative era for intelligent transportation systems (ITS), significantly\nenhancing road safety and vehicular communication. However, the intricate and\ndynamic nature of VANETs presents formidable challenges, particularly in\nvehicle-to-infrastructure (V2I) communications. Roadside Units (RSUs), integral\ncomponents of VANETs, are increasingly susceptible to cyberattacks, such as\njamming and distributed denial-of-service (DDoS) attacks. These vulnerabilities\npose grave risks to road safety, potentially leading to traffic congestion and\nvehicle malfunctions. Current approaches often struggle to effectively merge\ndigital twin technology with Artificial Intelligence (AI) models to boost\nsecurity and sustainability. Our study introduces an innovative cyber-twin\nframework tailored to enhance the security of RSUs in VANETs. This framework\nuniquely combines digital twin technology with cutting-edge AI to offer a\nreal-time, dynamic representation of RSUs. This allows for detailed monitoring\nand efficient detection of threats, significantly strengthening RSU security in\nVANETs. Moreover, our framework makes a notable contribution to eco-friendly\ncommunication by improving the computational efficiency of RSUs, leading to\nincreased energy efficiency and extended hardware durability. Our results show\na considerable enhancement in resource management and attack detection,\nsurpassing the performance of existing solutions. In particular, the cyber-twin\nframework showed a substantial reduction in RSU load and an optimal balance\nbetween resource consumption and high attack detection efficiency, with a\ndefined twinning rate range of seventy-six to ninety per cent. These\nadvancements underscore our commitment to developing sustainable, secure, and\nresilient vehicular communication systems for the future of smart cities.", "field": "Computer Science", "categories": "cs.CR"}, {"arxiv_id": "2401.14008", "title": "Massive Unsourced Random Access for Near-Field Communications", "abstract": "This paper investigates the unsourced random access (URA) problem with a\nmassive multiple-input multiple-output receiver that serves wireless devices in\nthe near-field of radiation. We employ an uncoupled transmission protocol\nwithout appending redundancies to the slot-wise encoded messages. To exploit\nthe channel sparsity for block length reduction while facing the collapsed\nsparse structure in the angular domain of near-field channels, we propose a\nsparse channel sampling method that divides the angle-distance (polar) domain\nbased on the maximum permissible coherence. Decoding starts with retrieving\nactive codewords and channels from each slot. We address the issue by\nleveraging the structured channel sparsity in the spatial and polar domains and\npropose a novel turbo-based recovery algorithm. Furthermore, we investigate an\noff-grid compressed sensing method to refine discretely estimated channel\nparameters over the continuum that improves the detection performance.\nAfterward, without the assistance of redundancies, we recouple the separated\nmessages according to the similarity of the users' channel information and\npropose a modified K-medoids method to handle the constraints and collisions\ninvolved in channel clustering. Simulations reveal that via exploiting the\nchannel sparsity, the proposed URA scheme achieves high spectral efficiency and\nsurpasses existing multi-slot-based schemes. Moreover, with more measurements\nprovided by the overcomplete channel sampling, the near-field-suited scheme\noutperforms its counterpart of the far-field.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.14009", "title": "On the Feasibility of Simple Transformer for Dynamic Graph Modeling", "abstract": "Dynamic graph modeling is crucial for understanding complex structures in web\ngraphs, spanning applications in social networks, recommender systems, and\nmore. Most existing methods primarily emphasize structural dependencies and\ntheir temporal changes. However, these approaches often overlook detailed\ntemporal aspects or struggle with long-term dependencies. Furthermore, many\nsolutions overly complicate the process by emphasizing intricate module designs\nto capture dynamic evolutions. In this work, we harness the strength of the\nTransformer's self-attention mechanism, known for adeptly handling long-range\ndependencies in sequence modeling. Our approach offers a simple Transformer\nmodel tailored for dynamic graph modeling without complex modifications. We\nre-conceptualize dynamic graphs as a sequence modeling challenge and introduce\nan innovative temporal alignment technique. This technique not only captures\nthe inherent temporal evolution patterns within dynamic graphs but also\nstreamlines the modeling process of their evolution. As a result, our method\nbecomes versatile, catering to an array of applications. Our model's\neffectiveness is underscored through rigorous experiments on four real-world\ndatasets from various sectors, solidifying its potential in dynamic graph\nmodeling.", "field": "Computer Science", "categories": "cs.SI"}, {"arxiv_id": "2401.14010", "title": "Leveraging Large Models for Crafting Narrative Visualization: A Survey", "abstract": "Narrative visualization effectively transforms data into engaging stories,\nmaking complex information accessible to a broad audience. Large models,\nessential for narrative visualization, inherently facilitate this process\nthrough their superior ability to handle natural language queries and answers,\ngenerate cohesive narratives, and enhance visual communication. Inspired by\nprevious work in narrative visualization and recent advances in large models,\nwe synthesized potential tasks and opportunities for large models at various\nstages of narrative visualization. In our study, we surveyed 79 papers to\nexplore the role of large models in automating narrative visualization\ncreation. We propose a comprehensive pipeline that leverages large models for\ncrafting narrative visualization, categorizing the reviewed literature into\nfour essential phases: Data, Narration, Visualization, and Presentation.\nAdditionally, we identify ten specific tasks where large models are applied\nacross these stages. This study maps out the landscape of challenges and\nopportunities in the LM4NV process, providing insightful directions for future\nresearch and valuable guidance for scholars in the field.", "field": "Computer Science", "categories": "cs.HC"}, {"arxiv_id": "2401.14011", "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question\n  Understanding and Reasoning", "abstract": "Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose a rigorous evaluation strategy called ShiftCheck for assessing\nmultiple-choice questions. The strategy aims to reduce position bias, minimize\nthe influence of randomness on correctness, and perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.MM"}, {"arxiv_id": "2401.14013", "title": "Coordinated Guiding Vector Field Design for Ordering-Flexible\n  Multi-Robot Surface Navigation", "abstract": "We design a distributed coordinated guiding vector field (CGVF) for a group\nof robots to achieve ordering-flexible motion coordination while maneuvering on\na desired two-dimensional (2D) surface. The CGVF is characterized by three\nterms, i.e., a convergence term to drive the robots to converge to the desired\nsurface, a propagation term to provide a traversing direction for maneuvering\non the desired surface, and a coordinated term to achieve the surface motion\ncoordination with an arbitrary ordering of the robotic group. By setting the\nsurface parameters as additional virtual coordinates, the proposed approach\neliminates the potential singularity of the CGVF and enables both the global\nconvergence to the desired surface and the maneuvering on the surface from all\npossible initial conditions. The ordering-flexible surface motion coordination\nis realized by each robot to share with its neighbors only two virtual\ncoordinates, i.e. that of a given target and that of its own, which reduces the\ncommunication and computation cost in multi-robot surface navigation. Finally,\nthe effectiveness of the CGVF is substantiated by extensive numerical\nsimulations.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.14014", "title": "Theoretical Analysis of Explicit Averaging and Novel Sign Averaging in\n  Comparison-Based Search", "abstract": "In black-box optimization, noise in the objective function is inevitable.\nNoise disrupts the ranking of candidate solutions in comparison-based\noptimization, possibly deteriorating the search performance compared with a\nnoiseless scenario. Explicit averaging takes the sample average of noisy\nobjective function values and is widely used as a simple and versatile\nnoise-handling technique. Although it is suitable for various applications, it\nis ineffective if the mean is not finite. We theoretically reveal that explicit\naveraging has a negative effect on the estimation of ground-truth rankings when\nassuming stably distributed noise without a finite mean. Alternatively, sign\naveraging is proposed as a simple but robust noise-handling technique. We\ntheoretically prove that the sign averaging estimates the order of the medians\nof the noisy objective function values of a pair of points with arbitrarily\nhigh probability as the number of samples increases. Its advantages over\nexplicit averaging and its robustness are also confirmed through numerical\nexperiments.", "field": "Computer Science", "categories": "cs.NE"}, {"arxiv_id": "2401.14016", "title": "Towards Uncertainty-Aware Language Agent", "abstract": "While Language Agents have achieved promising success by placing Large\nLanguage Models at the core of a more versatile design that dynamically\ninteracts with the external world, the existing approaches neglect the notion\nof uncertainty during these interactions. We present the Uncertainty-Aware\nLanguage Agent (UALA), a framework that orchestrates the interaction between\nthe agent and the external world using uncertainty quantification. Compared\nwith other well-known counterparts like ReAct, our extensive experiments across\n3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes\ndemonstrates that UALA brings a significant improvement of performance, while\nhaving a substantially lower reliance on the external world (i.e., reduced\nnumber of tool calls and tokens). Our analyses provide various insights\nincluding the great potential of UALA compared with agent fine-tuning, and\nunderscoring the unreliably of verbalised confidence of LLMs as a proxy for\nuncertainty.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.14019", "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation\n  for Generative AI", "abstract": "In the dynamic landscape of generative NLP, traditional text processing\npipelines limit research flexibility and reproducibility, as they are tailored\nto specific dataset, task, and model combinations. The escalating complexity,\ninvolving system prompts, model-specific formats, instructions, and more, calls\nfor a shift to a structured, modular, and customizable solution. Addressing\nthis need, we present Unitxt, an innovative library for customizable textual\ndata preparation and evaluation tailored to generative language models. Unitxt\nnatively integrates with common libraries like HuggingFace and LM-eval-harness\nand deconstructs processing flows into modular components, enabling easy\ncustomization and sharing between practitioners. These components encompass\nmodel-specific formats, task prompts, and many other comprehensive dataset\nprocessing definitions. The Unitxt-Catalog centralizes these components,\nfostering collaboration and exploration in modern textual data workflows.\nBeyond being a tool, Unitxt is a community-driven platform, empowering users to\nbuild, share, and advance their pipelines collaboratively. Join the Unitxt\ncommunity at https://github.com/IBM/unitxt!", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14021", "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation", "abstract": "Retrieval-augmented language models (RaLM) have demonstrated the potential to\nsolve knowledge-intensive natural language processing (NLP) tasks by combining\na non-parametric knowledge base with a parametric language model. Instead of\nfine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to\nthe latest data and better source attribution mechanisms. Among various RaLM\napproaches, iterative RaLM delivers a better generation quality due to a more\nfrequent interaction between the retriever and the language model. Despite the\nbenefits, iterative RaLM usually encounters high overheads due to the frequent\nretrieval step. To this end, we propose RaLMSpec, a speculation-inspired\nframework that provides generic speed-up over iterative RaLM while preserving\nthe same model outputs through speculative retrieval and batched verification.\nBy further incorporating prefetching, optimal speculation stride scheduler, and\nasynchronous verification, RaLMSpec can automatically exploit the acceleration\npotential to the fullest. For naive iterative RaLM serving, extensive\nevaluations over three language models on four downstream QA datasets\ndemonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,\n1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,\napproximate dense retriever, and sparse retriever respectively compared with\nthe baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to\n7.59x and 2.45x when the retriever is an exact dense retriever and approximate\ndense retriever, respectively, compared with the baseline.", "field": "Computer Science", "categories": "cs.LG,cs.CL,cs.IR"}, {"arxiv_id": "2401.14024", "title": "PLCNet: Patch-wise Lane Correction Network for Automatic Lane Correction\n  in High-definition Maps", "abstract": "In High-definition (HD) maps, lane elements constitute the majority of\ncomponents and demand stringent localization requirements to ensure safe\nvehicle navigation. Vision lane detection with LiDAR position assignment is a\nprevalent method to acquire initial lanes for HD maps. However, due to\nincorrect vision detection and coarse camera-LiDAR calibration, initial lanes\nmay deviate from their true positions within an uncertain range. To mitigate\nthe need for manual lane correction, we propose a patch-wise lane correction\nnetwork (PLCNet) to automatically correct the positions of initial lane points\nin local LiDAR images that are transformed from point clouds. PLCNet first\nextracts multi-scale image features and crops patch (ROI) features centered at\neach initial lane point. By applying ROIAlign, the fix-sized ROI features are\nflattened into 1D features. Then, a 1D lane attention module is devised to\ncompute instance-level lane features with adaptive weights. Finally, lane\ncorrection offsets are inferred by a multi-layer perceptron and used to correct\nthe initial lane positions. Considering practical applications, our automatic\nmethod supports merging local corrected lanes into global corrected lanes.\nThrough extensive experiments on a self-built dataset, we demonstrate that\nPLCNet achieves fast and effective initial lane correction.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14027", "title": "The Risk of Federated Learning to Skew Fine-Tuning Features and\n  Underperform Out-of-Distribution Robustness", "abstract": "To tackle the scarcity and privacy issues associated with domain-specific\ndatasets, the integration of federated learning in conjunction with fine-tuning\nhas emerged as a practical solution. However, our findings reveal that\nfederated learning has the risk of skewing fine-tuning features and\ncompromising the out-of-distribution robustness of the model. By introducing\nthree robustness indicators and conducting experiments across diverse robust\ndatasets, we elucidate these phenomena by scrutinizing the diversity,\ntransferability, and deviation within the model feature space. To mitigate the\nnegative impact of federated learning on model robustness, we introduce GNP, a\n\\underline{G}eneral \\underline{N}oisy \\underline{P}rojection-based robust\nalgorithm, ensuring no deterioration of accuracy on the target distribution.\nSpecifically, the key strategy for enhancing model robustness entails the\ntransfer of robustness from the pre-trained model to the fine-tuned model,\ncoupled with adding a small amount of Gaussian noise to augment the\nrepresentative capacity of the model. Comprehensive experimental results\ndemonstrate that our approach markedly enhances the robustness across diverse\nscenarios, encompassing various parameter-efficient fine-tuning methods and\nconfronting different levels of data heterogeneity.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.14028", "title": "Comparison of modularity-based approaches for nodes clustering in binary\n  hypergraphs", "abstract": "We conducted a comparative analysis of the performance of modularity-based\nmethods for clustering nodes in binary hypergraphs. Statistical analysis and\nnode clustering in hypergraphs constitute an emerging topic suffering from a\nlack of standardization. In contrast to the case of graphs, the concept of\nnodes' community in hypergraphs is not unique and encompasses various distinct\nsituations. To address this, we begin by presenting, within a unified\nframework, the various hypergraph modularity criteria proposed in the\nliterature, emphasizing their differences and respective focuses. Subsequently,\nwe provide an overview of the state-of-the-art codes available to maximize\nhypergraph modularities for detecting node communities in binary hypergraphs.\nThrough exploration of various simulation settings with controlled ground truth\nclustering, we offer a comparison of these methods using different quality\nmeasures, including true clustering recovery, running time, (local)\nmaximization of the objective, and the number of clusters detected. Our\ncontribution marks the first attempt to clarify the advantages and drawbacks of\nthese newly available methods. This effort lays the foundation for a better\nunderstanding of the primary objectives of modularity-based node clustering\nmethods for binary hypergraphs.", "field": "Computer Science", "categories": "cs.SI,math.CO,physics.data-an,stat.AP"}, {"arxiv_id": "2401.14031", "title": "Sparse and Transferable Universal Singular Vectors Attack", "abstract": "The research in the field of adversarial attacks and models' vulnerability is\none of the fundamental directions in modern machine learning. Recent studies\nreveal the vulnerability phenomenon, and understanding the mechanisms behind\nthis is essential for improving neural network characteristics and\ninterpretability. In this paper, we propose a novel sparse universal white-box\nadversarial attack. Our approach is based on truncated power iteration\nproviding sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian\nmatrices. Using the ImageNet benchmark validation subset, we analyze the\nproposed method in various settings, achieving results comparable to dense\nbaselines with more than a 50% fooling rate while damaging only 5% of pixels\nand utilizing 256 samples for perturbation fitting. We also show that our\nalgorithm admits higher attack magnitude without affecting the human ability to\nsolve the task. Furthermore, we investigate that the constructed perturbations\nare highly transferable among different models without significantly decreasing\nthe fooling rate. Our findings demonstrate the vulnerability of\nstate-of-the-art models to sparse attacks and highlight the importance of\ndeveloping robust machine learning systems.", "field": "Computer Science", "categories": "cs.LG,cs.CR,cs.CV"}, {"arxiv_id": "2401.14032", "title": "GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D\n  Reconstruction Dataset Using Gaussian Splatting", "abstract": "We introduce a novel large-scale scene reconstruction benchmark using the\nnewly developed 3D representation approach, Gaussian Splatting, on our\nexpansive U-Scene dataset. U-Scene encompasses over one and a half square\nkilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground\ntruth. For data acquisition, we employed the Matrix 300 drone equipped with the\nhigh-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This\ndataset, offers a unique blend of urban and academic environments for advanced\nspatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with\nGaussian Splatting includes a detailed analysis across various novel\nviewpoints. We also juxtapose these results with those derived from our\naccurate point cloud dataset, highlighting significant differences that\nunderscore the importance of combine multi-modal information", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.14033", "title": "Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted\n  Activations", "abstract": "Recently, semidefinite programming (SDP) techniques have shown great promise\nin providing accurate Lipschitz bounds for neural networks. Specifically, the\nLipSDP approach (Fazlyab et al., 2019) has received much attention and provides\nthe least conservative Lipschitz upper bounds that can be computed with\npolynomial time guarantees. However, one main restriction of LipSDP is that its\nformulation requires the activation functions to be slope-restricted on\n$[0,1]$, preventing its further use for more general activation functions such\nas GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for\nexample as residual ReLU networks. However, a direct application of LipSDP to\nthe resultant residual ReLU networks is conservative and even fails in\nrecovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our\npaper bridges this gap and extends LipSDP beyond slope-restricted activation\nfunctions. To this end, we provide novel quadratic constraints for GroupSort,\nMaxMin, and Householder activations via leveraging their underlying properties\nsuch as sum preservation. Our proposed analysis is general and provides a\nunified approach for estimating $\\ell_2$ and $\\ell_\\infty$ Lipschitz bounds for\na rich class of neural network architectures, including non-residual and\nresidual neural networks and implicit models, with GroupSort, MaxMin, and\nHouseholder activations. Finally, we illustrate the utility of our approach\nwith a variety of experiments and show that our proposed SDPs generate less\nconservative Lipschitz bounds in comparison to existing approaches.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.14034", "title": "Unsupervised Spatial-Temporal Feature Enrichment and Fidelity\n  Preservation Network for Skeleton based Action Recognition", "abstract": "Unsupervised skeleton based action recognition has achieved remarkable\nprogress recently. Existing unsupervised learning methods suffer from severe\noverfitting problem, and thus small networks are used, significantly reducing\nthe representation capability. To address this problem, the overfitting\nmechanism behind the unsupervised learning for skeleton based action\nrecognition is first investigated. It is observed that the skeleton is already\na relatively high-level and low-dimension feature, but not in the same manifold\nas the features for action recognition. Simply applying the existing\nunsupervised learning method may tend to produce features that discriminate the\ndifferent samples instead of action classes, resulting in the overfitting\nproblem. To solve this problem, this paper presents an Unsupervised\nspatial-temporal Feature Enrichment and Fidelity Preservation framework\n(U-FEFP) to generate rich distributed features that contain all the information\nof the skeleton sequence. A spatial-temporal feature transformation subnetwork\nis developed using spatial-temporal graph convolutional network and graph\nconvolutional gate recurrent unit network as the basic feature extraction\nnetwork. The unsupervised Bootstrap Your Own Latent based learning is used to\ngenerate rich distributed features and the unsupervised pretext task based\nlearning is used to preserve the information of the skeleton sequence. The two\nunsupervised learning ways are collaborated as U-FEFP to produce robust and\ndiscriminative representations. Experimental results on three widely used\nbenchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate\nthat the proposed U-FEFP achieves the best performance compared with the\nstate-of-the-art unsupervised learning methods. t-SNE illustrations further\nvalidate that U-FEFP can learn more discriminative features for unsupervised\nskeleton based action recognition.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14036", "title": "Diverse and Lifespan Facial Age Transformation Synthesis with Identity\n  Variation Rationality Metric", "abstract": "Face aging has received continuous research attention over the past two\ndecades. Although previous works on this topic have achieved impressive\nsuccess, two longstanding problems remain unsettled: 1) generating diverse and\nplausible facial aging patterns at the target age stage; 2) measuring the\nrationality of identity variation between the original portrait and its\nsyntheses with age progression or regression. In this paper, we introduce DLAT\n+ , the first algorithm that can realize Diverse and Lifespan Age\nTransformation on human faces, where the diversity jointly manifests in the\ntransformation of facial textures and shapes. Apart from the diversity\nmechanism embedded in the model, multiple consistency restrictions are\nleveraged to keep it away from counterfactual aging syntheses. Moreover, we\npropose a new metric to assess the rationality of Identity Deviation under Age\nGaps (IDAG) between the input face and its series of age-transformed\ngenerations, which is based on statistical laws summarized from plenty of\ngenuine face-aging data. Extensive experimental results demonstrate the\nuniqueness and effectiveness of our method in synthesizing diverse and\nperceptually reasonable faces across the whole lifetime.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14038", "title": "Deep Clustering with Diffused Sampling and Hardness-aware\n  Self-distillation", "abstract": "Deep clustering has gained significant attention due to its capability in\nlearning clustering-friendly representations without labeled data. However,\nprevious deep clustering methods tend to treat all samples equally, which\nneglect the variance in the latent distribution and the varying difficulty in\nclassifying or clustering different samples. To address this, this paper\nproposes a novel end-to-end deep clustering method with diffused sampling and\nhardness-aware self-distillation (HaDis). Specifically, we first align one view\nof instances with another view via diffused sampling alignment (DSA), which\nhelps improve the intra-cluster compactness. To alleviate the sampling bias, we\npresent the hardness-aware self-distillation (HSD) mechanism to mine the\nhardest positive and negative samples and adaptively adjust their weights in a\nself-distillation fashion, which is able to deal with the potential imbalance\nin sample contributions during optimization. Further, the prototypical\ncontrastive learning is incorporated to simultaneously enhance the\ninter-cluster separability and intra-cluster compactness. Experimental results\non five challenging image datasets demonstrate the superior clustering\nperformance of our HaDis method over the state-of-the-art. Source code is\navailable at https://github.com/Regan-Zhang/HaDis.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14040", "title": "(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection", "abstract": "In the universe of Natural Language Processing, Transformer-based language\nmodels like BERT and (Chat)GPT have emerged as lexical superheroes with great\npower to solve open research problems. In this paper, we specifically focus on\nthe temporal problem of semantic change, and evaluate their ability to solve\ntwo diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and\nHistoWiC. In particular, we investigate the potential of a novel, off-the-shelf\ntechnology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a\nfamily of models that currently stand as the state-of-the-art for modeling\nsemantic change. Our experiments represent the first attempt to assess the use\nof (Chat)GPT for studying semantic change. Our results indicate that ChatGPT\nperforms significantly worse than the foundational GPT version. Furthermore,\nour results demonstrate that (Chat)GPT achieves slightly lower performance than\nBERT in detecting long-term changes but performs significantly worse in\ndetecting short-term changes.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.14043", "title": "Towards Goal-oriented Large Language Model Prompting: A Survey", "abstract": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks in which prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not as an overview of current prompt engineering\nmethods, aims to highlight the limitation of designing prompts while holding an\nanthropomorphic assumption that expects LLMs to think like humans. From our\nreview of 35 representative studies, we demonstrate that a goal-oriented prompt\nformulation, which guides LLMs to follow established human logical thinking,\nsignificantly improves the performance of LLMs. Furthermore, We introduce a\nnovel taxonomy that categorizes goal-oriented prompting methods into five\ninterconnected stages and we demonstrate the broad applicability of our\nframework by summarizing ten applicable tasks. With four future directions\nproposed, we hope to further emphasize and promote goal-oriented prompt\nengineering.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14047", "title": "Engineering a sustainable world by enhancing the scope of systems of\n  systems engineering and mastering dynamics", "abstract": "Engineering a sustainable world requires to consider various systems that\ninteract with each other. These systems include ecological systems, economical\nsystems, social systems and tech-nical systems. They are loosely coupled,\ngeographically distributed, evolve permanently and generate emergent behavior.\nAs these are characteristics of systems of systems (SoS), we discuss the\nengi-neering of a sustainable world from a SoS engineering perspective. We\nstudied SoS engineering in context of a research project, which aims at\npolitical recommendations and a research roadmap for engineering dynamic SoS.\nThe project included an exhaustive literature review, interviews and work-shops\nwith representatives from industry and academia from different application\ndomains. Based on these results and observations, we will discuss how suitable\nthe current state-of-the-art in SoS engi-neering is in order to engineer\nsustainability. Sustainability was a major driver for SoS engineering in all\ndomains, but we argue that the current scope of SoS engineering is too limited\nin order to engineer sustainability. Further, we argue that mastering dynamics\nin this larger scope is essential to engineer sustainability and that this is\naccompanied by dynamic adaptation of technological SoS.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.14051", "title": "A real-time rendering method for high albedo anisotropic materials with\n  multiple scattering", "abstract": "We propose a neural network-based real-time volume rendering method for\nrealistic and efficient rendering of volumetric media. The traditional volume\nrendering method uses path tracing to solve the radiation transfer equation,\nwhich requires a huge amount of calculation and cannot achieve real-time\nrendering. Therefore, this paper uses neural networks to simulate the iterative\nintegration process of solving the radiative transfer equation to speed up the\nvolume rendering of volume media. Specifically, the paper first performs data\nprocessing on the volume medium to generate a variety of sampling features,\nincluding density features, transmittance features and phase features. The\nhierarchical transmittance fields are fed into a 3D-CNN network to compute more\nimportant transmittance features. Secondly, the diffuse reflection sampling\ntemplate and the highlight sampling template are used to layer the three types\nof sampling features into the network. This method can pay more attention to\nlight scattering, highlights and shadows, and then select important channel\nfeatures through the attention module. Finally, the scattering distribution of\nthe center points of all sampling templates is predicted through the backbone\nneural network. This method can achieve realistic volumetric media rendering\neffects and greatly increase the rendering speed while maintaining rendering\nquality, which is of great significance for real-time rendering applications.\nExperimental results indicate that our method outperforms previous methods.", "field": "Computer Science", "categories": "cs.GR,cs.CV"}, {"arxiv_id": "2401.14055", "title": "Multi-machine preventative maintenance scheduling with imperfect\n  interventions: a restless bandit approach", "abstract": "In this paper we address the problem of allocating the efforts of a\ncollection of repairmen to a number of deteriorating machines in order to\nreduce operation costs and to mitigate the cost (and likelihood) of unexpected\nfailures. Notwithstanding these preventive maintenance interventions are aimed\nat returning the machine to a so-called as-good-as-new state, unforeseeable\nfactors may imply that maintenance interventions are not perfect and the\nmachine is only returned to an earlier (uncertain) state of wear. The problem\nis modelled as a restless bandit problem and an index policy for the sequential\nallocation of maintenance tasks is proposed. A series of numerical experiments\nshows the strong performance of the proposed policy. Moreover, the methodology\nis of interest in the general context of dynamic resource allocation and\nrestless bandit problems, as well as being useful in the particular imperfect\nmaintenance model described.", "field": "Computer Science", "categories": "cs.DM,cs.NA,math.NA"}, {"arxiv_id": "2401.14056", "title": "Model CBOR Serialization for Federated Learning", "abstract": "The typical federated learning workflow requires communication between a\ncentral server and a large set of clients synchronizing model parameters\nbetween each other. The current frameworks use communication protocols not\nsuitable for resource-constrained devices and are either hard to deploy or\nrequire high-throughput links not available on these devices. In this paper, we\npresent a generic message framework using CBOR for communication with existing\nfederated learning frameworks optimised for use with resource-constrained\ndevices and low power and lossy network links. We evaluate the resulting\nmessage sizes against JSON serialized messages where compare both with model\nparameters resulting in optimal and worst case serialization length, and with a\nreal-world LeNet-5 model. Our benchmarks show that with our approach, messages\nare up to 75 % smaller in size when compared to the JSON alternative.", "field": "Computer Science", "categories": "cs.NI"}, {"arxiv_id": "2401.14057", "title": "Left/Right Brain, human motor control and the implications for robotics", "abstract": "Neural Network movement controllers promise a variety of advantages over\nconventional control methods however they are not widely adopted due to their\ninability to produce reliably precise movements. This research explores a\nbilateral neural network architecture as a control system for motor tasks. We\naimed to achieve hemispheric specialisation similar to what is observed in\nhumans across different tasks; the dominant system (usually the right hand,\nleft hemisphere) excels at tasks involving coordination and efficiency of\nmovement, and the non-dominant system performs better at tasks requiring\npositional stability. Specialisation was achieved by training the hemispheres\nwith different loss functions tailored toward the expected behaviour of the\nrespective hemispheres. We compared bilateral models with and without\nspecialised hemispheres, with and without inter-hemispheric connectivity\n(representing the biological Corpus Callosum), and unilateral models with and\nwithout specialisation. The models were trained and tested on two tasks common\nin the human motor control literature: the random reach task, suited to the\ndominant system, a model with better coordination, and the hold position task,\nsuited to the non-dominant system, a model with more stable movement. Each\nsystem out-performed the non-favoured system in its preferred task. For both\ntasks, a bilateral model outperforms the 'non-preferred' hand, and is as good\nor better than the 'preferred' hand. The Corpus Callosum tends to improve\nperformance, but not always for the specialised models.", "field": "Computer Science", "categories": "cs.RO,cs.AI,cs.LG,cs.NE,q-bio.NC,I.2.6; I.2.9"}, {"arxiv_id": "2401.14060", "title": "On Sparse Covers of Minor Free Graphs, Low Dimensional Metric\n  Embeddings, and other applications", "abstract": "Given a metric space $(X,d_X)$, a $(\\beta,s,\\Delta)$-sparse cover is a\ncollection of clusters $\\mathcal{C}\\subseteq P(X)$ with diameter at most\n$\\Delta$, such that for every point $x\\in X$, the ball\n$B_X(x,\\frac\\Delta\\beta)$ is fully contained in some cluster $C\\in\n\\mathcal{C}$, and $x$ belongs to at most $s$ clusters in $\\mathcal{C}$. Our\nmain contribution is to show that the shortest path metric of every $K_r$-minor\nfree graphs admits $(O(r),O(r^2),\\Delta)$-sparse cover, and for every\n$\\epsilon>0$, $(4+\\epsilon,O(\\frac1\\epsilon)^r,\\Delta)$-sparse cover (for\narbitrary $\\Delta>0$). We then use this sparse cover to show that every\n$K_r$-minor free graph embeds into\n$\\ell_\\infty^{\\tilde{O}(\\frac1\\epsilon)^{r+1}\\cdot\\log n}$ with distortion\n$3+\\eps$ (resp. into $\\ell_\\infty^{\\tilde{O}(r^2)\\cdot\\log n}$ with distortion\n$O(r)$). Further, we provide applications of these sparse covers into padded\ndecompositions, sparse partitions, universal TSP / Steiner tree, oblivious buy\nat bulk, name independent routing, and path reporting distance oracles.", "field": "Computer Science", "categories": "cs.DS,cs.CG,math.CO"}, {"arxiv_id": "2401.14065", "title": "Novel application of Relief Algorithm in cascaded artificial neural\n  network to predict wind speed for wind power resource assessment in India", "abstract": "Wind power generated by wind has non-schedule nature due to stochastic nature\nof meteorological variable. Hence energy business and control of wind power\ngeneration requires prediction of wind speed (WS) from few seconds to different\ntime steps in advance. To deal with prediction shortcomings, various WS\nprediction methods have been used. Predictive data mining offers variety of\nmethods for WS predictions where artificial neural network (ANN) is one of the\nreliable and accurate methods. It is observed from the result of this study\nthat ANN gives better accuracy in comparison conventional model. The accuracy\nof WS prediction models is found to be dependent on input parameters and\narchitecture type algorithms utilized. So the selection of most relevant input\nparameters is important research area in WS predicton field. The objective of\nthe paper is twofold: first extensive review of ANN for wind power and WS\nprediction is carried out. Discussion and analysis of feature selection using\nRelief Algorithm (RA) in WS prediction are considered for different Indian\nsites. RA identify atmospheric pressure, solar radiation and relative humidity\nare relevant input variables. Based on relevant input variables Cascade ANN\nmodel is developed and prediction accuracy is evaluated. It is found that root\nmean square error (RMSE) for comparison between predicted and measured WS for\ntraining and testing wind speed are found to be 1.44 m/s and 1.49 m/s\nrespectively. The developed cascade ANN model can be used to predict wind speed\nfor sites where there are not WS measuring instruments are installed in India.", "field": "Computer Science", "categories": "cs.LG,cs.SY,eess.SY"}, {"arxiv_id": "2401.14066", "title": "CreativeSynth: Creative Blending and Synthesis of Visual Arts based on\n  Multimodal Diffusion", "abstract": "Large-scale text-to-image generative models have made impressive strides,\nshowcasing their ability to synthesize a vast array of high-quality images.\nHowever, adapting these models for artistic image editing presents two\nsignificant challenges. Firstly, users struggle to craft textual prompts that\nmeticulously detail visual elements of the input image. Secondly, prevalent\nmodels, when effecting modifications in specific zones, frequently disrupt the\noverall artistic style, complicating the attainment of cohesive and\naesthetically unified artworks. To surmount these obstacles, we build the\ninnovative unified framework CreativeSynth, which is based on a diffusion model\nwith the ability to coordinate multimodal inputs and multitask in the field of\nartistic image generation. By integrating multimodal features with customized\nattention mechanisms, CreativeSynth facilitates the importation of real-world\nsemantic content into the domain of art through inversion and real-time style\ntransfer. This allows for the precise manipulation of image style and content\nwhile maintaining the integrity of the original model parameters. Rigorous\nqualitative and quantitative evaluations underscore that CreativeSynth excels\nin enhancing artistic images' fidelity and preserves their innate aesthetic\nessence. By bridging the gap between generative models and artistic finesse,\nCreativeSynth becomes a custom digital palette.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.14067", "title": "Ta'keed: The First Generative Fact-Checking System for Arabic Claims", "abstract": "This paper introduces Ta'keed, an explainable Arabic automatic fact-checking\nsystem. While existing research often focuses on classifying claims as \"True\"\nor \"False,\" there is a limited exploration of generating explanations for claim\ncredibility, particularly in Arabic. Ta'keed addresses this gap by assessing\nclaim truthfulness based on retrieved snippets, utilizing two main components:\ninformation retrieval and LLM-based claim verification. We compiled the\nArFactEx, a testing gold-labelled dataset with manually justified references,\nto evaluate the system. The initial model achieved a promising F1 score of 0.72\nin the classification task. Meanwhile, the system's generated explanations are\ncompared with gold-standard explanations syntactically and semantically. The\nstudy recommends evaluating using semantic similarities, resulting in an\naverage cosine similarity score of 0.76. Additionally, we explored the impact\nof varying snippet quantities on claim classification accuracy, revealing a\npotential correlation, with the model using the top seven hits outperforming\nothers with an F1 score of 0.77.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14069", "title": "Neural Sinkhorn Gradient Flow", "abstract": "Wasserstein Gradient Flows (WGF) with respect to specific functionals have\nbeen widely used in the machine learning literature. Recently, neural networks\nhave been adopted to approximate certain intractable parts of the underlying\nWasserstein gradient flow and result in efficient inference procedures. In this\npaper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which\nparametrizes the time-varying velocity field of the Wasserstein gradient flow\nw.r.t. the Sinkhorn divergence to the target distribution starting a given\nsource distribution. We utilize the velocity field matching training scheme in\nNSGF, which only requires samples from the source and target distribution to\ncompute an empirical velocity field approximation. Our theoretical analyses\nshow that as the sample size increases to infinity, the mean-field limit of the\nempirical approximation converges to the true underlying velocity field. To\nfurther enhance model efficiency on high-dimensional tasks, a two-phase NSGF++\nmodel is devised, which first follows the Sinkhorn flow to approach the image\nmanifold quickly ($\\le 5$ NFEs) and then refines the samples along a simple\nstraight flow. Numerical experiments with synthetic and real-world benchmark\ndatasets support our theoretical results and demonstrate the effectiveness of\nthe proposed methods.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.14074", "title": "ProCNS: Progressive Prototype Calibration and Noise Suppression for\n  Weakly-Supervised Medical Image Segmentation", "abstract": "Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate\nthe conflict between annotation cost and model performance by adopting sparse\nannotation formats (e.g., point, scribble, block, etc.). Typical approaches\nattempt to exploit anatomy and topology priors to directly expand sparse\nannotations into pseudo-labels. However, due to a lack of attention to the\nambiguous edges in medical images and insufficient exploration of sparse\nsupervision, existing approaches tend to generate erroneous and overconfident\npseudo proposals in noisy regions, leading to cumulative model error and\nperformance degradation. In this work, we propose a novel WSS approach, named\nProCNS, encompassing two synergistic modules devised with the principles of\nprogressive prototype calibration and noise suppression. Specifically, we\ndesign a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the\npair-wise affinities between spatial and semantic elements, providing our model\nof interest with more reliable guidance. The affinities are derived from the\ninput images and the prototype-refined predictions. Meanwhile, we propose an\nAdaptive Noise Perception and Masking (ANPM) module to obtain more enriched and\nrepresentative prototype representations, which adaptively identifies and masks\nnoisy regions within the pseudo proposals, reducing potential erroneous\ninterference during prototype computation. Furthermore, we generate specialized\nsoft pseudo-labels for the noisy regions identified by ANPM, providing\nsupplementary supervision. Extensive experiments on three medical image\nsegmentation tasks involving different modalities demonstrate that the proposed\nframework significantly outperforms representative state-of-the-art methods", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.14076", "title": "Quantum Resistant Ciphertext-Policy Attribute-Based Encryption Scheme\n  with Flexible Access Structure", "abstract": "In this paper, we present a novel ciphertext-policy attribute based\nencryption (CP-ABE) scheme that offers a flexible access structure. Our\nproposed scheme incorporates an access tree as its access control policy,\nenabling fine-grained access control over encrypted data. The security of our\nscheme is provable under the hardness assumption of the decisional\nRing-Learning with Errors (R-LWE) problem, ensuring robust protection against\nunauthorized access. CP-ABE is a cryptographic technique that allows data\nowners to encrypt their data with access policies defined in terms of\nattributes. Only users possessing the required attributes can decrypt and\naccess the encrypted data. Our scheme extends the capabilities of CP-ABE by\nintroducing a flexible access structure based on an access tree. This structure\nenables more complex and customizable access policies, accommodating a wider\nrange of real-world scenarios. To ensure the security of our scheme, we rely on\nthe decisional R-LWE problem, a well-established hardness assumption in\ncryptography. By proving the security of our scheme under this assumption, we\nprovide a strong guarantee of protection against potential attacks.\nFurthermore, our proposed scheme operates in the standard model, which means it\ndoes not rely on any additional assumptions or idealized cryptographic\nprimitives. This enhances the practicality and applicability of our scheme,\nmaking it suitable for real-world deployment. We evaluate the performance and\nefficiency of our scheme through extensive simulations and comparisons with\nexisting CP-ABE schemes. The results demonstrate the effectiveness and\nscalability of our proposed approach, highlighting its potential for secure and\nflexible data access control in various domains.", "field": "Computer Science", "categories": "cs.CR"}, {"arxiv_id": "2401.14077", "title": "LongMemory.jl: Generating, Estimating, and Forecasting Long Memory\n  Models in Julia", "abstract": "LongMemory.jl is a package for time series long memory modelling in Julia.\nThe package provides functions to generate long memory, estimate model\nparameters, and forecast. Generating methods include fractional differencing,\nstochastic error duration, and cross-sectional aggregation. Estimators include\nthe classic ones used to estimate the Hurst effect, those inspired by\nlog-periodogram regression, and parametric ones. Forecasting is provided for\nall parametric estimators. Moreover, the package adds plotting capabilities to\nillustrate long memory dynamics and forecasting. This article presents the\ntheoretical developments for long memory modelling, show examples using the\ndata included with the package, and compares the properties of LongMemory.jl\nwith current alternatives, including benchmarks. For some of the theoretical\ndevelopments, LongMemory.jl provides the first publicly available\nimplementation in any programming language. A notable feature of this package\nis that all functions are implemented in the same programming language, taking\nadvantage of the ease of use and speed provided by Julia. Therefore, all code\nis accessible to the user. Multiple dispatch, a novel feature of the language,\nis used to speed computations and provide consistent calls to related methods.\nThe package is related to the R packages LongMemoryTS and fracdiff.", "field": "Computer Science", "categories": "cs.MS,stat.CO"}, {"arxiv_id": "2401.14078", "title": "The Adaptive Architectural Layout: How the Control of a Semi-Autonomous\n  Mobile Robotic Partition was Shared to Mediate the Environmental Demands and\n  Resources of an Open-Plan Office", "abstract": "A typical open-plan office layout is unable to optimally host multiple\ncollocated work activities, personal needs, and situational events, as its\nspace exerts a range of environmental demands on workers in terms of\nmaintaining their acoustic, visual or privacy comfort. As we hypothesise that\nthese demands could be coped by optimising the environmental resources of the\narchitectural layout, we deployed a mobile robotic partition that autonomously\nmanoeuvres between predetermined locations. During a five-weeks in-the-wild\nstudy within a real-world open-plan office, we studied how 13 workers adopted\nfour distinct adaptation strategies when sharing the spatiotemporal control of\nthe robotic partition. Based on their logged and self-reported reasoning, we\npresent six initiation regulating factors that determine the appropriateness of\neach adaptation strategy. This study thus contributes to how future\nhuman-building interaction could autonomously improve the experience, comfort,\nperformance, and even the health and wellbeing of multiple workers that share\nthe same workplace.", "field": "Computer Science", "categories": "cs.HC,cs.RO"}, {"arxiv_id": "2401.14079", "title": "From Requirements to Architecture: An AI-Based Journey to\n  Semi-Automatically Generate Software Architectures", "abstract": "Designing domain models and software architectures represents a significant\nchallenge in software development, as the resulting architectures play a vital\nrole in fulfilling the system's quality of service. Due to time pressure,\narchitects often model only one architecture based on their known limited\ndomain understanding, patterns, and experience instead of thoroughly analyzing\nthe domain and evaluating multiple candidates, selecting the best fitting.\nExisting approaches try to generate domain models based on requirements, but\nstill require time-consuming manual effort to achieve good results. Therefore,\nin this vision paper, we propose a method to generate software architecture\ncandidates semi-automatically based on requirements using artificial\nintelligence techniques. We further envision an automatic evaluation and\ntrade-off analysis of the generated architecture candidates using, e.g., the\narchitecture trade-off analysis method combined with large language models and\nquantitative analyses. To evaluate this approach, we aim to analyze the quality\nof the generated architecture models and the efficiency and effectiveness of\nour proposed process by conducting qualitative studies.", "field": "Computer Science", "categories": "cs.SE,cs.AI,D.2.2"}, {"arxiv_id": "2401.14081", "title": "Accelerating Fractional PINNs using Operational Matrices of Derivative", "abstract": "This paper presents a novel operational matrix method to accelerate the\ntraining of fractional Physics-Informed Neural Networks (fPINNs). Our approach\ninvolves a non-uniform discretization of the fractional Caputo operator,\nfacilitating swift computation of fractional derivatives within Caputo-type\nfractional differential problems with $0<\\alpha<1$. In this methodology, the\noperational matrix is precomputed, and during the training phase, automatic\ndifferentiation is replaced with a matrix-vector product. While our methodology\nis compatible with any network, we particularly highlight its successful\nimplementation in PINNs, emphasizing the enhanced accuracy achieved when\nutilizing the Legendre Neural Block (LNB) architecture. LNB incorporates\nLegendre polynomials into the PINN structure, providing a significant boost in\naccuracy. The effectiveness of our proposed method is validated across diverse\ndifferential equations, including Delay Differential Equations (DDEs) and\nSystems of Differential Algebraic Equations (DAEs). To demonstrate its\nversatility, we extend the application of the method to systems of differential\nequations, specifically addressing nonlinear Pantograph fractional-order\nDDEs/DAEs. The results are supported by a comprehensive analysis of numerical\noutcomes.", "field": "Computer Science", "categories": "cs.LG,cs.NA,math.NA"}, {"arxiv_id": "2401.14085", "title": "Enhanced Multi-Target Tracking in Dynamic Environments: Distributed\n  Control Methods Within the Random Finite Set Framework", "abstract": "Tracking multiple targets in dynamic environments using distributed sensor\nnetworks is a challenging problem that has received significant attention in\nrecent years. In such scenarios, the network of sensors must coordinate their\nactions to estimate the locations and trajectories of multiple targets\naccurately. Multi-sensor control methods can improve the performance of these\nnetworks by enabling efficient utilization of resources and enhancing the\naccuracy of the estimated target states. This paper proposes two novel\nmulti-sensor control methods that utilize the Random Finite Set (RFS) framework\nto address this problem. Our methods improve computational tractability and\nenable fully distributed control, making them suitable for real-time\napplications.", "field": "Computer Science", "categories": "eess.SY,cs.SY,eess.SP"}, {"arxiv_id": "2401.14086", "title": "Generating Likely Counterfactuals Using Sum-Product Networks", "abstract": "Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI\nsystems need to be explained. These decisions are often explainable only post\nhoc, where counterfactual explanations are popular. The question of what\nconstitutes the best counterfactual explanation must consider multiple aspects,\nwhere \"distance from the sample\" is the most common. We argue that this\nrequirement frequently leads to explanations that are unlikely and, therefore,\nof limited value. Here, we present a system that provides high-likelihood\nexplanations. We show that the search for the most likely explanations\nsatisfying many common desiderata for counterfactual explanations can be\nmodeled using mixed-integer optimization (MIO). In the process, we propose an\nMIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the\nlikelihood of a counterfactual, which can be of independent interest. A\nnumerical comparison against several methods for generating counterfactual\nexplanations is provided.", "field": "Computer Science", "categories": "cs.AI,cs.LG,math.OC"}, {"arxiv_id": "2401.14088", "title": "Double Trouble? Impact and Detection of Duplicates in Face Image\n  Datasets", "abstract": "Various face image datasets intended for facial biometrics research were\ncreated via web-scraping, i.e. the collection of images publicly available on\nthe internet. This work presents an approach to detect both exactly and nearly\nidentical face image duplicates, using file and image hashes. The approach is\nextended through the use of face image preprocessing. Additional steps based on\nface recognition and face image quality assessment models reduce false\npositives, and facilitate the deduplication of the face images both for intra-\nand inter-subject duplicate sets. The presented approach is applied to five\ndatasets, namely LFW, TinyFace, Adience, CASIA-WebFace, and C-MS-Celeb (a\ncleaned MS-Celeb-1M variant). Duplicates are detected within every dataset,\nwith hundreds to hundreds of thousands of duplicates for all except LFW. Face\nrecognition and quality assessment experiments indicate a minor impact on the\nresults through the duplicate removal. The final deduplication data is publicly\navailable.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14090", "title": "A Modular Approach to Automatic Cyber Threat Attribution using Opinion\n  Pools", "abstract": "Cyber threat attribution can play an important role in increasing resilience\nagainst digital threats. Recent research focuses on automating the threat\nattribution process and on integrating it with other efforts, such as threat\nhunting. To support increasing automation of the cyber threat attribution\nprocess, this paper proposes a modular architecture as an alternative to\ncurrent monolithic automated approaches. The modular architecture can utilize\nopinion pools to combine the output of concrete attributors. The proposed\nsolution increases the tractability of the threat attribution problem and\noffers increased usability and interpretability, as opposed to monolithic\nalternatives. In addition, a Pairing Aggregator is proposed as an aggregation\nmethod that forms pairs of attributors based on distinct features to produce\nintermediary results before finally producing a single Probability Mass\nFunction (PMF) as output. The Pairing Aggregator sequentially applies both the\nlogarithmic opinion pool and the linear opinion pool. An experimental\nvalidation suggests that the modular approach does not result in decreased\nperformance and can even enhance precision and recall compared to monolithic\nalternatives. The results also suggest that the Pairing Aggregator can improve\nprecision over the linear and logarithmic opinion pools. Furthermore, the\nimproved k-accuracy in the experiment suggests that forensic experts can\nleverage the resulting PMF during their manual attribution processes to enhance\ntheir efficiency.", "field": "Computer Science", "categories": "cs.CR,cs.LG,cs.SE"}, {"arxiv_id": "2401.14093", "title": "McUDI: Model-Centric Unsupervised Degradation Indicator for Failure\n  Prediction AIOps Solutions", "abstract": "Due to the continuous change in operational data, AIOps solutions suffer from\nperformance degradation over time. Although periodic retraining is the\nstate-of-the-art technique to preserve the failure prediction AIOps models'\nperformance over time, this technique requires a considerable amount of labeled\ndata to retrain. In AIOps obtaining label data is expensive since it requires\nthe availability of domain experts to intensively annotate it. In this paper,\nwe present McUDI, a model-centric unsupervised degradation indicator that is\ncapable of detecting the exact moment the AIOps model requires retraining as a\nresult of changes in data. We further show how employing McUDI in the\nmaintenance pipeline of AIOps solutions can reduce the number of samples that\nrequire annotations with 30k for job failure prediction and 260k for disk\nfailure prediction while achieving similar performance with periodic\nretraining.", "field": "Computer Science", "categories": "cs.SE,cs.LG"}, {"arxiv_id": "2401.14095", "title": "Evaluating User Experience and Data Quality in a Gamified Data\n  Collection for Appearance-Based Gaze Estimation", "abstract": "Appearance-based gaze estimation, which uses only a regular camera to\nestimate human gaze, is important in various application fields. While the\ntechnique faces data bias issues, data collection protocol is often demanding,\nand collecting data from a wide range of participants is difficult. It is an\nimportant challenge to design opportunities that allow a diverse range of\npeople to participate while ensuring the quality of the training data. To\ntackle this challenge, we introduce a novel gamified approach for collecting\ntraining data. In this game, two players communicate words via eye gaze through\na transparent letter board. Images captured during gameplay serve as valuable\ntraining data for gaze estimation models. The game is designed as a physical\ninstallation that involves communication between players, and it is expected to\nattract the interest of diverse participants. We assess the game's significance\non data quality and user experience through a comparative user study.", "field": "Computer Science", "categories": "cs.HC"}, {"arxiv_id": "2401.14098", "title": "Carry Your Fault: A Fault Propagation Attack on Side-Channel Protected\n  LWE-based KEM", "abstract": "Post-quantum cryptographic (PQC) algorithms, especially those based on the\nlearning with errors (LWE) problem, have been subjected to several physical\nattacks in the recent past. Although the attacks broadly belong to two classes\n- passive side-channel attacks and active fault attacks, the attack strategies\nvary significantly due to the inherent complexities of such algorithms.\nExploring further attack surfaces is, therefore, an important step for\neventually securing the deployment of these algorithms. Also, it is important\nto test the robustness of the already proposed countermeasures in this regard.\nIn this work, we propose a new fault attack on side-channel secure masked\nimplementation of LWE-based key-encapsulation mechanisms (KEMs) exploiting\nfault propagation. The attack typically originates due to an algorithmic\nmodification widely used to enable masking, namely the Arithmetic-to-Boolean\n(A2B) conversion. We exploit the data dependency of the adder carry chain in\nA2B and extract sensitive information, albeit masking (of arbitrary order)\nbeing present. As a practical demonstration of the exploitability of this\ninformation leakage, we show key recovery attacks of Kyber, although the\nleakage also exists for other schemes like Saber. The attack on Kyber targets\nthe decapsulation module and utilizes Belief Propagation (BP) for key recovery.\nTo the best of our knowledge, it is the first attack exploiting an algorithmic\ncomponent introduced to ease masking rather than only exploiting the randomness\nintroduced by masking to obtain desired faults (as done by Delvaux). Finally,\nwe performed both simulated and electromagnetic (EM) fault-based practical\nvalidation of the attack for an open-source first-order secure Kyber\nimplementation running on an STM32 platform.", "field": "Computer Science", "categories": "cs.CR,E.3.3"}, {"arxiv_id": "2401.14100", "title": "Randomized Complexity of Mean Computation and the Adaption Problem", "abstract": "Recently the adaption problem of Information-Based Complexity (IBC) for\nlinear problems in the randomized setting was solved in Heinrich (J. Complexity\n82, 2024, 101821). Several papers treating further aspects of this problem\nfollowed. However, all examples obtained so far were vector-valued. In this\npaper we settle the scalar-valued case. We study the complexity of mean\ncomputation in finite dimensional sequence spaces with mixed $L_p^N$ norms. We\ndetermine the $n$-th minimal errors in the randomized adaptive and non-adaptive\nsetting. It turns out that among the problems considered there are examples\nwhere adaptive and non-adaptive $n$-th minimal errors deviate by a power of\n$n$. The gap can be (up to log factors) of the order $n^{1/4}$. We also show\nhow to turn such results into infinite dimensional examples with suitable\ndeviation for all $n$ simultaneously.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.14106", "title": "Epimorphisms and Acyclic Types in Univalent Mathematics", "abstract": "We characterize the epimorphisms in homotopy type theory (HoTT) as the\nfiberwise acyclic maps and develop a type-theoretic treatment of acyclic maps\nand types in the context of synthetic homotopy theory. We present examples and\napplications in group theory, such as the acyclicity of the Higman group,\nthrough the identification of groups with $0$-connected, pointed $1$-types.\nMany of our results are formalized as part of the agda-unimath library.", "field": "Computer Science", "categories": "cs.LO,math.AT,math.CT"}, {"arxiv_id": "2401.14107", "title": "Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement", "abstract": "Wearable technologies enable continuous monitoring of various health metrics,\nsuch as physical activity, heart rate, sleep, and stress levels. A key\nchallenge with wearable data is obtaining quality labels. Unlike modalities\nlike video where the videos themselves can be effectively used to label objects\nor events, wearable data do not contain obvious cues about the physical\nmanifestation of the users and usually require rich metadata. As a result,\nlabel noise can become an increasingly thorny issue when labeling such data. In\nthis paper, we propose a novel solution to address noisy label learning,\nentitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially\nlearns a seed model using weak labels. Next, it fine-tunes the seed model using\na handful of expert corrections. Finally, it achieves better generalizability\nand robustness by merging the seed and fine-tuned models via weighted parameter\naveraging. We evaluate our approach on four challenging tasks and datasets, and\ncompare it against eight competitive baselines designed to deal with noisy\nlabels. We show that FHLR achieves significantly better performance when\nlearning from noisy labels and achieves state-of-the-art by a large margin,\nwith up to 19% accuracy improvement under symmetric and asymmetric noise.\nNotably, we find that FHLR is particularly robust to increased label noise,\nunlike prior works that suffer from severe performance degradation. Our work\nnot only achieves better generalization in high-stakes health sensing\nbenchmarks but also sheds light on how noise affects commonly-used models.", "field": "Computer Science", "categories": "cs.LG,eess.SP"}, {"arxiv_id": "2401.14109", "title": "CompactifAI: Extreme Compression of Large Language Models using\n  Quantum-Inspired Tensor Networks", "abstract": "Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly\nin generative Artificial Intelligence (AI), but their immense size poses\nsignificant challenges, such as huge training and inference costs, substantial\nenergy demands, and limitations for on-site deployment. Traditional compression\nmethods such as pruning, distillation, and low-rank approximation focus on\nreducing the effective number of neurons in the network, while quantization\nfocuses on reducing the numerical precision of individual weights to reduce the\nmodel size while keeping the number of neurons fixed. While these compression\nmethods have been relatively successful in practice, there's no compelling\nreason to believe that truncating the number of neurons is an optimal strategy.\nIn this context, this paper introduces CompactifAI, an innovative LLM\ncompression approach using quantum-inspired Tensor Networks that focuses on the\nmodel's correlation space instead, allowing for a more controlled, refined and\ninterpretable model compression. Our method is versatile and can be implemented\nwith - or on top of - other compression techniques. As a benchmark, we\ndemonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model\nto only $30\\%$ of its original size while recovering over $90\\%$ of the\noriginal accuracy after a brief distributed retraining.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG,quant-ph"}, {"arxiv_id": "2401.14110", "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width\n  Accumulators", "abstract": "The majority of the research on the quantization of Deep Neural Networks\n(DNNs) is focused on reducing the precision of tensors visible by high-level\nframeworks (e.g., weights, activations, and gradients). However, current\nhardware still relies on high-accuracy core operations. Most significant is the\noperation of accumulating products. This high-precision accumulation operation\nis gradually becoming the main computational bottleneck. This is because, so\nfar, the usage of low-precision accumulators led to a significant degradation\nin performance. In this work, we present a simple method to train and fine-tune\nhigh-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\naccumulators, with no significant degradation in accuracy. Lastly, we show that\nas we decrease the accumulation precision further, using fine-grained gradient\napproximations can improve the DNN accuracy.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.AR"}, {"arxiv_id": "2401.14111", "title": "Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph\n  Conditioning in Diffusion Models", "abstract": "Advancements in generative models have sparked significant interest in\ngenerating images while adhering to specific structural guidelines. Scene graph\nto image generation is one such task of generating images which are consistent\nwith the given scene graph. However, the complexity of visual scenes poses a\nchallenge in accurately aligning objects based on specified relations within\nthe scene graph. Existing methods approach this task by first predicting a\nscene layout and generating images from these layouts using adversarial\ntraining. In this work, we introduce a novel approach to generate images from\nscene graphs which eliminates the need of predicting intermediate layouts. We\nleverage pre-trained text-to-image diffusion models and CLIP guidance to\ntranslate graph knowledge into images. Towards this, we first pre-train our\ngraph encoder to align graph features with CLIP features of corresponding\nimages using a GAN based training. Further, we fuse the graph features with\nCLIP embedding of object labels present in the given scene graph to create a\ngraph consistent CLIP guided conditioning signal. In the conditioning input,\nobject embeddings provide coarse structure of the image and graph features\nprovide structural alignment based on relationships among objects. Finally, we\nfine tune a pre-trained diffusion model with the graph consistent conditioning\nsignal with reconstruction and CLIP alignment loss. Elaborate experiments\nreveal that our method outperforms existing methods on standard benchmarks of\nCOCO-stuff and Visual Genome dataset.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14112", "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric\n  Algorithm-System Co-Design", "abstract": "Six-bit quantization (FP6) can effectively reduce the size of large language\nmodels (LLMs) and preserve the model quality consistently across varied\napplications. However, existing systems do not provide Tensor Core support for\nFP6 quantization and struggle to achieve practical performance improvements\nduring LLM inference. It is challenging to support FP6 quantization on GPUs due\nto (1) unfriendly memory access of model weights with irregular bit-width and\n(2) high runtime overhead of weight de-quantization. To address these problems,\nwe propose TC-FPx, the first full-stack GPU kernel design scheme with unified\nTensor Core support of float-point weights for various quantization bit-width.\nWe integrate TC-FPx kernel into an existing inference system, providing new\nend-to-end support (called FP6-LLM) for quantized LLM inference, where better\ntrade-offs between inference cost and model quality are achieved. Experiments\nshow that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,\nachieving 1.69x-2.65x higher normalized inference throughput than the FP16\nbaseline. The source code will be publicly available soon.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.AR"}, {"arxiv_id": "2401.14113", "title": "On the Affinity, Rationality, and Diversity of Hierarchical Topic\n  Modeling", "abstract": "Hierarchical topic modeling aims to discover latent topics from a corpus and\norganize them into a hierarchy to understand documents with desirable semantic\ngranularity. However, existing work struggles with producing topic hierarchies\nof low affinity, rationality, and diversity, which hampers document\nunderstanding. To overcome these challenges, we in this paper propose Transport\nPlan and Context-aware Hierarchical Topic Model (TraCo). Instead of early\nsimple topic dependencies, we propose a transport plan dependency method. It\nconstrains dependencies to ensure their sparsity and balance, and also\nregularizes topic hierarchy building with them. This improves affinity and\ndiversity of hierarchies. We further propose a context-aware disentangled\ndecoder. Rather than previously entangled decoding, it distributes different\nsemantic granularity to topics at different levels by disentangled decoding.\nThis facilitates the rationality of hierarchies. Experiments on benchmark\ndatasets demonstrate that our method surpasses state-of-the-art baselines,\neffectively improving the affinity, rationality, and diversity of hierarchical\ntopic modeling with better performance on downstream tasks.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.14115", "title": "MIFI: MultI-camera Feature Integration for Roust 3D Distracted Driver\n  Activity Recognition", "abstract": "Distracted driver activity recognition plays a critical role in risk\naversion-particularly beneficial in intelligent transportation systems.\nHowever, most existing methods make use of only the video from a single view\nand the difficulty-inconsistent issue is neglected. Different from them, in\nthis work, we propose a novel MultI-camera Feature Integration (MIFI) approach\nfor 3D distracted driver activity recognition by jointly modeling the data from\ndifferent camera views and explicitly re-weighting examples based on their\ndegree of difficulty. Our contributions are two-fold: (1) We propose a simple\nbut effective multi-camera feature integration framework and provide three\ntypes of feature fusion techniques. (2) To address the difficulty-inconsistent\nproblem in distracted driver activity recognition, a periodic learning method,\nnamed example re-weighting that can jointly learn the easy and hard samples, is\npresented. The experimental results on the 3MDAD dataset demonstrate that the\nproposed MIFI can consistently boost performance compared to single-view\nmodels.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14117", "title": "Evaluation of POSIT Arithmetic with Accelerators", "abstract": "We present an evaluation of 32-bit POSIT arithmetic through its\nimplementation as accelerators on FPGAs and GPUs. POSIT, a floating-point\nnumber format, adaptively changes the size of its fractional part. We developed\nhardware designs for FPGAs and software for GPUs to accelerate linear algebra\noperations using Posit(32,2) arithmetic. Our FPGA- and GPU-based accelerators\nin Posit(32,2) arithmetic significantly accelerated the Cholesky and LU\ndecomposition algorithms for dense matrices. In terms of numerical accuracy,\nPosit(32,2) arithmetic is approximately 0.5 - 1.0 digits more accurate than the\nstandard 32-bit format, especially when the norm of the elements of the input\nmatrix is close to 1. Evaluating power consumption, we observed that the power\nefficiency of the accelerators ranged between 0.043 - 0.076 Gflops/watts for\nthe LU decomposition in Posit(32,2) arithmetic. The power efficiency of the\nlatest GPUs as accelerators of Posit(32,2) arithmetic is better than that of\nthe evaluated FPGA chip.", "field": "Computer Science", "categories": "cs.DC,cs.AR,cs.MS"}, {"arxiv_id": "2401.14121", "title": "Incorporating Exemplar Optimization into Training with Dual Networks for\n  Human Mesh Recovery", "abstract": "We propose a novel optimization-based human mesh recovery method from a\nsingle image. Given a test exemplar, previous approaches optimize the\npre-trained regression network to minimize the 2D re-projection loss, which\nhowever suffer from over-/under-fitting problems. This is because the\n``exemplar optimization'' at testing time has too weak relation to the\npre-training process, and the exemplar optimization loss function is different\nfrom the training loss function. (1) We incorporate exemplar optimization into\nthe training stage. During training, our method first executes exemplar\noptimization and subsequently proceeds with training-time optimization. The\nexemplar optimization may run into a wrong direction, while the subsequent\ntraining optimization serves to correct the deviation. Involved in training,\nthe exemplar optimization learns to adapt its behavior to training data,\nthereby acquires generalibility to test exemplars. (2) We devise a dual-network\narchitecture to convey the novel training paradigm, which is composed of a main\nregression network and an auxiliary network, in which we can formulate the\nexemplar optimization loss function in the same form as the training loss\nfunction. This further enhances the compatibility between the exemplar and\ntraining optimizations. Experiments demonstrate that our exemplar optimization\nafter the novel training scheme significantly outperforms state-of-the-art\napproaches.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14126", "title": "An Indexed Linear Logic for Idempotent Intersection Types (Long version)", "abstract": "Indexed Linear Logic has been introduced by Ehrhard and Bucciarelli, it can\nbe seen as a logical presentation of non-idempotent intersection types extended\nthrough the relational semantics to the full linear logic. We introduce an\nidempotent variant of Indexed Linear Logic. We give a fine-grained\nreformulation of the syntax by exposing implicit parameters and by unifying\nseveral operations on formulae via the notion of base change. Idempotency is\nachieved by means of an appropriate subtyping relation. We carry on an in-depth\nstudy of indLL as a logic, showing how it determines a refinement of classical\nlinear logic and establishing a terminating cut-elimination procedure.\nCut-elimination is proved to be confluent up to an appropriate congruence\ninduced by the subtyping relation.", "field": "Computer Science", "categories": "cs.LO"}, {"arxiv_id": "2401.14129", "title": "Performance Analysis for Near-Field ISAC: A Holographic MIMO Design", "abstract": "A near-field holographic multiple-input multiple-output (MIMO) based\nintegrated sensing and communications (ISAC) framework is proposed for both\ndownlink and uplink scenarios, where spherical wave-based model is considered\nto capture the characteristics of the near field. The coupling effect\nintroduced by the densely spaced antennas of the holographic MIMO are\ncharacterized by spatially correlated Rayleigh fading. Based on the proposed\nframework, by considering both instantaneous channel state information (CSI)\nand statistical CSI, closed-form expressions are derived for sensing rates\n(SRs), communication rates (CRs), and outage probabilities under different ISAC\ndesigns. Further insights are gained by examining high signal-to-noise ratio\nslopes and diversity orders. Specifically, 1) for the downlink case, a\nsensing-centric (S-C) design and a communications-centric (C-C) design are\ninvestigated based on different beamforming strategies, and a Pareto optimal\ndesign is proposed to characterize the attainable SR-CR region; and 2) for the\nuplink case, the S-C design and the C-C design are distinguished by the\ninterference cancellation order of the communication signal and the sensing\nsignal, and the rate region is obtained through a time-sharing strategy.\nNumerical results reveal that the proposed ISAC system achieves more extensive\nrate regions than the conventional frequency-division sensing and\ncommunications system, highlighting its superior performance.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.14131", "title": "Equivariant Manifold Neural ODEs and Differential Invariants", "abstract": "In this paper we develop a manifestly geometric framework for equivariant\nmanifold neural ordinary differential equations (NODEs), and use it to analyse\ntheir modelling capabilities for symmetric data. First, we consider the action\nof a Lie group $G$ on a smooth manifold $M$ and establish the equivalence\nbetween equivariance of vector fields, symmetries of the corresponding Cauchy\nproblems, and equivariance of the associated NODEs. We also propose a novel\nformulation of the equivariant NODEs in terms of the differential invariants of\nthe action of $G$ on $M$, based on Lie theory for symmetries of differential\nequations, which provides an efficient parameterisation of the space of\nequivariant vector fields in a way that is agnostic to both the manifold $M$\nand the symmetry group $G$. Second, we construct augmented manifold NODEs,\nthrough embeddings into equivariant flows, and show that they are universal\napproximators of equivariant diffeomorphisms on any path-connected $M$.\nFurthermore, we show that the augmented NODEs can be incorporated in the\ngeometric framework and parameterised using higher order differential\ninvariants. Finally, we consider the induced action of $G$ on different fields\non $M$ and show how it can be used to generalise previous work, on, e.g.,\ncontinuous normalizing flows, to equivariant models in any geometry.", "field": "Computer Science", "categories": "cs.LG,math.DS"}, {"arxiv_id": "2401.14132", "title": "Enabling Cross-Camera Collaboration for Video Analytics on Distributed\n  Smart Cameras", "abstract": "Overlapping cameras offer exciting opportunities to view a scene from\ndifferent angles, allowing for more advanced, comprehensive and robust\nanalysis. However, existing visual analytics systems for multi-camera streams\nare mostly limited to (i) per-camera processing and aggregation and (ii)\nworkload-agnostic centralized processing architectures. In this paper, we\npresent Argus, a distributed video analytics system with cross-camera\ncollaboration on smart cameras. We identify multi-camera, multi-target tracking\nas the primary task of multi-camera video analytics and develop a novel\ntechnique that avoids redundant, processing-heavy identification tasks by\nleveraging object-wise spatio-temporal association in the overlapping fields of\nview across multiple cameras. We further develop a set of techniques to perform\nthese operations across distributed cameras without cloud support at low\nlatency by (i) dynamically ordering the camera and object inspection sequence\nand (ii) flexibly distributing the workload across smart cameras, taking into\naccount network transmission and heterogeneous computational capacities.\nEvaluation of three real-world overlapping camera datasets with two Nvidia\nJetson devices shows that Argus reduces the number of object identifications\nand end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to\nthe state-of-the-art), while achieving comparable tracking quality.", "field": "Computer Science", "categories": "cs.CV,cs.DC"}, {"arxiv_id": "2401.14135", "title": "Convolutional Neural Networks can achieve binary bail judgement\n  classification", "abstract": "There is an evident lack of implementation of Machine Learning (ML) in the\nlegal domain in India, and any research that does take place in this domain is\nusually based on data from the higher courts of law and works with English\ndata. The lower courts and data from the different regional languages of India\nare often overlooked. In this paper, we deploy a Convolutional Neural Network\n(CNN) architecture on a corpus of Hindi legal documents. We perform a bail\nPrediction task with the help of a CNN model and achieve an overall accuracy of\n93\\% which is an improvement on the benchmark accuracy, set by Kapoor et al.\n(2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.", "field": "Computer Science", "categories": "cs.CL,cs.CY,cs.LG"}, {"arxiv_id": "2401.14136", "title": "Expression-aware video inpainting for HMD removal in XR applications", "abstract": "Head-mounted displays (HMDs) serve as indispensable devices for observing\nextended reality (XR) environments and virtual content. However, HMDs present\nan obstacle to external recording techniques as they block the upper face of\nthe user. This limitation significantly affects social XR applications,\nspecifically teleconferencing, where facial features and eye gaze information\nplay a vital role in creating an immersive user experience. In this study, we\npropose a new network for expression-aware video inpainting for HMD removal\n(EVI-HRnet) based on generative adversarial networks (GANs). Our model\neffectively fills in missing information with regard to facial landmarks and a\nsingle occlusion-free reference image of the user. The framework and its\ncomponents ensure the preservation of the user's identity across frames using\nthe reference frame. To further improve the level of realism of the inpainted\noutput, we introduce a novel facial expression recognition (FER) loss function\nfor emotion preservation. Our results demonstrate the remarkable capability of\nthe proposed framework to remove HMDs from facial videos while maintaining the\nsubject's facial expression and identity. Moreover, the outputs exhibit\ntemporal consistency along the inpainted frames. This lightweight framework\npresents a practical approach for HMD occlusion removal, with the potential to\nenhance various collaborative XR applications without the need for additional\nhardware.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14141", "title": "Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users", "abstract": "In the pursuit of bolstering user safety, social media platforms deploy\nactive moderation strategies, including content removal and user suspension.\nThese measures target users engaged in discussions marked by hate speech or\ntoxicity, often linked to specific keywords or hashtags. Nonetheless, the\nincreasing prevalence of toxicity indicates that certain users adeptly\ncircumvent these measures. This study examines consistently toxic users on\nTwitter (rebranded as X) Rather than relying on traditional methods based on\nspecific topics or hashtags, we employ a novel approach based on patterns of\ntoxic tweets, yielding deeper insights into their behavior. We analyzed 38\nmillion tweets from the timelines of 12,148 Twitter users and identified the\ntop 1,457 users who consistently exhibit toxic behavior, relying on metrics\nlike the Gini index and Toxicity score. By comparing their posting patterns to\nthose of non-consistently toxic users, we have uncovered distinctive temporal\npatterns, including contiguous activity spans, inter-tweet intervals (referred\nto as 'Burstiness'), and churn analysis. These findings provide strong evidence\nfor the existence of a unique tweeting pattern associated with toxic behavior\non Twitter. Crucially, our methodology transcends Twitter and can be adapted to\nvarious social media platforms, facilitating the identification of consistently\ntoxic users based on their posting behavior. This research contributes to\nongoing efforts to combat online toxicity and offers insights for refining\nmoderation strategies in the digital realm. We are committed to open research\nand will provide our code and data to the research community.", "field": "Computer Science", "categories": "cs.SI"}, {"arxiv_id": "2401.14142", "title": "Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept\n  Intervention, and Conditional Interpretations", "abstract": "Existing methods, such as concept bottleneck models (CBMs), have been\nsuccessful in providing concept-based interpretations for black-box deep\nlearning models. They typically work by predicting concepts given the input and\nthen predicting the final class label given the predicted concepts. However,\n(1) they often fail to capture the high-order, nonlinear interaction between\nconcepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not\nhelp correct highly correlated concepts (e.g., \"yellow belly\"), leading to\nsuboptimal final accuracy; (2) they cannot naturally quantify the complex\nconditional dependencies between different concepts and class labels (e.g., for\nan image with the class label \"Kentucky Warbler\" and a concept \"black bill\",\nwhat is the probability that the model correctly predicts another concept\n\"black crown\"), therefore failing to provide deeper insight into how a\nblack-box model works. In response to these limitations, we propose\nEnergy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural\nnetworks to define the joint energy of candidate (input, concept, class)\ntuples. With such a unified interface, prediction, concept correction, and\nconditional dependency quantification are then represented as conditional\nprobabilities, which are generated by composing different energy functions. Our\nECBMs address both limitations of existing CBMs, providing higher accuracy and\nricher concept interpretations. Empirical results show that our approach\noutperforms the state-of-the-art on real-world datasets.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.LG,stat.ML"}, {"arxiv_id": "2401.14147", "title": "Concept: Dynamic Risk Assessment for AI-Controlled Robotic Systems", "abstract": "AI-controlled robotic systems pose a risk to human workers and the\nenvironment. Classical risk assessment methods cannot adequately describe such\nblack box systems. Therefore, new methods for a dynamic risk assessment of such\nAI-controlled systems are required. In this paper, we introduce the concept of\na new dynamic risk assessment approach for AI-controlled robotic systems. The\napproach pipelines five blocks: (i) a Data Logging that logs the data of the\ngiven simulation, (ii) a Skill Detection that automatically detects the\nexecuted skills with a deep learning technique, (iii) a Behavioral Analysis\nthat creates the behavioral profile of the robotic systems, (iv) a Risk Model\nGeneration that automatically transforms the behavioral profile and risk data\ncontaining the failure probabilities of robotic hardware components into\nadvanced hybrid risk models, and (v) Risk Model Solvers for the numerical\nevaluation of the generated hybrid risk models.\n  Keywords: Dynamic Risk Assessment, Hybrid Risk Models, M2M Transformation,\nROS, AI-Controlled Robotic Systems, Deep Learning, Reinforcement Learning", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.14148", "title": "LanDA: Language-Guided Multi-Source Domain Adaptation", "abstract": "Multi-Source Domain Adaptation (MSDA) aims to mitigate changes in data\ndistribution when transferring knowledge from multiple labeled source domains\nto an unlabeled target domain. However, existing MSDA techniques assume target\ndomain images are available, yet overlook image-rich semantic information.\nConsequently, an open question is whether MSDA can be guided solely by textual\ncues in the absence of target domain images. By employing a multimodal model\nwith a joint image and language embedding space, we propose a novel\nlanguage-guided MSDA approach, termed LanDA, based on optimal transfer theory,\nwhich facilitates the transfer of multiple source domains to a new target\ndomain, requiring only a textual description of the target domain without\nneeding even a single target domain image, while retaining task-relevant\ninformation. We present extensive experiments across different transfer\nscenarios using a suite of relevant benchmarks, demonstrating that LanDA\noutperforms standard fine-tuning and ensemble approaches in both target and\nsource domains.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14149", "title": "Developing a High-Performance Process Mining Library with Java and\n  Python Bindings in Rust", "abstract": "The most commonly used open-source process mining software tools today are\nProM and PM4Py, written in Java and Python, respectively. Such high-level,\noften interpreted, programming languages trade off performance with memory\nsafety and ease-of-use. In contrast, traditional compiled languages, like C or\nC++, can achieve top performance but often suffer from instability related to\nunsafe memory management. Lately, Rust emerged as a highly performant, compiled\nprogramming language with inherent memory safety. In this paper, we describe\nour approach to developing a shared process mining library in Rust with\nbindings to both Java and Python, allowing full integration into the existing\necosystems, like ProM and PM4Py. By facilitating interoperability, our\nmethodology enables researchers or industry to develop novel algorithms in Rust\nonce and make them accessible to the entire community while also achieving\nsuperior performance.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.14151", "title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied\n  Environments via Reinforcement Learning", "abstract": "Despite the impressive performance across numerous tasks, large language\nmodels (LLMs) often fail in solving simple decision-making tasks due to the\nmisalignment of the knowledge in LLMs with environments. On the contrary,\nreinforcement learning (RL) agents learn policies from scratch, which makes\nthem always align with environments but difficult to incorporate prior\nknowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a\nnovel general online framework that deploys LLMs as decision-making agents to\nefficiently interact and align with embodied environments via RL without\nrequiring any prepared datasets or prior knowledge of the environments.\nFirstly, we query the joint probabilities of each valid action with LLMs to\nform behavior policies. Then, to enhance the stability and robustness of the\npolicies, we propose two normalization methods and summarize four prompt design\nprinciples. Finally, we design a novel parameter-efficient training\narchitecture where the actor and critic share one frozen LLM equipped with\nlow-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to\nevaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency\nand performance compared to the conventional RL method, PPO, and prompt tuning\nmethod, SayCan, in both classical decision-making environment, Overcooked, and\nsimulated household environment, VirtualHome. ii) Benefiting from LLMs'\nopen-vocabulary feature, TWOSOME shows superior generalization ability to\nunseen tasks. iii) Under our framework, there is no significant loss of the\nLLMs' original ability during online PPO finetuning.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CL"}, {"arxiv_id": "2401.14153", "title": "Agent-based Simulation with Netlogo to Evaluate AmI Scenarios", "abstract": "In this paper an agent-based simulation is developed in order to evaluate an\nAmI scenario based on agents. Many AmI applications are implemented through\nagents but they are not compared to any other existing alternative in order to\nevaluate the relative benefits of using them. The proposal simulation\nenvironment developed in Netlogo analyse such benefits using two evaluation\ncriteria: First, measuring agent satisfaction of different types of desires\nalong the execution. Second, measuring time savings obtained through a correct\nuse of context information.\n  So, here, a previously suggested agent architecture, an ontology and a\n12-steps protocol to provide AmI services in airports, is evaluated using a\nNetLogo simulation environment. The present work uses a NetLogo model\nconsidering scalability problems of this application domain but using FIPA and\nBDI extensions to be coherent with our previous works and our previous JADE\nimplementation of them.\n  The NetLogo model presented simulates an airport with agent users passing\nthrough several zones located in a specific order in a map: passport controls,\ncheck-in counters of airline companies, boarding gates, different types of\nshopping. Although initial data in simulations are generated randomly, and the\nmodel is just an approximation of real-world airports, the definition of this\ncase of use of Ambient Intelligence through NetLogo agents opens an interesting\nway to evaluate the benefits of using Ambient Intelligence, which is a\nsignificant contribution to the final development of them.", "field": "Computer Science", "categories": "cs.AI"}, {"arxiv_id": "2401.14155", "title": "Alleviating Structural Distribution Shift in Graph Anomaly Detection", "abstract": "Graph anomaly detection (GAD) is a challenging binary classification problem\ndue to its different structural distribution between anomalies and normal nodes\n-- abnormal nodes are a minority, therefore holding high heterophily and low\nhomophily compared to normal nodes. Furthermore, due to various time factors\nand the annotation preferences of human experts, the heterophily and homophily\ncan change across training and testing data, which is called structural\ndistribution shift (SDS) in this paper. The mainstream methods are built on\ngraph neural networks (GNNs), benefiting the classification of normals from\naggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and\nsuffering from poor generalization.\n  This work solves the problem from a feature view. We observe that the degree\nof SDS varies between anomalies and normal nodes. Hence to address the issue,\nthe key lies in resisting high heterophily for anomalies meanwhile benefiting\nthe learning of normals from homophily. We tease out the anomaly features on\nwhich we constrain to mitigate the effect of heterophilous neighbors and make\nthem invariant. We term our proposed framework as Graph Decomposition Network\n(GDN). Extensive experiments are conducted on two benchmark datasets, and the\nproposed framework achieves a remarkable performance boost in GAD, especially\nin an SDS environment where anomalies have largely different structural\ndistribution across training and testing environments. Codes are open-sourced\nin https://github.com/blacksingular/wsdm_GDN.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "abstract": "We introduce Grounded SAM, which uses Grounding DINO as an open-set object\ndetector to combine with the segment anything model (SAM). This integration\nenables the detection and segmentation of any regions based on arbitrary text\ninputs and opens a door to connecting various vision models. As shown in Fig.1,\na wide range of vision tasks can be achieved by using the versatile Grounded\nSAM pipeline. For example, an automatic annotation pipeline based solely on\ninput images can be realized by incorporating models such as BLIP and Recognize\nAnything. Additionally, incorporating Stable-Diffusion allows for controllable\nimage editing, while the integration of OSX facilitates promptable 3D human\nmotion analysis. Grounded SAM also shows superior performance on\nopen-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in\nthe wild) zero-shot benchmark with the combination of Grounding DINO-Base and\nSAM-Huge models.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14160", "title": "A Mathematical Theory of Semantic Communication: Overview", "abstract": "Semantic communication initiates a new direction for future communication. In\nthis paper, we aim to establish a systematic framework of semantic information\ntheory (SIT). First, we propose a semantic communication model and define the\nsynonymous mapping to indicate the critical relationship between semantic\ninformation and syntactic information. Based on this core concept, we introduce\nthe measures of semantic information, such as semantic entropy\n$H_s(\\tilde{U})$, up/down semantic mutual information\n$I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity\n$C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion\nfunction\n$R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq\nD}I_s(\\tilde{X};\\hat{\\tilde{X}})$. Furthermore, we prove three coding theorems\nof SIT, that is, the semantic source coding theorem, semantic channel coding\ntheorem, and semantic rate-distortion coding theorem. We find that the limits\nof information theory are extended by using synonymous mapping, that is,\n$H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works\ncomposite the basis of semantic information theory. In summary, the theoretic\nframework proposed in this paper is a natural extension of classic information\ntheory and may reveal great performance potential for future communication.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.14163", "title": "The stabilizer-free weak Galerkin finite element method for the\n  Biharmonic equation using polynomials of reduced order", "abstract": "In this article, we decrease the degree of the polynomials on the boundary of\nthe weak functions and modify the definition of the weak laplacian which are\nintroduced in \\cite{BiharmonicSFWG} to use the SFWG method for the biharmonic\nequation. Then we propose the relevant numerical format and obtain the optimal\norder of error estimates in $H^2$ and $L^2$ norms. Finally, we confirm the\nestimates using numerical experiments.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.14166", "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on\n  Few-shot Inference via Debiased Domain Abstraction", "abstract": "As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14168", "title": "Vivim: a Video Vision Mamba for Medical Video Object Segmentation", "abstract": "Traditional convolutional neural networks have a limited receptive field\nwhile transformer-based networks are mediocre in constructing long-term\ndependency from the perspective of computational complexity. Such the\nbottleneck poses a significant challenge when processing long video sequences\nin video analysis tasks. Very recently, the state space models (SSMs) with\nefficient hardware-aware designs, famous by Mamba, have exhibited impressive\nachievements in long sequence modeling, which facilitates the development of\ndeep neural networks on many vision tasks. To better capture available cues in\nvideo frames, this paper presents a generic Video Vision Mamba-based framework\nfor medical video object segmentation tasks, named Vivim. Our Vivim can\neffectively compress the long-term spatiotemporal representation into sequences\nat varying scales by our designed Temporal Mamba Block. Compared to existing\nvideo-level Transformer-based methods, our model maintains excellent\nsegmentation results with better speed performance. Extensive experiments on\nthe breast US dataset demonstrate the effectiveness and efficiency of our\nVivim. The code for Vivim is available at:\nhttps://github.com/scott-yjyang/Vivim.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14169", "title": "A finite volume method preserving the invariant region property for the\n  quasimonotone reaction-diffusion systems", "abstract": "We present a finite volume method preserving the invariant region property\n(IRP) for the reaction-diffusion systems with quasimonotone functions,\nincluding nondecreasing, decreasing, and mixed quasimonotone systems. The\ndiffusion terms and time derivatives are discretized by a finite volume method\nsatisfying the discrete maximum principle (DMP) and the backward Euler method,\nrespectively. The discretization leads to an implicit and nonlinear scheme, and\nit is proved to preserve the invariant region property unconditionally. We\nconstruct an iterative algorithm and prove the invariant region property ar\neach iteration step. Numerical examples are shown to confirm the accuracy and\ninvariant region property of our scheme.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.14174", "title": "The Boundaries of Tractability in Hierarchical Task Network Planning", "abstract": "We study the complexity-theoretic boundaries of tractability for three\nclassical problems in the context of Hierarchical Task Network Planning: the\nvalidation of a provided plan, whether an executable plan exists, and whether a\ngiven state can be reached by some plan. We show that all three problems can be\nsolved in polynomial time on primitive task networks of constant partial order\nwidth (and a generalization thereof), whereas for the latter two problems this\nholds only under a provably necessary restriction to the state space. Next, we\nobtain an algorithmic meta-theorem along with corresponding lower bounds to\nidentify tight conditions under which general polynomial-time solvability\nresults can be lifted from primitive to general task networks. Finally, we\nenrich our investigation by analyzing the parameterized complexity of the three\nconsidered problems, and show that (1) fixed-parameter tractability for all\nthree problems can be achieved by replacing the partial order width with the\nvertex cover number of the network as the parameter, and (2) other classical\ngraph-theoretic parameters of the network (including treewidth, treedepth, and\nthe aforementioned partial order width) do not yield fixed-parameter\ntractability for any of the three problems.", "field": "Computer Science", "categories": "cs.CC,cs.AI"}, {"arxiv_id": "2401.14176", "title": "Copilot Refinement: Addressing Code Smells in Copilot-Generated Python\n  Code", "abstract": "As one of the most popular dynamic languages, Python experiences a decrease\nin readability and maintainability when code smells are present. Recent\nadvancements in Large Language Models have sparked growing interest in\nAI-enabled tools for both code generation and refactoring. GitHub Copilot is\none such tool that has gained widespread usage. Copilot Chat, released on\nSeptember 2023, functions as an interactive tool aims at facilitating natural\nlanguage-powered coding. However, limited attention has been given to\nunderstanding code smells in Copilot-generated Python code and Copilot's\nability to fix the code smells it generates. To this end, we built a dataset\ncomprising 102 code smells in Copilot-generated Python code. Our aim is to\nfirst explore the occurrence of code smells in Copilot-generated Python code\nand then evaluate the effectiveness of Copilot in fixing these code smells\nemploying different prompts. The results show that 8 out of 10 types of Python\nsmells can be detected in Copilot-generated Python code, among which\nMultiply-Nested Container is the most common one. For these code smells,\nCopilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing\nPython code smells generated by Copilot itself. Besides, the effectiveness of\nCopilot Chat in fixing these smells can be improved with the provision of more\ndetailed prompts. However, using Copilot Chat to fix these smells might\nintroduce new code smells.", "field": "Computer Science", "categories": "cs.SE,cs.AI"}, {"arxiv_id": "2401.14183", "title": "Towards Autonomous Supply Chains: Definition, Characteristics,\n  Conceptual Framework, and Autonomy Levels", "abstract": "Recent global disruptions, such as the pandemic and geopolitical conflicts,\nhave profoundly exposed vulnerabilities in traditional supply chains, requiring\nexploration of more resilient alternatives. Autonomous supply chains (ASCs)\nhave emerged as a potential solution, offering increased visibility,\nflexibility, and resilience in turbulent trade environments. Despite\ndiscussions in industry and academia over several years, ASCs lack\nwell-established theoretical foundations. This paper addresses this research\ngap by presenting a formal definition of ASC along with its defining\ncharacteristics and auxiliary concepts. We propose a layered conceptual\nframework called the MIISI model. An illustrative case study focusing on the\nmeat supply chain demonstrates an initial ASC implementation based on this\nconceptual model. Additionally, we introduce a seven-level supply chain\nautonomy reference model, delineating a trajectory towards achieving a full\nsupply chain autonomy. Recognising that this work represents an initial\nendeavour, we emphasise the need for continued exploration in this emerging\ndomain. We anticipate that this work will stimulate further research, both\ntheoretical and technical, and contribute to the continual evolution of ASCs.", "field": "Computer Science", "categories": "cs.AI,cs.MA,cs.SY,eess.SY,math.OC"}, {"arxiv_id": "2401.14184", "title": "Friendly Attacks to Improve Channel Coding Reliability", "abstract": "This paper introduces a novel approach called \"friendly attack\" aimed at\nenhancing the performance of error correction channel codes. Inspired by the\nconcept of adversarial attacks, our method leverages the idea of introducing\nslight perturbations to the neural network input, resulting in a substantial\nimpact on the network's performance. By introducing small perturbations to\nfixed-point modulated codewords before transmission, we effectively improve the\ndecoder's performance without violating the input power constraint. The\nperturbation design is accomplished by a modified iterative fast gradient\nmethod. This study investigates various decoder architectures suitable for\ncomputing gradients to obtain the desired perturbations. Specifically, we\nconsider belief propagation (BP) for LDPC codes; the error correcting code\ntransformer, BP and neural BP (NBP) for polar codes, and neural BCJR for\nconvolutional codes. We demonstrate that the proposed friendly attack method\ncan improve the reliability across different channels, modulations, codes, and\ndecoders. This method allows us to increase the reliability of communication\nwith a legacy receiver by simply modifying the transmitted codeword\nappropriately.", "field": "Computer Science", "categories": "cs.IT,cs.LG,math.IT"}, {"arxiv_id": "2401.14185", "title": "TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down\n  Fusion", "abstract": "Audio-visual speech separation has gained significant traction in recent\nyears due to its potential applications in various fields such as speech\nrecognition, diarization, scene analysis and assistive technologies. Designing\na lightweight audio-visual speech separation network is important for\nlow-latency applications, but existing methods often require higher\ncomputational costs and more parameters to achieve better separation\nperformance. In this paper, we present an audio-visual speech separation model\ncalled Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for\naudio-visual speech separation, which builds upon the architecture of TDANet,\nan audio-only speech separation method. TDANet serves as the architectural\nfoundation for the auditory and visual networks within TDFNet, offering an\nefficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet\nachieves a performance increase of up to 10\\% across all performance metrics\ncompared with the previous SOTA method CTCNet. Remarkably, these results are\nachieved using fewer parameters and only 28\\% of the multiply-accumulate\noperations (MACs) of CTCNet. In essence, our method presents a highly effective\nand efficient solution to the challenges of speech separation within the\naudio-visual domain, making significant strides in harnessing visual\ninformation optimally.", "field": "Computer Science", "categories": "cs.SD,cs.AI,eess.AS"}, {"arxiv_id": "2401.14192", "title": "How Can Large Language Models Understand Spatial-Temporal Data?", "abstract": "While Large Language Models (LLMs) dominate tasks like natural language\nprocessing and computer vision, harnessing their power for spatial-temporal\nforecasting remains challenging. The disparity between sequential text and\ncomplex spatial-temporal data hinders this application. To address this issue,\nthis paper introduces STG-LLM, an innovative approach empowering LLMs for\nspatial-temporal forecasting. We tackle the data mismatch by proposing: 1)\nSTG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph\ndata into concise tokens capturing both spatial and temporal relationships; 2)\nSTG-Adapter: This minimalistic adapter, consisting of linear encoding and\ndecoding layers, bridges the gap between tokenized data and LLM comprehension.\nBy fine-tuning only a small set of parameters, it can effectively grasp the\nsemantics of tokens generated by STG-Tokenizer, while preserving the original\nnatural language understanding capabilities of LLMs. Extensive experiments on\ndiverse spatial-temporal benchmark datasets show that STG-LLM successfully\nunlocks LLM potential for spatial-temporal forecasting. Remarkably, our\napproach achieves competitive performance on par with dedicated SOTA methods.", "field": "Computer Science", "categories": "cs.LG,cs.CL"}, {"arxiv_id": "2401.14194", "title": "Parameter-Efficient Conversational Recommender System as a Language\n  Processing Task", "abstract": "Conversational recommender systems (CRS) aim to recommend relevant items to\nusers by eliciting user preference through natural language conversation. Prior\nwork often utilizes external knowledge graphs for items' semantic information,\na language model for dialogue generation, and a recommendation module for\nranking relevant items. This combination of multiple components suffers from a\ncumbersome training process, and leads to semantic misalignment issues between\ndialogue generation and item recommendation. In this paper, we represent items\nin natural language and formulate CRS as a natural language processing task.\nAccordingly, we leverage the power of pre-trained language models to encode\nitems, understand user intent via conversation, perform item recommendation\nthrough semantic matching, and generate dialogues. As a unified model, our\nPECRS (Parameter-Efficient CRS), can be optimized in a single stage, without\nrelying on non-textual metadata such as a knowledge graph. Experiments on two\nbenchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of\nPECRS on recommendation and conversation. Our code is available at:\nhttps://github.com/Ravoxsg/efficient_unified_crs.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.14196", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The\n  Rise of Code Intelligence", "abstract": "The rapid development of large language models has revolutionized code\nintelligence in software development. However, the predominance of\nclosed-source models has restricted extensive research and development. To\naddress this, we introduce the DeepSeek-Coder series, a range of open-source\ncode models with sizes from 1.3B to 33B, trained from scratch on 2 trillion\ntokens. These models are pre-trained on a high-quality project-level code\ncorpus and employ a fill-in-the-blank task with a 16K window to enhance code\ngeneration and infilling. Our extensive evaluations demonstrate that\nDeepSeek-Coder not only achieves state-of-the-art performance among open-source\ncode models across multiple benchmarks but also surpasses existing\nclosed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted\ncommercial use.", "field": "Computer Science", "categories": "cs.SE,cs.CL,cs.LG"}, {"arxiv_id": "2401.14199", "title": "MTRGL:Effective Temporal Correlation Discerning through Multi-modal\n  Temporal Relational Graph Learning", "abstract": "In this study, we explore the synergy of deep learning and financial market\napplications, focusing on pair trading. This market-neutral strategy is\nintegral to quantitative finance and is apt for advanced deep-learning\ntechniques. A pivotal challenge in pair trading is discerning temporal\ncorrelations among entities, necessitating the integration of diverse data\nmodalities. Addressing this, we introduce a novel framework, Multi-modal\nTemporal Relation Graph Learning (MTRGL). MTRGL combines time series data and\ndiscrete features into a temporal graph and employs a memory-based temporal\ngraph neural network. This approach reframes temporal correlation\nidentification as a temporal graph link prediction task, which has shown\nempirical success. Our experiments on real-world datasets confirm the superior\nperformance of MTRGL, emphasizing its promise in refining automated pair\ntrading strategies.", "field": "Computer Science", "categories": "cs.LG,econ.GN,q-fin.EC,q-fin.TR"}, {"arxiv_id": "2401.14210", "title": "At the junction between deep learning and statistics of extremes:\n  formalizing the landslide hazard definition", "abstract": "The most adopted definition of landslide hazard combines spatial information\nabout landslide location (susceptibility), threat (intensity), and frequency\n(return period). Only the first two elements are usually considered and\nestimated when working over vast areas. Even then, separate models constitute\nthe standard, with frequency being rarely investigated. Frequency and intensity\nare intertwined and depend on each other because larger events occur less\nfrequently and vice versa. However, due to the lack of multi-temporal\ninventories and joint statistical models, modelling such properties via a\nunified hazard model has always been challenging and has yet to be attempted.\nHere, we develop a unified model to estimate landslide hazard at the slope unit\nlevel to address such gaps. We employed deep learning, combined with a model\nmotivated by extreme-value theory to analyse an inventory of 30 years of\nobserved rainfall-triggered landslides in Nepal and assess landslide hazard for\nmultiple return periods. We also use our model to further explore landslide\nhazard for the same return periods under different climate change scenarios up\nto the end of the century. Our results show that the proposed model performs\nexcellently and can be used to model landslide hazard in a unified manner.\nGeomorphologically, we find that under both climate change scenarios (SSP245\nand SSP885), landslide hazard is likely to increase up to two times on average\nin the lower Himalayan regions while remaining the same in the middle Himalayan\nregion whilst decreasing slightly in the upper Himalayan region areas.", "field": "Computer Science", "categories": "cs.LG,physics.geo-ph,stat.AP,stat.ML"}, {"arxiv_id": "2401.14211", "title": "Communication-Efficient Federated Learning through Adaptive Weight\n  Clustering and Server-Side Distillation", "abstract": "Federated Learning (FL) is a promising technique for the collaborative\ntraining of deep neural networks across multiple devices while preserving data\nprivacy. Despite its potential benefits, FL is hindered by excessive\ncommunication costs due to repeated server-client communication during\ntraining. To address this challenge, model compression techniques, such as\nsparsification and weight clustering are applied, which often require modifying\nthe underlying model aggregation schemes or involve cumbersome hyperparameter\ntuning, with the latter not only adjusts the model's compression rate but also\nlimits model's potential for continuous improvement over growing data. In this\npaper, we propose FedCompress, a novel approach that combines dynamic weight\nclustering and server-side knowledge distillation to reduce communication costs\nwhile learning highly generalizable models. Through a comprehensive evaluation\non diverse public datasets, we demonstrate the efficacy of our approach\ncompared to baselines in terms of communication costs and inference speed. We\nwill make our implementation public upon acceptance.", "field": "Computer Science", "categories": "cs.LG,cs.DC"}, {"arxiv_id": "2401.14212", "title": "Explicitly Representing Syntax Improves Sentence-to-layout Prediction of\n  Unexpected Situations", "abstract": "Recognizing visual entities in a natural language sentence and arranging them\nin a 2D spatial layout require a compositional understanding of language and\nspace. This task of layout prediction is valuable in text-to-image synthesis as\nit allows localized and controlled in-painting of the image. In this\ncomparative study it is shown that we can predict layouts from language\nrepresentations that implicitly or explicitly encode sentence syntax, if the\nsentences mention similar entity-relationships to the ones seen during\ntraining. To test compositional understanding, we collect a test set of\ngrammatically correct sentences and layouts describing compositions of entities\nand relations that unlikely have been seen during training. Performance on this\ntest set substantially drops, showing that current models rely on correlations\nin the training data and have difficulties in understanding the structure of\nthe input sentences. We propose a novel structural loss function that better\nenforces the syntactic structure of the input sentence and show large\nperformance gains in the task of 2D spatial layout prediction conditioned on\ntext. The loss has the potential to be used in other generation tasks where a\ntree-like structure underlies the conditioning modality. Code, trained models\nand the USCOCO evaluation set will be made available via github.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.14214", "title": "A Quantitative Version of More Capable Channel Comparison", "abstract": "This paper introduces a quantitative generalization of the ``more capable''\ncomparison of broadcast channels, which is termed ``more capable with\nadvantage''. Some basic properties are demonstrated (including tensorization on\nproduct channels), and a characterisation is given for the cases of Binary\nSymmetric Channel (BSC) and Binary Erasure Channel (BEC).\n  It is then applied to two problems. First, a list decoding bound on the BSC\nis given that applies to transitive codes that achieve capacity on the BEC.\nSecond, new lower bounds on entropy rates of binary hidden Markov processes are\nderived.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.14215", "title": "Commonsense-augmented Memory Construction and Management in Long-term\n  Conversations via Context-aware Persona Refinement", "abstract": "Memorizing and utilizing speakers' personas is a common practice for response\ngeneration in long-term conversations. Yet, human-authored datasets often\nprovide uninformative persona sentences that hinder response quality. This\npaper presents a novel framework that leverages commonsense-based persona\nexpansion to address such issues in long-term conversation. While prior work\nfocuses on not producing personas that contradict others, we focus on\ntransforming contradictory personas into sentences that contain rich speaker\ninformation, by refining them based on their contextual backgrounds with\ndesigned strategies. As the pioneer of persona expansion in multi-session\nsettings, our framework facilitates better response generation via human-like\npersona refinement. The supplementary video of our work is available at\nhttps://caffeine-15bbf.web.app/.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14226", "title": "Sample Efficient Reinforcement Learning by Automatically Learning to\n  Compose Subtasks", "abstract": "Improving sample efficiency is central to Reinforcement Learning (RL),\nespecially in environments where the rewards are sparse. Some recent approaches\nhave proposed to specify reward functions as manually designed or learned\nreward structures whose integrations in the RL algorithms are claimed to\nsignificantly improve the learning efficiency. Manually designed reward\nstructures can suffer from inaccuracy and existing automatically learning\nmethods are often computationally intractable for complex tasks. The\nintegration of inaccurate or partial reward structures in RL algorithms fail to\nlearn optimal policies. In this work, we propose an RL algorithm that can\nautomatically structure the reward function for sample efficiency, given a set\nof labels that signify subtasks. Given such minimal knowledge about the task,\nwe train a high-level policy that selects optimal sub-tasks in each state\ntogether with a low-level policy that efficiently learns to complete each\nsub-task. We evaluate our algorithm in a variety of sparse-reward environments.\nThe experiment results show that our approach significantly outperforms the\nstate-of-art baselines as the difficulty of the task increases.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.14228", "title": "Assessing the Portability of Parameter Matrices Trained by\n  Parameter-Efficient Finetuning Methods", "abstract": "As the cost of training ever larger language models has grown, so has the\ninterest in reusing previously learnt knowledge. Transfer learning methods have\nshown how reusing non-task-specific knowledge can help in subsequent\ntask-specific learning. In this paper, we investigate the inverse: porting\nwhole functional modules that encode task-specific knowledge from one model to\nanother. We designed a study comprising 1,440 training/testing runs to test the\nportability of modules trained by parameter-efficient finetuning (PEFT)\ntechniques, using sentiment analysis as an example task. We test portability in\na wide range of scenarios, involving different PEFT techniques and different\npretrained host models, among other dimensions. We compare the performance of\nported modules with that of equivalent modules trained (i) from scratch, and\n(ii) from parameters sampled from the same distribution as the ported module.\nWe find that the ported modules far outperform the two alternatives tested, but\nthat there are interesting performance differences between the four PEFT\ntechniques. We conclude that task-specific knowledge in the form of\nstructurally modular sets of parameters as produced by PEFT techniques is\nhighly portable, but that degree of success depends on type of PEFT and on\ndifferences between originating and receiving pretrained models.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.14231", "title": "Strongly k-recursive sequences", "abstract": "Drawing inspiration from a recent paper of Heuberger, Krenn, and Lipnik, we\ndefine the class of strongly k-recursive sequences. We show that every\nk-automatic sequence is strongly $k$-recursive, therefore k-recursive, and\ndiscuss that the converse is not true.\n  We also show that the class of strongly k-recursive sequences is a proper\nsubclass of the class of k-regular sequences, and we present some explicit\nexamples. We then extend the proof techniques to answer the same question for\nthe class of k-recursive sequences.", "field": "Computer Science", "categories": "cs.FL,cs.DM,math.CO"}, {"arxiv_id": "2401.14232", "title": "AR-GAN: Generative Adversarial Network-Based Defense Method Against\n  Adversarial Attacks on the Traffic Sign Classification System of Autonomous\n  Vehicles", "abstract": "This study developed a generative adversarial network (GAN)-based defense\nmethod for traffic sign classification in an autonomous vehicle (AV), referred\nto as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i)\nassuming zero knowledge of adversarial attack models and samples and (ii)\nproviding consistently high traffic sign classification performance under\nvarious adversarial attack types. The AR-GAN classification system consists of\na generator that denoises an image by reconstruction, and a classifier that\nclassifies the reconstructed image. The authors have tested the AR-GAN under\nno-attack and under various adversarial attacks, such as Fast Gradient Sign\nMethod (FGSM), DeepFool, Carlini and Wagner (C&W), and Projected Gradient\nDescent (PGD). The authors considered two forms of these attacks, i.e., (i)\nblack-box attacks (assuming the attackers possess no prior knowledge of the\nclassifier), and (ii) white-box attacks (assuming the attackers possess full\nknowledge of the classifier). The classification performance of the AR-GAN was\ncompared with several benchmark adversarial defense methods. The results showed\nthat both the AR-GAN and the benchmark defense methods are resilient against\nblack-box attacks and could achieve similar classification performance to that\nof the unperturbed images. However, for all the white-box attacks considered in\nthis study, the AR-GAN method outperformed the benchmark defense methods. In\naddition, the AR-GAN was able to maintain its high classification performance\nunder varied white-box adversarial perturbation magnitudes, whereas the\nperformance of the other defense methods dropped abruptly at increased\nperturbation magnitudes.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.CR,cs.LG"}, {"arxiv_id": "2401.14236", "title": "Exploring the Unexplored: Understanding the Impact of Layer Adjustments\n  on Image Classification", "abstract": "This paper investigates how adjustments to deep learning architectures impact\nmodel performance in image classification. Small-scale experiments generate\ninitial insights although the trends observed are not consistent with the\nentire dataset. Filtering operations in the image processing pipeline are\ncrucial, with image filtering before pre-processing yielding better results.\nThe choice and order of layers as well as filter placement significantly impact\nmodel performance. This study provides valuable insights into optimizing deep\nlearning models, with potential avenues for future research including\ncollaborative platforms.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14240", "title": "Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer\n  Models for Classifying Depression Severity in English and Luganda", "abstract": "Depression is a global burden and one of the most challenging mental health\nconditions to control. Experts can detect its severity early using the Beck\nDepression Inventory (BDI) questionnaire, administer appropriate medication to\npatients, and impede its progression. Due to the fear of potential\nstigmatization, many patients turn to social media platforms like Reddit for\nadvice and assistance at various stages of their journey. This research\nextracts text from Reddit to facilitate the diagnostic process. It employs a\nproposed labeling approach to categorize the text and subsequently fine-tunes\nthe Longformer model. The model's performance is compared against baseline\nmodels, including Naive Bayes, Random Forest, Support Vector Machines, and\nGradient Boosting. Our findings reveal that the Longformer model outperforms\nthe baseline models in both English (48%) and Luganda (45%) languages on a\ncustom-made dataset.", "field": "Computer Science", "categories": "cs.CL,cs.LG"}, {"arxiv_id": "2401.14241", "title": "New Algorithms for Computing Sibson Capacity and Arimoto Capacity", "abstract": "The Arimoto capacity and Sibson capacity, which are based on the Arimoto and\nSibson mutual information (MI) of order {\\alpha}, respectively, are well-known\ngeneralizations of the channel capacity C. In this study, we derive novel\nalternating optimization algorithms for computing these capacities by providing\nnew max characterizations of the Arimoto MI and Sibson MI. Moreover, we prove\nthat all iterative algorithms for computing these capacities are equivalent\nunder appropriate conditions imposed on their initial distributions", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.14242", "title": "Improving Natural Language Capability of Code Large Language Model", "abstract": "Code large language models (Code LLMs) have demonstrated remarkable\nperformance in code generation. Nonetheless, most existing works focus on\nboosting code LLMs from the perspective of programming capabilities, while\ntheir natural language capabilities receive less attention. To fill this gap,\nwe thus propose a novel framework, comprising two modules: AttentionExtractor,\nwhich is responsible for extracting key phrases from the user's natural\nlanguage requirements, and AttentionCoder, which leverages these extracted\nphrases to generate target code to solve the requirement. This framework\npioneers an innovative idea by seamlessly integrating code LLMs with\ntraditional natural language processing tools. To validate the effectiveness of\nthe framework, we craft a new code generation benchmark, called MultiNL-H,\ncovering five natural languages. Extensive experimental results demonstrate the\neffectiveness of our proposed framework.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.14244", "title": "Contract Usage and Evolution in Android Mobile Applications", "abstract": "Formal contracts and assertions are effective methods to enhance software\nquality by enforcing preconditions, postconditions, and invariants. Previous\nresearch has demonstrated the value of contracts in traditional software\ndevelopment contexts. However, the adoption and impact of contracts in the\ncontext of mobile application development, particularly of Android\napplications, remain unexplored.\n  To address this, we present the first large-scale empirical study on the\npresence and use of contracts in Android applications, written in Java or\nKotlin. We consider different types of contract elements divided into five\ncategories: conditional runtime exceptions, APIs, annotations, assertions, and\nother. We analyzed 2,390 Android applications from the F-Droid repository and\nprocessed more than 51,749 KLOC to determine 1) how and to what extent\ncontracts are used, 2) how contract usage evolves, and 3) whether contracts are\nused safely in the context of program evolution and inheritance. Our findings\ninclude: 1) although most applications do not specify contracts,\nannotation-based approaches are the most popular among practitioners; 2)\napplications that use contracts continue to use them in later versions, but the\nnumber of methods increases at a higher rate than the number of contracts; and\n3) there are many potentially unsafe specification changes when applications\nevolve and in subtyping relationships, which indicates a lack of specification\nstability. Our findings show that it would be desirable to have libraries that\nstandardize contract specifications in Java and Kotlin, and tools that aid\npractitioners in writing stronger contracts and in detecting contract\nviolations in the context of program evolution and inheritance.", "field": "Computer Science", "categories": "cs.SE,cs.LO,cs.PL"}, {"arxiv_id": "2401.14250", "title": "JUMP: A joint multimodal registration pipeline for neuroimaging with\n  minimal preprocessing", "abstract": "We present a pipeline for unbiased and robust multimodal registration of\nneuroimaging modalities with minimal pre-processing. While typical multimodal\nstudies need to use multiple independent processing pipelines, with diverse\noptions and hyperparameters, we propose a single and structured framework to\njointly process different image modalities. The use of state-of-the-art\nlearning-based techniques enables fast inferences, which makes the presented\nmethod suitable for large-scale and/or multi-cohort datasets with a diverse\nnumber of modalities per session. The pipeline currently works with structural\nMRI, resting state fMRI and amyloid PET images. We show the predictive power of\nthe derived biomarkers using in a case-control study and study the cross-modal\nrelationship between different image modalities. The code can be found in\nhttps: //github.com/acasamitjana/JUMP.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14252", "title": "On mission Twitter Profiles: A Study of Selective Toxic Behavior", "abstract": "The argument for persistent social media influence campaigns, often funded by\nmalicious entities, is gaining traction. These entities utilize instrumented\nprofiles to disseminate divisive content and disinformation, shaping public\nperception. Despite ample evidence of these instrumented profiles, few\nidentification methods exist to locate them in the wild. To evade detection and\nappear genuine, small clusters of instrumented profiles engage in unrelated\ndiscussions, diverting attention from their true goals. This strategic thematic\ndiversity conceals their selective polarity towards certain topics and fosters\npublic trust.\n  This study aims to characterize profiles potentially used for influence\noperations, termed 'on-mission profiles,' relying solely on thematic content\ndiversity within unlabeled data. Distinguishing this work is its focus on\ncontent volume and toxicity towards specific themes. Longitudinal data from\n138K Twitter or X, profiles and 293M tweets enables profiling based on theme\ndiversity. High thematic diversity groups predominantly produce toxic content\nconcerning specific themes, like politics, health, and news classifying them as\n'on-mission' profiles.\n  Using the identified ``on-mission\" profiles, we design a classifier for\nunseen, unlabeled data. Employing a linear SVM model, we train and test it on\nan 80/20% split of the most diverse profiles. The classifier achieves a\nflawless 100% accuracy, facilitating the discovery of previously unknown\n``on-mission\" profiles in the wild.", "field": "Computer Science", "categories": "cs.CY"}, {"arxiv_id": "2401.14255", "title": "Interpretable Solutions for Breast Cancer Diagnosis with Grammatical\n  Evolution and Data Augmentation", "abstract": "Medical imaging diagnosis increasingly relies on Machine Learning (ML)\nmodels. This is a task that is often hampered by severely imbalanced datasets,\nwhere positive cases can be quite rare. Their use is further compromised by\ntheir limited interpretability, which is becoming increasingly important. While\npost-hoc interpretability techniques such as SHAP and LIME have been used with\nsome success on so-called black box models, the use of inherently\nunderstandable models makes such endeavors more fruitful. This paper addresses\nthese issues by demonstrating how a relatively new synthetic data generation\ntechnique, STEM, can be used to produce data to train models produced by\nGrammatical Evolution (GE) that are inherently understandable. STEM is a\nrecently introduced combination of the Synthetic Minority Oversampling\nTechnique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously\nbeen successfully used to tackle both between class and within class imbalance\nissues. We test our technique on the Digital Database for Screening Mammography\n(DDSM) and the Wisconsin Breast Cancer (WBC) datasets and compare Area Under\nthe Curve (AUC) results with an ensemble of the top three performing\nclassifiers from a set of eight standard ML classifiers with varying degrees of\ninterpretability. We demonstrate that the GE-derived models present the best\nAUC while still maintaining interpretable solutions.", "field": "Computer Science", "categories": "cs.LG,cs.NE"}, {"arxiv_id": "2401.14256", "title": "Producing Plankton Classifiers that are Robust to Dataset Shift", "abstract": "Modern plankton high-throughput monitoring relies on deep learning\nclassifiers for species recognition in water ecosystems. Despite satisfactory\nnominal performances, a significant challenge arises from Dataset Shift, which\ncauses performances to drop during deployment. In our study, we integrate the\nZooLake dataset with manually-annotated images from 10 independent days of\ndeployment, serving as test cells to benchmark Out-Of-Dataset (OOD)\nperformances. Our analysis reveals instances where classifiers, initially\nperforming well in In-Dataset conditions, encounter notable failures in\npractical scenarios. For example, a MobileNet with a 92% nominal test accuracy\nshows a 77% OOD accuracy. We systematically investigate conditions leading to\nOOD performance drops and propose a preemptive assessment method to identify\npotential pitfalls when classifying new data, and pinpoint features in OOD\nimages that adversely impact classification. We present a three-step pipeline:\n(i) identifying OOD degradation compared to nominal test performance, (ii)\nconducting a diagnostic analysis of degradation causes, and (iii) providing\nsolutions. We find that ensembles of BEiT vision transformers, with targeted\naugmentations addressing OOD robustness, geometric ensembling, and\nrotation-based test-time augmentation, constitute the most robust model, which\nwe call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated\non container classes. Moreover, it exhibits lower sensitivity to dataset shift,\nand reproduces well the plankton abundances. Our proposed pipeline is\napplicable to generic plankton classifiers, contingent on the availability of\nsuitable test cells. By identifying critical shortcomings and offering\npractical procedures to fortify models against dataset shift, our study\ncontributes to the development of more reliable plankton classification\ntechnologies.", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.14257", "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation", "abstract": "Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.14263", "title": "Pulse width modulation technique with harmonic injection in the\n  modulating wave and discontinuous frequency modulation for the carrier wave\n  to reduce vibrations in asynchronous machines", "abstract": "A new carrier-based pulse-width modulation (PWM) technique to control power\ninverters is presented in this paper. To generate the output waveform, this\ntechnique compares a harmonic-injection modulating wave and a\nfrequency-modulated triangular carrier wave. The instantaneous frequency for\nthe carrier wave is adjusted according to a periodic function synchronized with\nthe fundamental term of the modulating wave. The main motivation for using this\ntechnique compared to a classic PWM sinusoidal technique revolves around the\nreduction of total harmonic distortion, the reduction of the distortion factor\nand the shift of temporal harmonics to higher frequencies for any modulation\nfrequency order. Experimental results show that it is possible to optimize the\ntime harmonics generated to minimize vibrations produced by an induction motor\nwhen it is fed with a DC/AC converter controlled by the proposed control\nstrategy. This is made possible by using a control parameter that modifies the\ninstantaneous frequency of the carrier wave without modifying the number of\npulses per period of the modulating wave, i. e. the mean value of the carrier\nwave frequency. The proposed technique is applied to an open loop-controlled\ninverter that operates an induction motor, helping to reduce the vibration\nlevels produced.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.14265", "title": "Worst-Case Per-User Error Bound for Asynchronous Unsourced Multiple\n  Access", "abstract": "This work considers an asynchronous $\\textsf{K}_a$-active-user unsourced\nmultiple access channel (AUMAC) with the worst-case asynchronicity. The\ntransmitted messages must be decoded within $n$ channel uses, while some\ncodewords are not completely received due to asynchronicities. We consider a\nconstraint of the largest allowed delay of the transmission. The AUMAC lacks\nthe permutation-invariant property of the synchronous UMAC since different\npermutations of the same codewords with a fixed asynchronicity are\ndistinguishable. Hence, the analyses require calculating all\n$2^{\\textsf{K}_a}-1$ combinations of erroneously decoded messages. Moreover,\ntransmitters cannot adapt the corresponding codebooks according to\nasynchronicity due to a lack of information on asynchronicities. To overcome\nthis challenge, a uniform bound of the per-user probability of error (PUPE) is\nderived by investigating the worst-case of the asynchronous patterns with the\ndelay constraint. Numerical results show the trade-off between the\nenergy-per-bit and the number of active users for different delay constraints.\nIn addition, although the asynchronous transmission reduces interference, the\nrequired energy-per-bit increases as the receiver decodes with incompletely\nreceived codewords, compared to the synchronous case.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.14267", "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time", "abstract": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence into a long\n\"encoding vector\" - that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity, traveling across single cortical regions or across\nmultiple regions at the whole-brain scale, could implement a similar encoding\nprinciple. By encapsulating recent input history into a single spatial pattern\nat each moment in time, cortical waves may enable temporal context to be\nextracted from sequences of sensory inputs, the same computational principle\nused in transformers.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14268", "title": "GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone", "abstract": "Virtual assistants have the potential to play an important role in helping\nusers achieves different tasks. However, these systems face challenges in their\nreal-world usability, characterized by inefficiency and struggles in grasping\nuser intentions. Leveraging recent advances in Large Language Models (LLMs), we\nintroduce GptVoiceTasker, a virtual assistant poised to enhance user\nexperiences and task efficiency on mobile devices. GptVoiceTasker excels at\nintelligently deciphering user commands and executing relevant device\ninteractions to streamline task completion. The system continually learns from\nhistorical user commands to automate subsequent usages, further enhancing\nexecution efficiency. Our experiments affirm GptVoiceTasker's exceptional\ncommand interpretation abilities and the precision of its task automation\nmodule. In our user study, GptVoiceTasker boosted task efficiency in real-world\nscenarios by 34.85%, accompanied by positive participant feedback. We made\nGptVoiceTasker open-source, inviting further research into LLMs utilization for\ndiverse tasks through prompt engineering and leveraging user usage data to\nimprove efficiency.", "field": "Computer Science", "categories": "cs.HC"}, {"arxiv_id": "2401.14270", "title": "Viscoelasticty with physics-augmented neural networks: Model formulation\n  and training methods without prescribed internal variables", "abstract": "We present an approach for the data-driven modeling of nonlinear viscoelastic\nmaterials at small strains which is based on physics-augmented neural networks\n(NNs) and requires only stress and strain paths for training. The model is\nbuilt on the concept of generalized standard materials and is therefore\nthermodynamically consistent by construction. It consists of a free energy and\na dissipation potential, which can be either expressed by the components of\ntheir tensor arguments or by a suitable set of invariants. The two potentials\nare described by fully/partially input convex neural networks. For training of\nthe NN model by paths of stress and strain, an efficient and flexible training\nmethod based on a recurrent cell, particularly a long short-term memory cell,\nis developed to automatically generate the internal variable(s) during the\ntraining process. The proposed method is benchmarked and thoroughly compared\nwith existing approaches. These include a method that obtains the internal\nvariable by integrating the evolution equation over the entire sequence, while\nthe other method uses an an auxiliary feedforward neural network for the\ninternal variable(s). Databases for training are generated by using a\nconventional nonlinear viscoelastic reference model, where 3D and 2D plane\nstrain data with either ideal or noisy stresses are generated. The\ncoordinate-based and the invariant-based formulation are compared and the\nadvantages of the latter are demonstrated. Afterwards, the invariant-based\nmodel is calibrated by applying the three training methods using ideal or noisy\nstress data. All methods yield good results, but differ in computation time and\nusability for large data sets. The presented training method based on a\nrecurrent cell turns out to be particularly robust and widely applicable and\nthus represents a promising approach for the calibration of other types of\nmodels as well.", "field": "Computer Science", "categories": "cs.CE"}, {"arxiv_id": "2401.14272", "title": "libcdict: fast dictionaries in C", "abstract": "A common requirement in science is to store and share large sets of\nsimulation data in an efficient, nested, flexible and human-readable way. Such\ndatasets contain number counts and distributions, i.e. histograms and maps, of\narbitrary dimension and variable type, e.g. floating-point number, integer or\ncharacter string. Modern high-level programming languages like Perl and Python\nhave associated arrays, knowns as dictionaries or hashes, respectively, to\nfulfil this storage need. Low-level languages used more commonly for fast\ncomputational simulations, such as C and Fortran, lack this functionality. We\npresent libcdict, a C dictionary library, to solve this problem. Libcdict\nprovides C and Fortran application programming interfaces (APIs) to native\ndictionaries, called cdicts, and functions for cdicts to load and save these as\nJSON and hence for easy interpretation in other software and languages like\nPerl, Python and R.", "field": "Computer Science", "categories": "cs.DS,astro-ph.GA,astro-ph.HE,astro-ph.IM,astro-ph.SR"}, {"arxiv_id": "2401.14276", "title": "Optimization-based motion primitive automata for autonomous driving", "abstract": "Trajectory planning for autonomous cars can be addressed by primitive-based\nmethods, which encode nonlinear dynamical system behavior into automata. In\nthis paper, we focus on optimal trajectory planning. Since, typically, multiple\ncriteria have to be taken into account, multiobjective optimization problems\nhave to be solved. For the resulting Pareto-optimal motion primitives, we\nintroduce a universal automaton, which can be reduced or reconfigured according\nto prioritized criteria during planning. We evaluate a corresponding\nmulti-vehicle planning scenario with both simulations and laboratory\nexperiments.", "field": "Computer Science", "categories": "eess.SY,cs.RO,cs.SY"}, {"arxiv_id": "2401.14277", "title": "An Instance-Based Approach to the Trace Reconstruction Problem", "abstract": "In the trace reconstruction problem, one observes the output of passing a\nbinary string $s \\in \\{0,1\\}^n$ through a deletion channel $T$ times and wishes\nto recover $s$ from the resulting $T$ \"traces.\" Most of the literature has\nfocused on characterizing the hardness of this problem in terms of the number\nof traces $T$ needed for perfect reconstruction either in the worst case or in\nthe average case (over input sequences $s$). In this paper, we propose an\nalternative, instance-based approach to the problem. We define the \"Levenshtein\ndifficulty\" of a problem instance $(s,T)$ as the probability that the resulting\ntraces do not provide enough information for correct recovery with full\ncertainty. One can then try to characterize, for a specific $s$, how $T$ needs\nto scale in order for the Levenshtein difficulty to go to zero, and seek\nreconstruction algorithms that match this scaling for each $s$. For a class of\nbinary strings with alternating long runs, we precisely characterize the\nscaling of $T$ for which the Levenshtein difficulty goes to zero. For this\nclass, we also prove that a simple \"Las Vegas algorithm\" has an error\nprobability that decays to zero with the same rate as that with which the\nLevenshtein difficulty tends to zero.", "field": "Computer Science", "categories": "cs.IT,cs.DS,math.IT,math.PR,math.ST,stat.TH"}, {"arxiv_id": "2401.14278", "title": "CHIRON: Accelerating Node Synchronization without Security Trade-offs in\n  Distributed Ledgers", "abstract": "Blockchain performance has historically faced challenges posed by the\nthroughput limitations of consensus algorithms. Recent breakthroughs in\nresearch have successfully alleviated these constraints by introducing a\nmodular architecture that decouples consensus from execution. The move toward\nindependent optimization of the consensus layer has shifted attention to the\nexecution layer.\n  While concurrent transaction execution is a promising solution for increasing\nthroughput, practical challenges persist. Its effectiveness varies based on the\nworkloads, and the associated increased hardware requirements raise concerns\nabout undesirable centralization. This increased requirement results in full\nnodes and stragglers synchronizing from signed checkpoints, decreasing the\ntrustless nature of blockchain systems.\n  In response to these challenges, this paper introduces Chiron, a system\ndesigned to extract execution hints for the acceleration of straggling and full\nnodes. Notably, Chiron achieves this without compromising the security of the\nsystem or introducing overhead on the critical path of consensus. Evaluation\nresults demonstrate a notable speedup of up to 30%, effectively addressing the\ngap between theoretical research and practical deployment. The quantification\nof this speedup is achieved through realistic blockchain benchmarks derived\nfrom a comprehensive analysis of Ethereum and Solana workloads, constituting an\nindependent contribution.", "field": "Computer Science", "categories": "cs.DC"}, {"arxiv_id": "2401.14279", "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using ChatGPT", "abstract": "Technical question and answering (Q&A) sites such as Stack Overflow have\nbecome an important source for software developers to seek knowledge. However,\ncode snippets on Q&A sites are usually uncompilable and semantically incomplete\nfor compilation due to unresolved types and missing dependent libraries, which\nraises the obstacle for users to reuse or analyze Q&A code snippets. Prior\napproaches either are not designed for synthesizing compilable code or suffer\nfrom a low compilation success rate. To address this problem, we propose ZS4C,\na lightweight approach to perform zero-shot synthesis of compilable code from\nincomplete code snippets using Large Language Model (LLM). ZS4C operates in two\nstages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify\nmissing import statements for a given code snippet, leveraging our designed\ntask-specific prompt template. In the second stage, ZS4C fixes compilation\nerrors caused by incorrect import statements and syntax errors through\ncollaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C\non a widely used benchmark called StatType-SO against the SOTA approach SnR.\nCompared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a\n39.3% improvement. On average, ZS4C can infer more accurate import statements\nthan SnR, with an improvement of 6.6% in the F1.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.14280", "title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large\n  Language Models models via Romanization", "abstract": "This study addresses the challenge of extending Large Language Models (LLMs)\nto non-English languages, specifically those using non-Latin scripts. We\npropose an innovative approach that utilizes the romanized form of text as an\ninterface for LLMs, hypothesizing that its frequent informal use and shared\ntokens with English enhance cross-lingual alignment. Focusing on Hindi, we\ndemonstrate through Hindi-to-English translation and sentiment analysis tasks\nthat romanized text not only significantly improves inference efficiency due to\nits lower fertility compared to native text but also achieves competitive\nperformance with limited pre-training. Additionally, our novel multi-script\nprompting approach, which combines romanized and native texts, shows promise in\nfurther enhancing task performance. These findings suggest the potential of\nromanization in bridging the language gap for LLM applications, with future\nwork aimed at expanding this approach to more languages and tasks.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.14284", "title": "Bridging Education and Development: IDEs as Interactive Learning\n  Platforms", "abstract": "In this work, we introduce a novel approach to programming education - in-IDE\ncourses implemented for IntelliJ-based IDEs via the JetBrains Academy Plugin.\nThe primary objective of this approach is to address the challenge of\nfamiliarizing students with industrial technologies by moving all theory and\npractical materials to a professional IDE. This approach allows students to\nimmediately use modern industrial tools as they are fully integrated into the\nlearning process. We have already applied this approach in over 40 courses, and\nit successfully educates students across diverse topics such as Plugin\nDevelopment, Algorithms, Data Analysis, and Language mastery in various\nprogramming languages, including Kotlin, Java, C++, and Python. Along with the\npaper, we are providing the community not only with a new way of learning and a\nset of ready-made courses but also a collection of helpful resources to assist\neducators in getting started with the plugin. Finally, we describe in detail an\nIDE plugin development course that demonstrates how the in-IDE approach covers\ncomplex topics easily.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.14285", "title": "POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for\n  Low-Count PET Attenuation Map Generation", "abstract": "Low-dose PET offers a valuable means of minimizing radiation exposure in PET\nimaging. However, the prevalent practice of employing additional CT scans for\ngenerating attenuation maps (u-map) for PET attenuation correction\nsignificantly elevates radiation doses. To address this concern and further\nmitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an\ninnovative population-prior-aided over-under-representation network that aims\nfor high-quality attenuation map generation from low-dose PET. First, POUR-Net\nincorporates an over-under-representation network (OUR-Net) to facilitate\nefficient feature extraction, encompassing both low-resolution abstracted and\nfine-detail features, for assisting deep generation on the full-resolution\nlevel. Second, complementing OUR-Net, a population prior generation machine\n(PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional\nprior information to aid OUR-Net generation. The integration of OUR-Net and\nPPGM within a cascade framework enables iterative refinement of $\\mu$-map\ngeneration, resulting in the production of high-quality $\\mu$-maps.\nExperimental results underscore the effectiveness of POUR-Net, showing it as a\npromising solution for accurate CT-free low-count PET attenuation correction,\nwhich also surpasses the performance of previous baseline methods.", "field": "Computer Science", "categories": "cs.CV,cs.AI,eess.IV"}, {"arxiv_id": "2401.14286", "title": "Equivalence of Applicative Functors and Multifunctors", "abstract": "McBride and Paterson introduced Applicative functors to Haskell, which are\nequivalent to the lax monoidal functors (with strength) of category theory.\nApplicative functors F are presented via idiomatic application $\\_\\circledast\\_\n: F (A \\to B) \\to F A \\to F B$ and laws that are a bit hard to remember.\nCapriotti and Kaposi observed that applicative functors can be conceived as\nmultifunctors, i.e., by a family liftA$_n$ : $(A_1 \\to ... \\to A_n \\to C) \\to F\nA_1 \\to ... \\to F A_n \\to F C$ of zipWith-like functions that generalize pure\n$(n=0)$, fmap $(n=1)$ and liftA2 $(n=2)$. This reduces the associated laws to\njust the first functor law and a uniform scheme of second (multi)functor laws,\ni.e., a composition law for liftA. In this note, we rigorously prove that\napplicative functors are in fact equivalent to multifunctors, by interderiving\ntheir laws.", "field": "Computer Science", "categories": "cs.PL,cs.LO,68N18,D.1.1"}, {"arxiv_id": "2401.14289", "title": "Speech foundation models on intelligibility prediction for\n  hearing-impaired listeners", "abstract": "Speech foundation models (SFMs) have been benchmarked on many speech\nprocessing tasks, often achieving state-of-the-art performance with minimal\nadaptation. However, the SFM paradigm has been significantly less explored for\napplications of interest to the speech perception community. In this paper we\npresent a systematic evaluation of 10 SFMs on one such application: Speech\nintelligibility prediction. We focus on the non-intrusive setup of the Clarity\nPrediction Challenge 2 (CPC2), where the task is to predict the percentage of\nwords correctly perceived by hearing-impaired listeners from speech-in-noise\nrecordings. We propose a simple method that learns a lightweight specialized\nprediction head on top of frozen SFMs to approach the problem. Our results\nreveal statistically significant differences in performance across SFMs. Our\nmethod resulted in the winning submission in the CPC2, demonstrating its\npromise for speech perception applications.", "field": "Computer Science", "categories": "cs.SD,cs.LG,eess.AS"}, {"arxiv_id": "2401.14292", "title": "AST-2: Single and bi-layered 2-D acoustic soft tactile skin", "abstract": "This paper aims to present an innovative and cost-effective design for\nAcoustic Soft Tactile (AST) Skin, with the primary goal of significantly\nenhancing the accuracy of 2-D tactile feature estimation. The existing\nchallenge lies in achieving precise tactile feature estimation, especially\nconcerning contact geometry characteristics, using cost-effective solutions. We\nhypothesise that by harnessing acoustic energy through dedicated acoustic\nchannels in 2 layers beneath the sensing surface and analysing amplitude\nmodulation, we can effectively decode interactions on the sensory surface,\nthereby improving tactile feature estimation. Our approach involves the\ndistinct separation of hardware components responsible for emitting and\nreceiving acoustic signals, resulting in a modular and highly customizable skin\ndesign. Practical tests demonstrate the effectiveness of this novel design,\nachieving remarkable precision in estimating contact normal forces (MAE < 0.8\nN), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE <\n0.3 mm). In conclusion, the AST skin, with its innovative design and modular\narchitecture, successfully addresses the challenge of tactile feature\nestimation. The presented results showcase its ability to precisely estimate\nvarious tactile features, making it a practical and cost-effective solution for\nrobotic applications.", "field": "Computer Science", "categories": "cs.RO,cs.AI"}, {"arxiv_id": "2401.14295", "title": "Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of\n  Thoughts", "abstract": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and others parts of the LLM ecosystem such as knowledge bases, and\nthe associated research challenges. Our work will help to advance future prompt\nengineering techniques.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.14296", "title": "\"All of Me\": Mining Users' Attributes from their Public Spotify\n  Playlists", "abstract": "In the age of digital music streaming, playlists on platforms like Spotify\nhave become an integral part of individuals' musical experiences. People create\nand publicly share their own playlists to express their musical tastes, promote\nthe discovery of their favorite artists, and foster social connections. These\npublicly accessible playlists transcend the boundaries of mere musical\npreferences: they serve as sources of rich insights into users' attributes and\nidentities. For example, the musical preferences of elderly individuals may\nlean more towards Frank Sinatra, while Billie Eilish remains a favored choice\namong teenagers. These playlists thus become windows into the diverse and\nevolving facets of one's musical identity.\n  In this work, we investigate the relationship between Spotify users'\nattributes and their public playlists. In particular, we focus on identifying\nrecurring musical characteristics associated with users' individual attributes,\nsuch as demographics, habits, or personality traits. To this end, we conducted\nan online survey involving 739 Spotify users, yielding a dataset of 10,286\npublicly shared playlists encompassing over 200,000 unique songs and 55,000\nartists. Through extensive statistical analyses, we first assess a deep\nconnection between a user's Spotify playlists and their real-life attributes.\nFor instance, we found individuals high in openness often create playlists\nfeaturing a diverse array of artists, while female users prefer Pop and K-pop\nmusic genres. Building upon these observed associations, we create accurate\npredictive models for users' attributes, presenting a novel DeepSet application\nthat outperforms baselines in most of these users' attributes.", "field": "Computer Science", "categories": "cs.CR,cs.LG,cs.SI"}, {"arxiv_id": "2401.14297", "title": "PWM strategy with harmonics injection and modulated frequency triangular\n  carrier. A review", "abstract": "A new, programmed pulse width modulation (PWM) technique to control power\ninverters, which uses a harmonic injection modulator and a frequency modulated\ntriangular carrier, synchronized with the modulating signal is presented in\nthis paper. The instantaneous carrier frequency is adjusted according to a\nperiodic function synchronized with the fundamental term of the modulating\nsignal, in order to maintain the average value of the instantaneous frequency\nas an odd positive integer multiple of 3, for each period of the modulating\nsignal which is known as the average modulation order. The advantages of using\nthe proposed technique over the conventional PWM techniques are the reduction\nin the total harmonic distortion and shift the frequency up of the temporal\nharmonics for any average modulation order. The experimental results show the\nviability of optimizing the time harmonics generated to minimize the vibrations\nin an induction motor or avoid the resonant frequencies.The mathematical\nformulation for the output modulated voltage is defined and the results are\nalso checked experimentally and compared to a sinusoidal PWM technique", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.14303", "title": "On Some Complexity Results for Even Linear Languages", "abstract": "We deal with a normal form for context-free grammars, called Dyck normal\nform. This normal form is a syntactical restriction of the Chomsky normal form,\nin which the two nonterminals occurring on the right-hand side of a rule are\npaired nonterminals. This pairwise property, along with several other terminal\nrewriting conditions, makes it possible to define a homomorphism from Dyck\nwords to words generated by a grammar in Dyck normal form. We prove that for\neach context-free language L, there exist an integer K and a homomorphism phi\nsuch that L=phi(D'_K), where D'_K is a subset of D_K and D_K is the one-sided\nDyck language over K letters. As an application we give an alternative proof of\nthe inclusion of the class of even linear languages in AC1.", "field": "Computer Science", "categories": "cs.FL,cs.CC,cs.LO,03D10, 03D15,,F.1.1; F.4.1; F.4.3"}, {"arxiv_id": "2401.14304", "title": "Constraint-Aware Mesh Refinement Method by Reachability Set Envelope of\n  Curvature Bounded Paths", "abstract": "This paper presents an enhanced direct-method-based approach for the\nreal-time solution of optimal control problems to handle path constraints, such\nas obstacles. The principal contributions of this work are twofold: first, the\nexisting methods for constructing reachability sets in the literature are\nextended to derive the envelope of these sets, which determines the region\nswept by all feasible trajectories between adjacent sample points. Second, we\npropose a novel method to guarantee constraint violation-free between discrete\nstates in two dimensions through mesh refinement approach. To illustrate the\neffectiveness of the proposed methodology, numerical simulations are conducted\non real-time path planning for fixed-wing unmanned aerial vehicles.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.14310", "title": "A high-order discontinuous Galerkin method for the numerical modeling of\n  epileptic seizures", "abstract": "Epilepsy is a clinical neurological disorder characterized by recurrent and\nspontaneous seizures consisting of abnormal high-frequency electrical activity\nin the brain. In this condition, the transmembrane potential dynamics are\ncharacterized by rapid and sharp wavefronts traveling along the heterogeneous\nand anisotropic conduction pathways of the brain. This work employs the\nmonodomain model, coupled with specific neuronal ionic models characterizing\nion concentration dynamics, to mathematically describe brain tissue\nelectrophysiology in grey and white matter at the organ scale. This multiscale\nmodel is discretized in space with the high-order discontinuous Galerkin method\non polygonal and polyhedral grids (PolyDG) and advanced in time with a\nCrank-Nicolson scheme. This ensures, on the one hand, efficient and accurate\nsimulations of the high-frequency electrical activity that is responsible for\nepileptic seizure and, on the other hand, keeps reasonably low the\ncomputational costs by a suitable combination of high-order approximations and\nagglomerated polytopal meshes. We numerically investigate synthetic test cases\non a two-dimensional heterogeneous squared domain discretized with a polygonal\ngrid, and on a two-dimensional brainstem in a sagittal plane with an\nagglomerated polygonal grid that takes full advantage of the flexibility of the\nPolyDG approximation of the semidiscrete formulation. Finally, we provide a\ntheoretical analysis of stability and an a-priori convergence analysis for a\nsimplified mathematical problem.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.14314", "title": "MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor\n  Fusion Perception Systems", "abstract": "Multi-sensor fusion stands as a pivotal technique in addressing numerous\nsafety-critical tasks and applications, e.g., self-driving cars and automated\nrobotic arms. With the continuous advancement in data-driven artificial\nintelligence (AI), MSF's potential for sensing and understanding intricate\nexternal environments has been further amplified, bringing a profound impact on\nintelligent systems and specifically on their perception systems. Similar to\ntraditional software, adequate testing is also required for AI-enabled MSF\nsystems. Yet, existing testing methods primarily concentrate on single-sensor\nperception systems (e.g., image-/point cloud-based object detection systems).\nThere remains a lack of emphasis on generating multi-modal test cases for MSF\nsystems. To address these limitations, we design and implement MultiTest, a\nfitness-guided metamorphic testing method for complex MSF perception systems.\nMultiTest employs a physical-aware approach to synthesize realistic multi-modal\nobject instances and insert them into critical positions of background images\nand point clouds. A fitness metric is designed to guide and boost the test\ngeneration process. We conduct extensive experiments with five SOTA perception\nsystems to evaluate MultiTest from the perspectives of: (1) generated test\ncases' realism, (2) fault detection capabilities, and (3) performance\nimprovement. The results show that MultiTest can generate realistic and\nmodality-consistent test data and effectively detect hundreds of diverse faults\nof an MSF system under test. Moreover, retraining an MSF system on the test\ncases generated by MultiTest can improve the system's robustness.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.14317", "title": "Maximizing the Minimum Eigenvalue in Constant Dimension", "abstract": "In an instance of the minimum eigenvalue problem, we are given a collection\nof $n$ vectors $v_1,\\ldots, v_n \\subset {\\mathbb{R}^d}$, and the goal is to\npick a subset $B\\subseteq [n]$ of given vectors to maximize the minimum\neigenvalue of the matrix $\\sum_{i\\in B} v_i v_i^{\\top} $. Often, additional\ncombinatorial constraints such as cardinality constraint $\\left(|B|\\leq\nk\\right)$ or matroid constraint ($B$ is a basis of a matroid defined on $[n]$)\nmust be satisfied by the chosen set of vectors. The minimum eigenvalue problem\nwith matroid constraints models a wide variety of problems including the Santa\nClause problem, the E-design problem, and the constructive Kadison-Singer\nproblem.\n  In this paper, we give a randomized algorithm that finds a set $B\\subseteq\n[n]$ subject to any matroid constraint whose minimum eigenvalue is at least\n$(1-\\epsilon)$ times the optimum, with high probability. The running time of\nthe algorithm is $O\\left( n^{O(d\\log(d)/\\epsilon^2)}\\right)$. In particular,\nour results give a polynomial time asymptotic scheme when the dimension of the\nvectors is constant. Our algorithm uses a convex programming relaxation of the\nproblem after guessing a rescaling which allows us to apply pipage rounding and\nmatrix Chernoff inequalities to round to a good solution. The key new component\nis a structural lemma which enables us to \"guess'' the appropriate rescaling,\nwhich could be of independent interest. Our approach generalizes the\napproximation guarantee to monotone, homogeneous functions and as such we can\nmaximize $\\det(\\sum_{i\\in B} v_i v_i^\\top)^{1/d}$, or minimize any norm of the\neigenvalues of the matrix $\\left(\\sum_{i\\in B} v_i v_i^\\top\\right)^{-1} $, with\nthe same running time under some mild assumptions. As a byproduct, we also get\na simple algorithm for an algorithmic version of Kadison-Singer problem.", "field": "Computer Science", "categories": "cs.DS"}, {"arxiv_id": "2401.14319", "title": "A Quantum \"Lifting Theorem\" for Constructions of Pseudorandom Generators\n  from Random Oracles", "abstract": "We study the (quantum) security of pseudorandom generators (PRGs) constructed\nfrom random oracles. We prove a ``lifting theorem'' showing, roughly, that if\nsuch a PRG is unconditionally secure against classical adversaries making\npolynomially many queries to the random oracle, then it is also\n(unconditionally) secure against quantum adversaries in the same sense. As a\nresult of independent interest, we also show that any pseudo-deterministic\nquantum-oracle algorithm (i.e., a quantum algorithm that with high probability\nreturns the same value on repeated executions) can be simulated by a\ncomputationally unbounded but query bounded classical-oracle algorithm with\nonly a polynomial blowup in the number of queries. This implies as a corollary\nthat our lifting theorem holds even for PRGs that themselves make quantum\nqueries to the random oracle.", "field": "Computer Science", "categories": "cs.CR"}, {"arxiv_id": "2401.14320", "title": "Quantifying Software Correctness by Combining Architecture Modeling and\n  Formal Program Analysis", "abstract": "Most formal methods see the correctness of a software system as a binary\ndecision. However, proving the correctness of complex systems completely is\ndifficult because they are composed of multiple components, usage scenarios,\nand environments. We present QuAC, a modular approach for quantifying the\ncorrectness of service-oriented software systems by combining software\narchitecture modeling with deductive verification. Our approach is based on a\nmodel of the service-oriented architecture and the probabilistic usage\nscenarios of the system. The correctness of a single service is approximated by\na coverage region, which is a formula describing which inputs for that service\nare proven to not lead to an erroneous execution. The coverage regions can be\ndetermined by a combination of various analyses, e.g., formal verification,\nexpert estimations, or testing. The coverage regions and the software model are\nthen combined into a probabilistic program. From this, we can compute the\nprobability that under a given usage profile no service is called outside its\ncoverage region. If the coverage region is large enough, then instead of\nattempting to get 100% coverage, which may be prohibitively expensive, run-time\nverification or testing approaches may be used to deal with inputs outside the\ncoverage region. We also present an implementation of QuAC for Java using the\nmodeling tool Palladio and the deductive verification tool KeY. We demonstrate\nits usability by applying it to a software simulation of an energy system.", "field": "Computer Science", "categories": "cs.SE,cs.LO"}, {"arxiv_id": "2401.14322", "title": "Generalized People Diversity: Learning a Human Perception-Aligned\n  Diversity Representation for People Images", "abstract": "Capturing the diversity of people in images is challenging: recent literature\ntends to focus on diversifying one or two attributes, requiring expensive\nattribute labels or building classifiers. We introduce a diverse people image\nranking method which more flexibly aligns with human notions of people\ndiversity in a less prescriptive, label-free manner. The Perception-Aligned\nText-derived Human representation Space (PATHS) aims to capture all or many\nrelevant features of people-related diversity, and, when used as the\nrepresentation space in the standard Maximal Marginal Relevance (MMR) ranking\nalgorithm, is better able to surface a range of types of people-related\ndiversity (e.g. disability, cultural attire). PATHS is created in two stages.\nFirst, a text-guided approach is used to extract a person-diversity\nrepresentation from a pre-trained image-text model. Then this representation is\nfine-tuned on perception judgments from human annotators so that it captures\nthe aspects of people-related similarity that humans find most salient.\nEmpirical results show that the PATHS method achieves diversity better than\nbaseline methods, according to side-by-side ratings from human annotators.", "field": "Computer Science", "categories": "cs.CV,cs.CY"}, {"arxiv_id": "2401.14323", "title": "Common Randomness Generation from Finite Compound Sources", "abstract": "We investigate the problem of generating common randomness (CR) from finite\ncompound sources aided by unidirectional communication over rate-limited\nperfect channels. The two communicating parties, often referred to as\nterminals, observe independent and identically distributed (i.i.d.) samples of\na finite compound source and aim to agree on a common random variable with a\nhigh probability for every possible realization of the source state. Both\nparties know the set of source states as well as their statistics. However,\nthey are unaware of the actual realization of the source state. We establish a\nsingle-letter lower and upper bound on the compound CR capacity for the\nspecified model. Furthermore, we present two special scenarios where the\nestablished bounds coincide.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.14324", "title": "Scalable Tree-based Register Automata Learning", "abstract": "Existing active automata learning (AAL) algorithms have demonstrated their\npotential in capturing the behavior of complex systems (e.g., in analyzing\nnetwork protocol implementations). The most widely used AAL algorithms generate\nfinite state machine models, such as Mealy machines. For many analysis tasks,\nhowever, it is crucial to generate richer classes of models that also show how\nrelations between data parameters affect system behavior. Such models have\nshown potential to uncover critical bugs, but their learning algorithms do not\nscale beyond small and well curated experiments. In this paper, we present\n$SL^\\lambda$, an effective and scalable register automata (RA) learning\nalgorithm that significantly reduces the number of tests required for inferring\nmodels. It achieves this by combining a tree-based cost-efficient data\nstructure with mechanisms for computing short and restricted tests. We have\nimplemented $SL^\\lambda$ as a new algorithm in RALib. We evaluate its\nperformance by comparing it against $SL^*$, the current state-of-the-art RA\nlearning algorithm, in a series of experiments, and show superior performance\nand substantial asymptotic improvements in bigger systems.", "field": "Computer Science", "categories": "cs.FL"}, {"arxiv_id": "2401.14325", "title": "Unlocking Past Information: Temporal Embeddings in Cooperative Bird's\n  Eye View Prediction", "abstract": "Accurate and comprehensive semantic segmentation of Bird's Eye View (BEV) is\nessential for ensuring safe and proactive navigation in autonomous driving.\nAlthough cooperative perception has exceeded the detection capabilities of\nsingle-agent systems, prevalent camera-based algorithms in cooperative\nperception neglect valuable information derived from historical observations.\nThis limitation becomes critical during sensor failures or communication issues\nas cooperative perception reverts to single-agent perception, leading to\ndegraded performance and incomplete BEV segmentation maps. This paper\nintroduces TempCoBEV, a temporal module designed to incorporate historical cues\ninto current observations, thereby improving the quality and reliability of BEV\nmap segmentations. We propose an importance-guided attention architecture to\neffectively integrate temporal information that prioritizes relevant properties\nfor BEV map segmentation. TempCoBEV is an independent temporal module that\nseamlessly integrates into state-of-the-art camera-based cooperative perception\nmodels. We demonstrate through extensive experiments on the OPV2V dataset that\nTempCoBEV performs better than non-temporal models in predicting current and\nfuture BEV map segmentations, particularly in scenarios involving communication\nfailures. We show the efficacy of TempCoBEV and its capability to integrate\nhistorical cues into the current BEV map, improving predictions under optimal\ncommunication conditions by up to 2% and under communication failures by up to\n19%. The code will be published on GitHub.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14332", "title": "SunBlock: Cloudless Protection for IoT Systems", "abstract": "With an increasing number of Internet of Things (IoT) devices present in\nhomes, there is a rise in the number of potential information leakage channels\nand their associated security threats and privacy risks. Despite a long history\nof attacks on IoT devices in unprotected home networks, the problem of\naccurate, rapid detection and prevention of such attacks remains open. Many\nexisting IoT protection solutions are cloud-based, sometimes ineffective, and\nmight share consumer data with unknown third parties. This paper investigates\nthe potential for effective IoT threat detection locally, on a home router,\nusing AI tools combined with classic rule-based traffic-filtering algorithms.\nOur results show that with a slight rise of router hardware resources caused by\nmachine learning and traffic filtering logic, a typical home router\ninstrumented with our solution is able to effectively detect risks and protect\na typical home IoT network, equaling or outperforming existing popular\nsolutions, without any effects on benign IoT functionality, and without relying\non cloud services and third parties.", "field": "Computer Science", "categories": "cs.CR,cs.LG"}, {"arxiv_id": "2401.14336", "title": "Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for\n  Fine-grained Vehicle Recognition", "abstract": "Fine-grained vehicle recognition (FGVR) is an essential fundamental\ntechnology for intelligent transportation systems, but very difficult because\nof its inherent intra-class variation. Most previous FGVR studies only focus on\nthe intra-class variation caused by different shooting angles, positions, etc.,\nwhile the intra-class variation caused by image noise has received little\nattention. This paper proposes a progressive multi-task anti-noise learning\n(PMAL) framework and a progressive multi-task distilling (PMD) framework to\nsolve the intra-class variation problem in FGVR due to image noise. The PMAL\nframework achieves high recognition accuracy by treating image denoising as an\nadditional task in image recognition and progressively forcing a model to learn\nnoise invariance. The PMD framework transfers the knowledge of the PMAL-trained\nmodel into the original backbone network, which produces a model with about the\nsame recognition accuracy as the PMAL-trained model, but without any additional\noverheads over the original backbone network. Combining the two frameworks, we\nobtain models that significantly exceed previous state-of-the-art methods in\nrecognition accuracy on two widely-used, standard FGVR datasets, namely\nStanford Cars, and CompCars, as well as three additional surveillance\nimage-based vehicle-type classification datasets, namely Beijing Institute of\nTechnology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images\nDataset for Make Model Recognition (VIDMMR), without any additional overheads\nover the original backbone networks. The source code is available at\nhttps://github.com/Dichao-Liu/Anti-noise_FGVR", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.14341", "title": "Efficient Construction of Long Orientable Sequences", "abstract": "An orientable sequence of order $n$ is a cyclic binary sequence such that\neach length-$n$ substring appears at most once \\emph{in either direction}.\nMaximal length orientable sequences are known only for $n\\leq 7$, and a trivial\nupper bound on their length is $2^{n-1} - 2^{\\lfloor(n-1)/2\\rfloor}$. This\npaper presents the first efficient algorithm to construct orientable sequences\nwith asymptotically optimal length; more specifically, our algorithm constructs\norientable sequences via cycle-joining and a successor-rule approach requiring\n$O(n)$ time per symbol and $O(n)$ space. This answers a longstanding open\nquestion from Dai, Martin, Robshaw, Wild [Cryptography and Coding III (1993)].\nOur sequences are applied to find new longest-known orientable sequences for\n$n\\leq 20$.", "field": "Computer Science", "categories": "cs.DS,cs.DM,cs.IT,math.CO,math.IT"}, {"arxiv_id": "2401.14343", "title": "Class-attribute Priors: Adapting Optimization to Heterogeneity and\n  Fairness Objective", "abstract": "Modern classification problems exhibit heterogeneities across individual\nclasses: Each class may have unique attributes, such as sample size, label\nquality, or predictability (easy vs difficult), and variable importance at\ntest-time. Without care, these heterogeneities impede the learning process,\nmost notably, when optimizing fairness objectives. Confirming this, under a\ngaussian mixture setting, we show that the optimal SVM classifier for balanced\naccuracy needs to be adaptive to the class attributes. This motivates us to\npropose CAP: An effective and general method that generates a class-specific\nlearning strategy (e.g. hyperparameter) based on the attributes of that class.\nThis way, optimization process better adapts to heterogeneities. CAP leads to\nsubstantial improvements over the naive approach of assigning separate\nhyperparameters to each class. We instantiate CAP for loss function design and\npost-hoc logit adjustment, with emphasis on label-imbalanced problems. We show\nthat CAP is competitive with prior art and its flexibility unlocks clear\nbenefits for fairness objectives beyond balanced accuracy. Finally, we evaluate\nCAP on problems with label noise as well as weighted test objectives to\nshowcase how CAP can jointly adapt to different heterogeneities.", "field": "Computer Science", "categories": "cs.LG,cs.CY,stat.ML"}, {"arxiv_id": "2401.14347", "title": "Evolving higher-order synergies reveals a trade-off between stability\n  and information integration capacity in complex systems", "abstract": "There has recently been an explosion of interest in how \"higher-order\"\nstructures emerge in complex systems. This \"emergent\" organization has been\nfound in a variety of natural and artificial systems, although at present the\nfield lacks a unified understanding of what the consequences of higher-order\nsynergies and redundancies are for systems. Typical research treat the presence\n(or absence) of synergistic information as a dependent variable and report\nchanges in the level of synergy in response to some change in the system. Here,\nwe attempt to flip the script: rather than treating higher-order information as\na dependent variable, we use evolutionary optimization to evolve boolean\nnetworks with significant higher-order redundancies, synergies, or statistical\ncomplexity. We then analyse these evolved populations of networks using\nestablished tools for characterizing discrete dynamics: the number of\nattractors, average transient length, and Derrida coefficient. We also assess\nthe capacity of the systems to integrate information. We find that high-synergy\nsystems are unstable and chaotic, but with a high capacity to integrate\ninformation. In contrast, evolved redundant systems are extremely stable, but\nhave negligible capacity to integrate information. Finally, the complex systems\nthat balance integration and segregation (known as Tononi-Sporns-Edelman\ncomplexity) show features of both chaosticity and stability, with a greater\ncapacity to integrate information than the redundant systems while being more\nstable than the random and synergistic systems. We conclude that there may be a\nfundamental trade-off between the robustness of a systems dynamics and its\ncapacity to integrate information (which inherently requires flexibility and\nsensitivity), and that certain kinds of complexity naturally balance this\ntrade-off.", "field": "Computer Science", "categories": "cs.IT,math.DS,math.IT,nlin.CD,nlin.CG"}, {"arxiv_id": "2401.14349", "title": "Learning to navigate efficiently and precisely in real environments", "abstract": "In the context of autonomous navigation of terrestrial robots, the creation\nof realistic models for agent dynamics and sensing is a widespread habit in the\nrobotics literature and in commercial applications, where they are used for\nmodel based control and/or for localization and mapping. The more recent\nEmbodied AI literature, on the other hand, focuses on modular or end-to-end\nagents trained in simulators like Habitat or AI-Thor, where the emphasis is put\non photo-realistic rendering and scene diversity, but high-fidelity robot\nmotion is assigned a less privileged role. The resulting sim2real gap\nsignificantly impacts transfer of the trained models to real robotic platforms.\nIn this work we explore end-to-end training of agents in simulation in settings\nwhich minimize the sim2real gap both, in sensing and in actuation. Our agent\ndirectly predicts (discretized) velocity commands, which are maintained through\nclosed-loop control in the real robot. The behavior of the real robot\n(including the underlying low-level controller) is identified and simulated in\na modified Habitat simulator. Noise models for odometry and localization\nfurther contribute in lowering the sim2real gap. We evaluate on real navigation\nscenarios, explore different localization and point goal calculation methods\nand report significant gains in performance and robustness compared to prior\nwork.", "field": "Computer Science", "categories": "cs.RO,cs.CV"}, {"arxiv_id": "2401.14350", "title": "5G Network Security Practices: An Overview and Survey", "abstract": "This document provides an overview of 5G network security, describing various\ncomponents of the 5G core network architecture and what kind of security\nservices are offered by these 5G components. It also explores the potential\nsecurity risks and vulnerabilities presented by the security architecture in 5G\nand recommends some of the best practices for the 5G network admins to consider\nwhile deploying a secure 5G network, based on the surveyed documents from the\nEuropean government's efforts in commercializing the IoT devices and securing\nsupply chain over 5G networks.", "field": "Computer Science", "categories": "cs.NI,cs.CR"}, {"arxiv_id": "2401.14351", "title": "ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language\n  Models", "abstract": "This paper presents ServerlessLLM, a locality-enhanced serverless inference\nsystem for Large Language Models (LLMs). ServerlessLLM exploits the substantial\ncapacity and bandwidth of storage and memory devices available on GPU servers,\nthereby reducing costly remote checkpoint downloads and achieving efficient\ncheckpoint loading. ServerlessLLM achieves this through three main\ncontributions: (i) fast LLM checkpoint loading via a novel loading-optimized\ncheckpoint format design, coupled with an efficient multi-tier checkpoint\nloading system; (ii) locality-driven LLM inference with live migration, which\nallows ServerlessLLM to effectively achieve locality-driven server allocation\nwhile preserving the low latency of ongoing LLM inference; and (iii)\nlocality-aware server allocation, enabling ServerlessLLM to evaluate the status\nof each server in a cluster and effectively schedule model startup time to\ncapitalize on local checkpoint placement. Our comprehensive experiments, which\ninclude microbenchmarks and real-world traces, show that ServerlessLLM\nsurpasses state-of-the-art systems by 10 - 200X in latency performance when\nrunning various LLM inference workloads.", "field": "Computer Science", "categories": "cs.LG,cs.DC"}, {"arxiv_id": "2401.14352", "title": "Skyline-based exploration of temporal property graphs", "abstract": "In this paper, we focus on temporal property graphs, that is, property graphs\nwhose labeled nodes and edges as well as the values of the properties\nassociated with them may change with time. For instance, consider a\nbibliographic network, with nodes representing authors and conferences with\nproperties such as gender and location respectively, and edges representing\ncollaboration between authors and publications in conferences. A key challenge\nin studying temporal graphs lies in detecting interesting events in their\nevolution, defined as time intervals of significant stability, growth, or\nshrinkage. To address this challenge, we build aggregated graphs, where nodes\nare grouped based on the values of their properties, and seek events at the\naggregated level, for example, time intervals of significant growth in the\ncollaborations between authors of the same gender. To locate such events, we\npropose a novel approach based on unified evolution skylines. A unified\nevolution skyline assesses the significance of an event in conjunction with the\nduration of the interval in which the event occurs. Significance is measured by\na set of counts, where each count refers to the number of graph elements that\nremain stable, are created, or deleted, for a specific property value. For\nexample, for property gender, we measure the number of female-female,\nfemale-male, and male-male collaborations. Lastly, we share experimental\nfindings that highlight the efficiency and effectiveness of our approach.", "field": "Computer Science", "categories": "cs.SI"}, {"arxiv_id": "2401.14354", "title": "Learning Robust Generalizable Radiance Field with Visibility and Feature\n  Augmented Point Representation", "abstract": "This paper introduces a novel paradigm for the generalizable neural radiance\nfield (NeRF). Previous generic NeRF methods combine multiview stereo techniques\nwith image-based neural rendering for generalization, yielding impressive\nresults, while suffering from three issues. First, occlusions often result in\ninconsistent feature matching. Then, they deliver distortions and artifacts in\ngeometric discontinuities and locally sharp shapes due to their individual\nprocess of sampled points and rough feature aggregation. Third, their\nimage-based representations experience severe degradations when source views\nare not near enough to the target view. To address challenges, we propose the\nfirst paradigm that constructs the generalizable neural field based on\npoint-based rather than image-based rendering, which we call the Generalizable\nneural Point Field (GPF). Our approach explicitly models visibilities by\ngeometric priors and augments them with neural features. We propose a novel\nnonuniform log sampling strategy to improve both rendering speed and\nreconstruction quality. Moreover, we present a learnable kernel spatially\naugmented with features for feature aggregations, mitigating distortions at\nplaces with drastically varying geometries. Besides, our representation can be\neasily manipulated. Experiments show that our model can deliver better\ngeometries, view consistencies, and rendering quality than all counterparts and\nbenchmarks on three datasets in both generalization and finetuning settings,\npreliminarily proving the potential of the new paradigm for generalizable NeRF.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14360", "title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bengali Texts", "abstract": "While Bengali is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bengali texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bengali texts. At first, given an input noisy text, we\nidentify the noise type, addressing this as a multi-label classification task.\nThen, we introduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts", "field": "Computer Science", "categories": "cs.CL,68T50 (Primary),I.2.7"}, {"arxiv_id": "2401.14361", "title": "MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE\n  Serving", "abstract": "This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE)\nserving system that realizes activation-aware expert offloading. MoE-Infinity\nfeatures sequence-level expert activation tracing, a new approach adept at\nidentifying sparse activations and capturing the temporal locality of MoE\ninference. By analyzing these traces, MoE-Infinity performs novel\nactivation-aware expert prefetching and caching, substantially reducing the\nlatency overheads usually associated with offloading experts for improved cost\nperformance. Extensive experiments in a cluster show that MoE-Infinity\noutperforms numerous existing systems and approaches, reducing latency by 4 -\n20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's\nsource code is publicly available at https://github.com/TorchMoE/MoE-Infinity", "field": "Computer Science", "categories": "cs.LG,cs.PF"}, {"arxiv_id": "2401.14362", "title": "The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support", "abstract": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.", "field": "Computer Science", "categories": "cs.HC,cs.AI,cs.CY"}, {"arxiv_id": "2401.14367", "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation", "abstract": "The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.14371", "title": "Efficient Optimisation of Physical Reservoir Computers using only a\n  Delayed Input", "abstract": "We present an experimental validation of a recently proposed optimization\ntechnique for reservoir computing, using an optoelectronic setup. Reservoir\ncomputing is a robust framework for signal processing applications, and the\ndevelopment of efficient optimization approaches remains a key challenge. The\ntechnique we address leverages solely a delayed version of the input signal to\nidentify the optimal operational region of the reservoir, simplifying the\ntraditionally time-consuming task of hyperparameter tuning. We verify the\neffectiveness of this approach on different benchmark tasks and reservoir\noperating conditions.", "field": "Computer Science", "categories": "cs.ET,cs.AI,cs.NE,physics.optics"}, {"arxiv_id": "2401.14373", "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced\n  Understanding and Generation", "abstract": "The recent advances in natural language processing have predominantly favored\nwell-resourced English-centric models, resulting in a significant gap with\nlow-resource languages. In this work, we introduce the language model TURNA,\nwhich is developed for the low-resource language Turkish and is capable of both\nnatural language understanding and generation tasks. TURNA is pretrained with\nan encoder-decoder architecture based on the unified framework UL2 with a\ndiverse corpus that we specifically curated for this purpose. We evaluated\nTURNA with three generation tasks and five understanding tasks for Turkish. The\nresults show that TURNA outperforms several multilingual models in both\nunderstanding and generation tasks, and competes with monolingual Turkish\nmodels in understanding tasks. TURNA is made available at\nhttps://huggingface.co/boun-tabi-LMG/TURNA .", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.14375", "title": "The GraphTempo Framework for Exploring the Evolution of a Graph through\n  Pattern Aggregation", "abstract": "When the focus is on the relationships or interactions between entities,\ngraphs offer an intuitive model for many real-world data. Such graphs are\nusually large and change over time, thus, requiring models and strategies that\nexplore their evolution. We study the evolution of aggregated graphs and\nintroduce the GraphTempo model that allows temporal and attribute aggregation\nnot only on node level by grouping individual nodes, but on a pattern level as\nwell, where subgraphs are grouped together. Furthermore, We propose an\nefficient strategy for exploring the evolution of the graph based on\nidentifying time intervals of significant growth, shrinkage or stability.\nFinally, we evaluate the efficiency and effectiveness of the proposed approach\nusing three real graphs.", "field": "Computer Science", "categories": "cs.SI"}, {"arxiv_id": "2401.14377", "title": "Bonding Grammars", "abstract": "We introduce bonding grammars, a graph grammar formalism developed to model\nDNA computation by means of graph transformations. It is a modification of\nfusion grammars introduced by Kreowski, Kuske and Lye in 2017. Bonding is a\ngraph transformation that consists of merging two hyperedges into a single\nlarger one. We show why bonding better reflects interaction between DNA\nmolecules than fusion. We prove that bonding grammars naturally generalise\nregular sticker systems. We also study the relation between bonding grammars\nand hyperedge replacement grammars proving that each of these kinds of grammars\ngenerates a language the other one cannot generate. Finally, we prove that the\nmembership problem for bonding grammars is NP-complete and, moreover, that some\nbonding grammar generates an NP-complete set.", "field": "Computer Science", "categories": "cs.FL"}, {"arxiv_id": "2401.14379", "title": "UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation\n  and Diffusion Models", "abstract": "In contemporary design practices, the integration of computer vision and\ngenerative artificial intelligence (genAI) represents a transformative shift\ntowards more interactive and inclusive processes. These technologies offer new\ndimensions of image analysis and generation, which are particularly relevant in\nthe context of urban landscape reconstruction. This paper presents a novel\nworkflow encapsulated within a prototype application, designed to leverage the\nsynergies between advanced image segmentation and diffusion models for a\ncomprehensive approach to urban design. Our methodology encompasses the\nOneFormer model for detailed image segmentation and the Stable Diffusion XL\n(SDXL) diffusion model, implemented through ControlNet, for generating images\nfrom textual descriptions. Validation results indicated a high degree of\nperformance by the prototype application, showcasing significant accuracy in\nboth object detection and text-to-image generation. This was evidenced by\nsuperior Intersection over Union (IoU) and CLIP scores across iterative\nevaluations for various categories of urban landscape features. Preliminary\ntesting included utilising UrbanGenAI as an educational tool enhancing the\nlearning experience in design pedagogy, and as a participatory instrument\nfacilitating community-driven urban planning. Early results suggested that\nUrbanGenAI not only advances the technical frontiers of urban landscape\nreconstruction but also provides significant pedagogical and participatory\nplanning benefits. The ongoing development of UrbanGenAI aims to further\nvalidate its effectiveness across broader contexts and integrate additional\nfeatures such as real-time feedback mechanisms and 3D modelling capabilities.\nKeywords: generative AI; panoptic image segmentation; diffusion models; urban\nlandscape design; design pedagogy; co-design", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.14381", "title": "Manifold GCN: Diffusion-based Convolutional Neural Network for\n  Manifold-valued Graphs", "abstract": "We propose two graph neural network layers for graphs with features in a\nRiemannian manifold. First, based on a manifold-valued graph diffusion\nequation, we construct a diffusion layer that can be applied to an arbitrary\nnumber of nodes and graph connectivity patterns. Second, we model a tangent\nmultilayer perceptron by transferring ideas from the vector neuron framework to\nour general setting. Both layers are equivariant with respect to node\npermutations and isometries of the feature manifold. These properties have been\nshown to lead to a beneficial inductive bias in many deep learning tasks.\nNumerical examples on synthetic data as well as on triangle meshes of the right\nhippocampus to classify Alzheimer's disease demonstrate the very good\nperformance of our layers.", "field": "Computer Science", "categories": "cs.LG,math.DG,53Z50,I.2.4"}, {"arxiv_id": "2401.14382", "title": "An Orthogonal Polynomial Kernel-Based Machine Learning Model for\n  Differential-Algebraic Equations", "abstract": "The recent introduction of the Least-Squares Support Vector Regression\n(LS-SVR) algorithm for solving differential and integral equations has sparked\ninterest. In this study, we expand the application of this algorithm to address\nsystems of differential-algebraic equations (DAEs). Our work presents a novel\napproach to solving general DAEs in an operator format by establishing\nconnections between the LS-SVR machine learning model, weighted residual\nmethods, and Legendre orthogonal polynomials. To assess the effectiveness of\nour proposed method, we conduct simulations involving various DAE scenarios,\nsuch as nonlinear systems, fractional-order derivatives, integro-differential,\nand partial DAEs. Finally, we carry out comparisons between our proposed method\nand currently established state-of-the-art approaches, demonstrating its\nreliability and effectiveness.", "field": "Computer Science", "categories": "math.NA,cs.LG,cs.NA"}, {"arxiv_id": "2401.14383", "title": "A Sum-of-Squares Hierarchy in the Absence of Pointwise Proofs I: Energy\n  Certificates", "abstract": "We devise a parameterized family of distributions, the high-entropy step\ndistributions (HES), which are expressive enough to capture near-optima of\nspherical spin glass models in the full Replica Symmetry Breaking (fRSB) regime\nand yet permit low-degree Sum-of-Squares (SoS) certificates that no such\ndistribution can achieve value slightly larger than the true optimum. This\nyields a SoS optimization program and rounding scheme that attains near-optimal\nsolutions for spherical spin glasses in the fRSB regime. In other regimes, the\nsame results occur at the ALG value, which is a conjectured best-value\nattainable by any polynomial time algorithm. These SoS programs optimize over\nfamilies of distributions of possible solutions, and circumvent the oft-cited\nimpossibility of providing a low-degree SoS proof of concentration of measure\nby instead proving the same bounds only in expectation on solution\ndistributions that can be produced by the chosen rounding algorithm. The new\nSoS hierarchy does not make any specific reference to the spherical spin glass\nproblem, and we conjecture that it can be applied to a broad range of\naverage-case problems to obtain value that is optimal among polynomial-time\nalgorithms. We give evidence for this with examples of ensembles that provably\nfool certain local iterative algorithms but for which there is either proof or\nevidence that the SoS program is better. This opens the door to addressing a\nquestion posed by Barak about the possible optimality of SoS on average-case\noptimization problems, and by Schramm about reductions between different\nfamilies of algorithms for average-case problems. In this paper, we give\nlow-degree SoS proofs certifying key properties about HES distributions as well\nas the ALG threshold for spherical spin glasses. The rounding algorithm is\nintroduced and analyzed in a companion paper.", "field": "Computer Science", "categories": "cs.CC,math-ph,math.CA,math.MP,math.OC,math.PR"}, {"arxiv_id": "2401.14387", "title": "Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label\n  Pairs", "abstract": "Generating sufficient labeled data is a significant hurdle in the efficient\nexecution of deep learning projects, especially in uncharted territories of\nimage segmentation where labeling demands extensive time, unlike classification\ntasks. Our study confronts this challenge, operating in an environment\nconstrained by limited hardware resources and the lack of extensive datasets or\npre-trained models. We introduce the novel use of Inconsistency Masks (IM) to\neffectively filter uncertainty in image-pseudo-label pairs, substantially\nelevating segmentation quality beyond traditional semi-supervised learning\ntechniques. By integrating IM with other methods, we demonstrate remarkable\nbinary segmentation performance on the ISIC 2018 dataset, starting with just\n10% labeled data. Notably, three of our hybrid models outperform those trained\non the fully labeled dataset. Our approach consistently achieves exceptional\nresults across three additional datasets and shows further improvement when\ncombined with other techniques. For comprehensive and robust evaluation, this\npaper includes an extensive analysis of prevalent semi-supervised learning\nstrategies, all trained under identical starting conditions. The full code is\navailable at: https://github.com/MichaelVorndran/InconsistencyMasks", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14388", "title": "Smooth Ranking SVM via Cutting-Plane Method", "abstract": "The most popular classification algorithms are designed to maximize\nclassification accuracy during training. However, this strategy may fail in the\npresence of class imbalance since it is possible to train models with high\naccuracy by overfitting to the majority class. On the other hand, the Area\nUnder the Curve (AUC) is a widely used metric to compare classification\nperformance of different algorithms when there is a class imbalance, and\nvarious approaches focusing on the direct optimization of this metric during\ntraining have been proposed. Among them, SVM-based formulations are especially\npopular as this formulation allows incorporating different regularization\nstrategies easily. In this work, we develop a prototype learning approach that\nrelies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our\nalgorithm learns simpler models by iteratively introducing cutting planes, thus\noverfitting is prevented in an unconventional way. Furthermore, it penalizes\nthe changes in the weights at each iteration to avoid large jumps that might be\nobserved in the test performance, thus facilitating a smooth learning process.\nBased on the experiments conducted on 73 binary classification datasets, our\nmethod yields the best test AUC in 25 datasets among its relevant competitors.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.14391", "title": "Rethinking Patch Dependence for Masked Autoencoders", "abstract": "In this work, we re-examine inter-patch dependencies in the decoding\nmechanism of masked autoencoders (MAE). We decompose this decoding mechanism\nfor masked patch reconstruction in MAE into self-attention and cross-attention.\nOur investigations suggest that self-attention between mask patches is not\nessential for learning good representations. To this end, we propose a novel\npretraining framework: Cross-Attention Masked Autoencoders (CrossMAE).\nCrossMAE's decoder leverages only cross-attention between masked and visible\ntokens, with no degradation in downstream performance. This design also enables\ndecoding only a small subset of mask tokens, boosting efficiency. Furthermore,\neach decoder block can now leverage different encoder features, resulting in\nimproved representation learning. CrossMAE matches MAE in performance with 2.5\nto 3.7$\\times$ less decoding compute. It also surpasses MAE on ImageNet\nclassification and COCO instance segmentation under the same compute. Code and\nmodels: https://crossmae.github.io", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14394", "title": "O(1) Insertion for Random Walk d-ary Cuckoo Hashing up to the Load\n  Threshold", "abstract": "The random walk $d$-ary cuckoo hashing algorithm was defined by Fotakis,\nPagh, Sanders, and Spirakis to generalize and improve upon the standard cuckoo\nhashing algorithm of Pagh and Rodler. Random walk $d$-ary cuckoo hashing has\nlow space overhead, guaranteed fast access, and fast in practice insertion\ntime. In this paper, we give a theoretical insertion time bound for this\nalgorithm. More precisely, for every $d\\ge 3$ hashes, let $c_d^*$ be the sharp\nthreshold for the load factor at which a valid assignment of $cm$ objects to a\nhash table of size $m$ likely exists. We show that for any $d\\ge 4$ hashes and\nload factor $c<c_d^*$, the expectation of the random walk insertion time is\n$O(1)$, that is, a constant depending only on $d$ and $c$ but not $m$.", "field": "Computer Science", "categories": "cs.DS,math.CO,F.2.2; E.2"}, {"arxiv_id": "2401.14398", "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes", "abstract": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation,\nwhich learns to estimate the shape and appearance of whole objects that are\nonly partially visible behind occlusions. By capitalizing on large-scale\ndiffusion models and transferring their representations to this task, we learn\na conditional diffusion model for reconstructing whole objects in challenging\nzero-shot cases, including examples that break natural and physical priors,\nsuch as art. As training data, we use a synthetically curated dataset\ncontaining occluded objects paired with their whole counterparts. Experiments\nshow that our approach outperforms supervised baselines on established\nbenchmarks. Our model can furthermore be used to significantly improve the\nperformance of existing object recognition and 3D reconstruction methods in the\npresence of occlusions.", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.14400", "title": "Modular Adaptation of Multilingual Encoders to Written Swiss German\n  Dialect", "abstract": "Creating neural text encoders for written Swiss German is challenging due to\na dearth of training data combined with dialectal variation. In this paper, we\nbuild on several existing multilingual encoders and adapt them to Swiss German\nusing continued pre-training. Evaluation on three diverse downstream tasks\nshows that simply adding a Swiss German adapter to a modular encoder achieves\n97.5% of fully monolithic adaptation performance. We further find that for the\ntask of retrieving Swiss German sentences given Standard German queries,\nadapting a character-level model is more effective than the other adaptation\nstrategies. We release our code and the models trained for our experiments at\nhttps://github.com/ZurichNLP/swiss-german-text-encoders", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.14401", "title": "Range-Agnostic Multi-View Depth Estimation With Keyframe Selection", "abstract": "Methods for 3D reconstruction from posed frames require prior knowledge about\nthe scene metric range, usually to recover matching cues along the epipolar\nlines and narrow the search range. However, such prior might not be directly\navailable or estimated inaccurately in real scenarios -- e.g., outdoor 3D\nreconstruction from video sequences -- therefore heavily hampering performance.\nIn this paper, we focus on multi-view depth estimation without requiring prior\nknowledge about the metric range of the scene by proposing RAMDepth, an\nefficient and purely 2D framework that reverses the depth estimation and\nmatching steps order. Moreover, we demonstrate the capability of our framework\nto provide rich insights about the quality of the views used for prediction.\nAdditional material can be found on our project page\nhttps://andreaconti.github.io/projects/range_agnostic_multi_view_depth.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.14403", "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World", "abstract": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/", "field": "Computer Science", "categories": "cs.RO,cs.AI,cs.CV,cs.LG,cs.SY,eess.SY"}, {"arxiv_id": "2401.14404", "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning", "abstract": "In this study, we examine the representation learning abilities of Denoising\nDiffusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive procedure allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large\nextent resembles a classical DAE. We hope our study will rekindle interest in a\nfamily of classical methods within the realm of modern self-supervised\nlearning.", "field": "Computer Science", "categories": "cs.CV,cs.LG"}, {"arxiv_id": "2401.14405", "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\n  Modalities", "abstract": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.LG"}]}