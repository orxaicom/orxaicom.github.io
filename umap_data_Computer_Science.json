{"embeddings": [[9.628702163696289, 4.127776622772217], [10.674605369567871, 3.7196578979492188], [9.968266487121582, 3.7846500873565674], [10.545456886291504, 3.6044440269470215], [9.917719841003418, 4.046431541442871], [9.987346649169922, 4.474289417266846], [10.388646125793457, 2.6486449241638184], [10.856819152832031, 3.8193840980529785], [9.73282241821289, 3.994471788406372], [10.024152755737305, 3.8008575439453125], [14.027523040771484, 4.645542144775391], [9.859304428100586, 2.6297569274902344], [11.038493156433105, 3.9499897956848145], [10.61933422088623, 3.580970048904419], [10.237106323242188, 2.7977023124694824], [10.532708168029785, 3.9205820560455322], [10.800660133361816, 3.202206611633301], [10.945018768310547, 3.3004117012023926], [13.498418807983398, 3.4066176414489746], [11.053374290466309, 6.7483391761779785], [13.029084205627441, 3.5611209869384766], [11.616249084472656, 5.887939929962158], [12.881173133850098, 3.2095072269439697], [12.840044021606445, 7.095745086669922], [10.046296119689941, 6.030660152435303], [12.841891288757324, 7.141796588897705], [12.707029342651367, 6.814539909362793], [10.901183128356934, 6.938145160675049], [13.251992225646973, 4.081886291503906], [11.021705627441406, 6.151556491851807], [11.436395645141602, 4.737094879150391], [10.210403442382812, 3.0895256996154785], [13.270760536193848, 8.043220520019531], [10.293219566345215, 6.319962978363037], [13.02249813079834, 4.767926216125488], [13.790589332580566, 5.038442611694336], [13.121088027954102, 7.965085029602051], [10.352798461914062, 4.88245153427124], [11.565703392028809, 3.1465225219726562], [13.589208602905273, 4.771650791168213], [9.77795696258545, 3.0458695888519287], [9.648126602172852, 3.7842116355895996], [13.388991355895996, 4.7017364501953125], [11.42869758605957, 4.609702110290527], [12.731789588928223, 4.732522010803223], [13.098773956298828, 6.084123611450195], [12.62812328338623, 6.456803321838379], [12.894309997558594, 6.2632598876953125], [13.00179386138916, 8.044876098632812], [11.649337768554688, 8.161147117614746], [10.791810035705566, 5.149944305419922], [10.265318870544434, 3.820343255996704], [11.087532043457031, 5.86256217956543], [13.02354907989502, 6.163643836975098], [12.76889419555664, 7.793955326080322], [12.671688079833984, 7.684243202209473], [10.272539138793945, 3.340139150619507], [9.76898193359375, 4.192953109741211], [9.69991683959961, 3.1438281536102295], [10.37917709350586, 5.666591167449951], [10.252335548400879, 5.008529186248779], [12.960060119628906, 7.269513130187988], [11.972794532775879, 6.047588348388672], [11.92793083190918, 6.140736103057861], [12.497649192810059, 5.306500434875488], [9.754733085632324, 2.7634363174438477], [13.134817123413086, 5.021486759185791], [9.625937461853027, 3.4933347702026367], [9.842718124389648, 5.270627498626709], [10.553667068481445, 3.1130433082580566], [13.484111785888672, 6.721199035644531], [13.664889335632324, 4.897215843200684], [13.132255554199219, 4.176137447357178], [10.94399356842041, 4.418900012969971], [9.793330192565918, 3.513583183288574], [11.498031616210938, 8.172751426696777], [12.153573989868164, 5.583750247955322], [13.047770500183105, 7.890923023223877], [12.819732666015625, 3.6555566787719727], [13.753442764282227, 5.858790874481201], [12.837859153747559, 4.622005939483643], [13.749619483947754, 6.244071960449219], [10.127445220947266, 4.803377151489258], [12.045530319213867, 4.8565874099731445], [11.41655445098877, 2.6239960193634033], [11.478463172912598, 6.86741304397583], [13.575126647949219, 3.4811177253723145], [12.912546157836914, 4.332067012786865], [11.702465057373047, 6.957655906677246], [13.785404205322266, 6.291456699371338], [11.618063926696777, 4.594401836395264], [12.663402557373047, 4.755306720733643], [12.492056846618652, 4.32582426071167], [13.612425804138184, 5.040931224822998], [11.630877494812012, 5.916309833526611], [10.14928913116455, 3.3255767822265625], [13.350065231323242, 4.388541221618652], [10.996450424194336, 2.8606436252593994], [10.37652587890625, 3.395073175430298], [11.189074516296387, 4.067960739135742], [11.647835731506348, 3.6344261169433594], [13.400777816772461, 7.664457321166992], [11.447625160217285, 6.777533531188965], [11.453532218933105, 5.578855991363525], [11.27375602722168, 5.64647912979126], [10.046178817749023, 6.414193630218506], [10.590775489807129, 3.792104959487915], [10.535490989685059, 3.225900650024414], [10.789108276367188, 4.969493865966797], [9.801645278930664, 4.51641321182251], [11.915886878967285, 5.57108736038208], [10.715947151184082, 6.812621116638184], [10.621417999267578, 2.782688856124878], [10.296710968017578, 2.6443889141082764], [10.763794898986816, 6.433740615844727], [12.529376029968262, 4.364626407623291], [9.565022468566895, 3.6067445278167725], [12.149471282958984, 3.207853078842163], [12.06017017364502, 8.077619552612305], [13.03053092956543, 3.6167941093444824], [13.648933410644531, 6.431465148925781], [10.34998893737793, 3.4992222785949707], [12.478010177612305, 3.971741199493408], [12.87535285949707, 6.136480331420898], [13.761270523071289, 4.857447624206543], [12.634021759033203, 3.287700653076172], [9.887306213378906, 3.7751829624176025], [11.242208480834961, 4.195315361022949], [13.481586456298828, 7.719137668609619], [9.810736656188965, 2.7893404960632324], [11.192049026489258, 7.312527656555176], [11.427523612976074, 2.7771973609924316], [12.093194007873535, 6.1803789138793945], [11.545868873596191, 2.9503767490386963], [12.077482223510742, 8.290156364440918], [11.658919334411621, 2.786560297012329], [9.986289024353027, 4.987673282623291], [13.814338684082031, 6.569055080413818], [10.298884391784668, 4.81246280670166], [12.405083656311035, 4.405791282653809], [11.810615539550781, 5.263625144958496], [10.183614730834961, 6.61462926864624], [11.841035842895508, 2.793525457382202], [12.434399604797363, 5.72500467300415], [12.511067390441895, 6.954005241394043], [13.922450065612793, 4.467953205108643], [11.788555145263672, 6.515484809875488], [11.312836647033691, 7.531968116760254], [11.918374061584473, 7.28737211227417], [10.966155052185059, 4.8294548988342285], [13.625629425048828, 4.116175174713135], [12.113746643066406, 6.13251256942749], [12.12148380279541, 8.378632545471191], [14.063363075256348, 4.768263339996338], [13.553771018981934, 6.300424098968506], [10.9927396774292, 7.125527381896973], [12.266490936279297, 4.554606914520264], [13.3419828414917, 6.378001689910889], [13.251212120056152, 7.269610404968262], [13.254144668579102, 3.2559382915496826], [11.644767761230469, 7.784273624420166], [11.637347221374512, 5.725342273712158], [10.316662788391113, 6.514206409454346], [10.894299507141113, 5.5086894035339355], [13.640826225280762, 6.357388019561768], [10.128689765930176, 6.572903633117676], [11.07562255859375, 6.262233734130859], [12.60857105255127, 3.7595884799957275], [10.48812198638916, 6.168524265289307], [12.477932929992676, 2.99287748336792], [10.274626731872559, 6.624051570892334], [10.708281517028809, 5.22779655456543], [11.280999183654785, 6.8286638259887695], [9.761096954345703, 3.7063255310058594], [10.184943199157715, 6.619767665863037], [13.625399589538574, 3.972703456878662], [10.003026008605957, 5.321040153503418], [10.565835952758789, 5.127963542938232], [11.361886978149414, 5.458999156951904], [13.283194541931152, 3.5626165866851807], [14.13763427734375, 4.77543306350708], [11.680543899536133, 3.9650890827178955], [11.901934623718262, 7.702526092529297], [13.249051094055176, 6.220200538635254], [11.15214729309082, 6.61794376373291], [13.57468318939209, 3.8667495250701904], [11.020950317382812, 4.907259941101074], [11.4743070602417, 7.734699726104736], [13.166815757751465, 7.972421169281006], [13.400067329406738, 8.003226280212402], [11.39952278137207, 7.085048198699951], [10.779051780700684, 3.167621374130249], [12.691179275512695, 6.83187198638916], [11.291601181030273, 5.442935466766357], [12.1776123046875, 8.331798553466797], [12.529332160949707, 7.0788044929504395], [13.460794448852539, 7.603705883026123], [10.626899719238281, 2.6427876949310303], [11.898571014404297, 8.1860933303833], [11.67494010925293, 3.035327196121216], [9.849907875061035, 4.625816345214844], [10.175130844116211, 4.242339134216309], [13.809370040893555, 6.481611251831055], [13.870386123657227, 6.258279800415039], [10.826181411743164, 7.104346752166748], [10.79340934753418, 5.476109981536865], [12.079404830932617, 7.649061679840088], [13.739524841308594, 6.060608386993408], [12.1234130859375, 8.427268028259277], [13.269623756408691, 3.117173671722412], [12.398039817810059, 3.7880871295928955], [11.181468963623047, 6.576426982879639], [12.773614883422852, 4.24159574508667], [9.847029685974121, 3.8823118209838867], [13.458693504333496, 3.2797112464904785], [10.909662246704102, 2.7124226093292236], [10.806506156921387, 3.79482102394104], [12.796253204345703, 3.8362009525299072], [11.68776798248291, 5.3869781494140625], [10.055685997009277, 2.3956961631774902], [13.022562980651855, 6.350377082824707], [13.286249160766602, 3.622659683227539], [10.665494918823242, 6.604475975036621], [12.032376289367676, 7.4526777267456055], [13.140193939208984, 8.025430679321289], [12.122130393981934, 4.563363075256348], [10.13638973236084, 3.0469186305999756], [11.770679473876953, 8.284411430358887], [10.16974925994873, 2.2671494483947754], [12.370132446289062, 4.405760288238525], [12.527626037597656, 7.648619174957275], [13.097334861755371, 3.942589521408081], [12.727911949157715, 3.860058307647705], [11.855552673339844, 8.287789344787598], [12.64393424987793, 4.856513023376465], [13.264939308166504, 4.324202060699463], [11.581071853637695, 8.260631561279297], [11.574554443359375, 7.264797210693359], [9.629724502563477, 2.9296860694885254], [11.005936622619629, 5.34390926361084], [13.378430366516113, 8.086917877197266], [11.566007614135742, 7.699560165405273], [13.776379585266113, 6.017192363739014], [11.26652717590332, 7.016034126281738], [12.462177276611328, 5.7368645668029785], [12.799962043762207, 4.774693489074707], [12.64441967010498, 4.456275939941406], [10.887428283691406, 7.032752513885498], [13.017865180969238, 7.704277038574219], [9.585088729858398, 3.212463140487671], [11.64886474609375, 6.8136725425720215], [10.693178176879883, 3.449873447418213], [10.610921859741211, 3.237274169921875], [13.616155624389648, 5.090802192687988], [10.753464698791504, 2.7887513637542725], [9.862854957580566, 2.705213785171509], [12.946529388427734, 7.7044782638549805], [10.974581718444824, 5.886569976806641], [10.376973152160645, 5.27828311920166], [13.213560104370117, 7.596131324768066], [11.169088363647461, 4.253181457519531], [12.955767631530762, 8.05733585357666], [10.131567001342773, 2.386404037475586], [12.592050552368164, 7.987607955932617], [11.806379318237305, 2.959620475769043], [12.690235137939453, 8.07503890991211], [10.734158515930176, 2.422661781311035], [12.478058815002441, 8.00660514831543], [10.051135063171387, 5.219864368438721], [13.404507637023926, 3.8304481506347656], [11.017745971679688, 7.422204971313477], [11.164350509643555, 7.267614364624023], [13.999247550964355, 4.664789199829102], [10.841480255126953, 2.4663338661193848], [10.429743766784668, 4.8808722496032715], [12.347627639770508, 6.073699474334717], [11.389076232910156, 2.882753610610962], [13.4041748046875, 5.858921051025391], [10.826568603515625, 6.438682556152344], [9.803360939025879, 4.51252555847168], [11.93537712097168, 5.152198791503906], [9.569583892822266, 2.977731704711914], [12.916967391967773, 7.020610332489014], [11.626625061035156, 6.5800275802612305], [9.956001281738281, 5.486013412475586]], "keys": ["2401.1298", "2401.12981", "2401.12982", "2401.12983", "2401.12985", "2401.12986", "2401.12987", "2401.12988", "2401.12989", "2401.1299", "2401.12991", "2401.12992", "2401.12993", "2401.12994", "2401.12995", "2401.12996", "2401.12997", "2401.12998", "2401.13001", "2401.13002", "2401.13006", "2401.13009", "2401.13011", "2401.13014", "2401.13019", "2401.1302", "2401.13034", "2401.13044", "2401.13051", "2401.13053", "2401.13054", "2401.1306", "2401.13062", "2401.13066", "2401.13068", "2401.13076", "2401.13078", "2401.13079", "2401.13081", "2401.13082", "2401.13085", "2401.13086", "2401.13087", "2401.13096", "2401.13097", "2401.13098", "2401.13099", "2401.131", "2401.13103", "2401.13105", "2401.13107", "2401.1311", "2401.13112", "2401.13115", "2401.13127", "2401.13128", "2401.13129", "2401.13133", "2401.13136", "2401.13138", "2401.13142", "2401.13148", "2401.1315", "2401.13154", "2401.13157", "2401.1316", "2401.13161", "2401.13165", "2401.13169", "2401.1317", "2401.13171", "2401.13172", "2401.13174", "2401.13177", "2401.13178", "2401.13182", "2401.13185", "2401.1319", "2401.13191", "2401.13192", "2401.13193", "2401.13196", "2401.13199", "2401.132", "2401.13201", "2401.13202", "2401.13203", "2401.13205", "2401.13206", "2401.13209", "2401.1321", "2401.13212", "2401.13213", "2401.13214", "2401.13216", "2401.13218", "2401.13221", "2401.13222", "2401.13223", "2401.13227", "2401.13229", "2401.13231", "2401.13232", "2401.13236", "2401.13239", "2401.13244", "2401.13245", "2401.13246", "2401.13247", "2401.13248", "2401.13254", "2401.13255", "2401.13256", "2401.1326", "2401.13262", "2401.13264", "2401.13266", "2401.13267", "2401.13268", "2401.1327", "2401.13274", "2401.13275", "2401.1328", "2401.13282", "2401.13285", "2401.13296", "2401.13298", "2401.13301", "2401.13302", "2401.13303", "2401.13306", "2401.13307", "2401.1331", "2401.13311", "2401.13312", "2401.13313", "2401.1332", "2401.13322", "2401.13324", "2401.13325", "2401.13327", "2401.13328", "2401.13329", "2401.1333", "2401.13334", "2401.13341", "2401.13343", "2401.13345", "2401.13346", "2401.13351", "2401.13352", "2401.13354", "2401.13355", "2401.13357", "2401.13358", "2401.13359", "2401.1336", "2401.13361", "2401.13362", "2401.13363", "2401.13365", "2401.13366", "2401.13369", "2401.13371", "2401.13376", "2401.13382", "2401.13384", "2401.13386", "2401.13387", "2401.13388", "2401.1339", "2401.13391", "2401.13394", "2401.13398", "2401.134", "2401.13405", "2401.13407", "2401.13408", "2401.1341", "2401.13414", "2401.13416", "2401.13418", "2401.1342", "2401.13428", "2401.13429", "2401.13432", "2401.13434", "2401.13438", "2401.13439", "2401.13441", "2401.13442", "2401.13444", "2401.13447", "2401.13448", "2401.13451", "2401.1346", "2401.13462", "2401.13463", "2401.13464", "2401.13478", "2401.1348", "2401.13481", "2401.13483", "2401.13486", "2401.13488", "2401.1349", "2401.13493", "2401.13494", "2401.13496", "2401.13498", "2401.13499", "2401.13502", "2401.13503", "2401.13504", "2401.13505", "2401.13509", "2401.13512", "2401.13516", "2401.13518", "2401.13527", "2401.1353", "2401.13531", "2401.13535", "2401.13539", "2401.1354", "2401.13544", "2401.13545", "2401.13546", "2401.13548", "2401.13551", "2401.13552", "2401.13554", "2401.13555", "2401.13556", "2401.13558", "2401.1356", "2401.13561", "2401.13564", "2401.13565", "2401.13566", "2401.13568", "2401.13569", "2401.1357", "2401.13573", "2401.13575", "2401.13578", "2401.13581", "2401.13584", "2401.13585", "2401.13586", "2401.13587", "2401.13588", "2401.13594", "2401.13596", "2401.13598", "2401.13601", "2401.13602", "2401.13604", "2401.13605", "2401.13606", "2401.13609", "2401.1361", "2401.13611", "2401.13612", "2401.13613", "2401.13614", "2401.13621", "2401.13622", "2401.13623", "2401.13627", "2401.1363", "2401.13631", "2401.13639", "2401.13641", "2401.13643", "2401.13645", "2401.13649", "2401.13652", "2401.13653", "2401.13656", "2401.13657", "2401.1366", "2401.13662", "2401.13666", "2401.13667"], "additional_info": [{"arxiv_id": "2401.1298", "title": "Identifying Risk Patterns in Brazilian Police Reports Preceding\n  Femicides: A Long Short Term Memory (LSTM) Based Analysis", "abstract": "Femicide refers to the killing of a female victim, often perpetrated by an\nintimate partner or family member, and is also associated with gender-based\nviolence. Studies have shown that there is a pattern of escalating violence\nleading up to these killings, highlighting the potential for prevention if the\nlevel of danger to the victim can be assessed. Machine learning offers a\npromising approach to address this challenge by predicting risk levels based on\ntextual descriptions of the violence. In this study, we employed the Long Short\nTerm Memory (LSTM) technique to identify patterns of behavior in Brazilian\npolice reports preceding femicides. Our first objective was to classify the\ncontent of these reports as indicating either a lower or higher risk of the\nvictim being murdered, achieving an accuracy of 66%. In the second approach, we\ndeveloped a model to predict the next action a victim might experience within a\nsequence of patterned events. Both approaches contribute to the understanding\nand assessment of the risks associated with domestic violence, providing\nauthorities with valuable insights to protect women and prevent situations from\nescalating.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12981", "title": "A General-purpose AI Avatar in Healthcare", "abstract": "Recent advancements in machine learning and natural language processing have\nled to the rapid development of artificial intelligence (AI) as a valuable tool\nin the healthcare industry. Using large language models (LLMs) as\nconversational agents or chatbots has the potential to assist doctors in\ndiagnosing patients, detecting early symptoms of diseases, and providing health\nadvice to patients. This paper focuses on the role of chatbots in healthcare\nand explores the use of avatars to make AI interactions more appealing to\npatients. A framework of a general-purpose AI avatar application is\ndemonstrated by using a three-category prompt dictionary and prompt improvement\nmechanism. A two-phase approach is suggested to fine-tune a general-purpose AI\nlanguage model and create different AI avatars to discuss medical issues with\nusers. Prompt engineering enhances the chatbot's conversational abilities and\npersonality traits, fostering a more human-like interaction with patients.\nUltimately, the injection of personality into the chatbot could potentially\nincrease patient engagement. Future directions for research include\ninvestigating ways to improve chatbots' understanding of context and ensuring\nthe accuracy of their outputs through fine-tuning with specialized medical data\nsets.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12982", "title": "Text Classification: A Review, Empirical, and Experimental Evaluation", "abstract": "The explosive and widespread growth of data necessitates the use of text\nclassification to extract crucial information from vast amounts of data.\nConsequently, there has been a surge of research in both classical and deep\nlearning text classification methods. Despite the numerous methods proposed in\nthe literature, there is still a pressing need for a comprehensive and\nup-to-date survey. Existing survey papers categorize algorithms for text\nclassification into broad classes, which can lead to the misclassification of\nunrelated algorithms and incorrect assessments of their qualities and behaviors\nusing the same metrics. To address these limitations, our paper introduces a\nnovel methodological taxonomy that classifies algorithms hierarchically into\nfine-grained classes and specific techniques. The taxonomy includes methodology\ncategories, methodology techniques, and methodology sub-techniques. Our study\nis the first survey to utilize this methodological taxonomy for classifying\nalgorithms for text classification. Furthermore, our study also conducts\nempirical evaluation and experimental comparisons and rankings of different\nalgorithms that employ the same specific sub-technique, different\nsub-techniques within the same technique, different techniques within the same\ncategory, and categories", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12983", "title": "Assessing Large Language Models in Mechanical Engineering Education: A\n  Study on Mechanics-Focused Conceptual Understanding", "abstract": "This study is a pioneering endeavor to investigate the capabilities of Large\nLanguage Models (LLMs) in addressing conceptual questions within the domain of\nmechanical engineering with a focus on mechanics. Our examination involves a\nmanually crafted exam encompassing 126 multiple-choice questions, spanning\nvarious aspects of mechanics courses, including Fluid Mechanics, Mechanical\nVibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of\nElasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),\nChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against\nengineering faculties and students with or without mechanical engineering\nbackground. The findings reveal GPT-4's superior performance over the other two\nLLMs and human cohorts in answering questions across various mechanics topics,\nexcept for Continuum Mechanics. This signals the potential future improvements\nfor GPT models in handling symbolic calculations and tensor analyses. The\nperformances of LLMs were all significantly improved with explanations prompted\nprior to direct responses, underscoring the crucial role of prompt engineering.\nInterestingly, GPT-3.5 demonstrates improved performance with prompts covering\na broader domain, while GPT-4 excels with prompts focusing on specific\nsubjects. Finally, GPT-4 exhibits notable advancements in mitigating input\nbias, as evidenced by guessing preferences for humans. This study unveils the\nsubstantial potential of LLMs as highly knowledgeable assistants in both\nmechanical pedagogy and scientific research.", "field": "Computer Science", "categories": "cs.CL,cs.AI,physics.ed-ph"}, {"arxiv_id": "2401.12985", "title": "The Effect of Human v/s Synthetic Test Data and Round-tripping on\n  Assessment of Sentiment Analysis Systems for Bias", "abstract": "Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence\n(AI) systems that output polarity and emotional intensity when given a piece of\ntext as input. Like other AIs, SASs are also known to have unstable behavior\nwhen subjected to changes in data which can make it problematic to trust out of\nconcerns like bias when AI works with humans and data has protected attributes\nlike gender, race, and age. Recently, an approach was introduced to assess SASs\nin a blackbox setting without training data or code, and rating them for bias\nusing synthetic English data. We augment it by introducing two human-generated\nchatbot datasets and also consider a round-trip setting of translating the data\nfrom one language to the same through an intermediate language. We find that\nthese settings show SASs performance in a more realistic light. Specifically,\nwe find that rating SASs on the chatbot data showed more bias compared to the\nsynthetic data, and round-tripping using Spanish and Danish as intermediate\nlanguages reduces the bias (up to 68% reduction) in human-generated data while,\nin synthetic data, it takes a surprising turn by increasing the bias! Our\nfindings will help researchers and practitioners refine their SAS testing\nstrategies and foster trust as SASs are considered part of more\nmission-critical applications for global use.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12986", "title": "Crowdsourced Adaptive Surveys", "abstract": "Public opinion surveys are vital for informing democratic decision-making,\nbut responding to rapidly changing information environments and measuring\nbeliefs within niche communities can be challenging for traditional survey\nmethods. This paper introduces a crowdsourced adaptive survey methodology\n(CSAS) that unites advances in natural language processing and adaptive\nalgorithms to generate question banks that evolve with user input. The CSAS\nmethod converts open-ended text provided by participants into Likert-style\nitems and applies a multi-armed bandit algorithm to determine user-provided\nquestions that should be prioritized in the survey. The method's adaptive\nnature allows for the exploration of new survey questions, while imposing\nminimal costs in survey length. Applications in the domains of Latino\ninformation environments and issue importance showcase CSAS's ability to\nidentify claims or issues that might otherwise be difficult to track using\nstandard approaches. I conclude by discussing the method's potential for\nstudying topics where participant-generated content might improve our\nunderstanding of public opinion.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.HC,stat.AP"}, {"arxiv_id": "2401.12987", "title": "TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition\n  in Conversation", "abstract": "Emotion Recognition in Conversation (ERC) plays a crucial role in enabling\ndialogue systems to effectively respond to user requests. The emotions in a\nconversation can be identified by the representations from various modalities,\nsuch as audio, visual, and text. However, due to the weak contribution of\nnon-verbal modalities to recognize emotions, multimodal ERC has always been\nconsidered a challenging task. In this paper, we propose Teacher-leading\nMultimodal fusion network for ERC (TelME). TelME incorporates cross-modal\nknowledge distillation to transfer information from a language model acting as\nthe teacher to the non-verbal students, thereby optimizing the efficacy of the\nweak modalities. We then combine multimodal features using a shifting fusion\napproach in which student networks support the teacher. TelME achieves\nstate-of-the-art performance in MELD, a multi-speaker conversation dataset for\nERC. Finally, we demonstrate the effectiveness of our components through\nadditional experiments.", "field": "Computer Science", "categories": "cs.CL,cs.LG,cs.SD,eess.AS"}, {"arxiv_id": "2401.12988", "title": "Few-Shot Learning for Chronic Disease Management: Leveraging Large\n  Language Models and Multi-Prompt Engineering with Medical Knowledge Injection", "abstract": "This study harnesses state-of-the-art AI technology for chronic disease\nmanagement, specifically in detecting various mental disorders through\nuser-generated textual content. Existing studies typically rely on fully\nsupervised machine learning, which presents challenges such as the\nlabor-intensive manual process of annotating extensive training data for each\ndisease and the need to design specialized deep learning architectures for each\nproblem. To address such challenges, we propose a novel framework that\nleverages advanced AI techniques, including large language models and\nmulti-prompt engineering. Specifically, we address two key technical challenges\nin data-driven chronic disease management: (1) developing personalized prompts\nto represent each user's uniqueness and (2) incorporating medical knowledge\ninto prompts to provide context for chronic disease detection, instruct\nlearning objectives, and operationalize prediction goals. We evaluate our\nmethod using four mental disorders, which are prevalent chronic diseases\nworldwide, as research cases. On the depression detection task, our method (F1\n= 0.975~0.978) significantly outperforms traditional supervised learning\nparadigms, including feature engineering (F1 = 0.760) and architecture\nengineering (F1 = 0.756). Meanwhile, our approach demonstrates success in\nfew-shot learning, i.e., requiring only a minimal number of training examples\nto detect chronic diseases based on user-generated textual content (i.e., only\n2, 10, or 100 subjects). Moreover, our method can be generalized to other\nmental disorder detection tasks, including anorexia, pathological gambling, and\nself-harm (F1 = 0.919~0.978).", "field": "Computer Science", "categories": "cs.CL,cs.AI,K.5,I.2.7; H.4.m"}, {"arxiv_id": "2401.12989", "title": "Into the crossfire: evaluating the use of a language model to\n  crowdsource gun violence reports", "abstract": "Gun violence is a pressing and growing human rights issue that affects nearly\nevery dimension of the social fabric, from healthcare and education to\npsychology and the economy. Reliable data on firearm events is paramount to\ndeveloping more effective public policy and emergency responses. However, the\nlack of comprehensive databases and the risks of in-person surveys prevent\nhuman rights organizations from collecting needed data in most countries. Here,\nwe partner with a Brazilian human rights organization to conduct a systematic\nevaluation of language models to assist with monitoring real-world firearm\nevents from social media data. We propose a fine-tuned BERT-based model trained\non Twitter (now X) texts to distinguish gun violence reports from ordinary\nPortuguese texts. Our model achieves a high AUC score of 0.97. We then\nincorporate our model into a web application and test it in a live\nintervention. We study and interview Brazilian analysts who continuously\nfact-check social media texts to identify new gun violence events. Qualitative\nassessments show that our solution helped all analysts use their time more\nefficiently and expanded their search capacities. Quantitative assessments show\nthat the use of our model was associated with more analysts' interactions with\nonline users reporting gun violence. Taken together, our findings suggest that\nmodern Natural Language Processing techniques can help support the work of\nhuman rights organizations.", "field": "Computer Science", "categories": "cs.CL,cs.IR"}, {"arxiv_id": "2401.1299", "title": "Topic Modelling: Going Beyond Token Outputs", "abstract": "Topic modelling is a text mining technique for identifying salient themes\nfrom a number of documents. The output is commonly a set of topics consisting\nof isolated tokens that often co-occur in such documents. Manual effort is\noften associated with interpreting a topic's description from such tokens.\nHowever, from a human's perspective, such outputs may not adequately provide\nenough information to infer the meaning of the topics; thus, their\ninterpretability is often inaccurately understood. Although several studies\nhave attempted to automatically extend topic descriptions as a means of\nenhancing the interpretation of topic models, they rely on external language\nsources that may become unavailable, must be kept up-to-date to generate\nrelevant results, and present privacy issues when training on or processing\ndata. This paper presents a novel approach towards extending the output of\ntraditional topic modelling methods beyond a list of isolated tokens. This\napproach removes the dependence on external sources by using the textual data\nitself by extracting high-scoring keywords and mapping them to the topic\nmodel's token outputs. To measure the interpretability of the proposed outputs\nagainst those of the traditional topic modelling approach, independent\nannotators manually scored each output based on their quality and usefulness,\nas well as the efficiency of the annotation task. The proposed approach\ndemonstrated higher quality and usefulness, as well as higher efficiency in the\nannotation task, in comparison to the outputs of a traditional topic modelling\nmethod, demonstrating an increase in their interpretability.", "field": "Computer Science", "categories": "cs.CL,cs.LG"}, {"arxiv_id": "2401.12991", "title": "Catenary and Mercator projection", "abstract": "The Mercator projection is sometimes confused with another mapping technique,\nspecifically the central cylindrical projection, which projects the Earth's\nsurface onto a cylinder tangent to the equator, as if a light source is at the\nEarth's center. Accidentally, this misconception is rather close to a truth.\nThe only operation that the map needs is a free bending in a uniform\ngravitational field if the map's material is dense and soft enough to produce a\ncatenary profile. The north and south edges of the map should be parallel and\nplaced in the same plane at the appropriate distance. In this case, the bent\nmap been projected onto this plane gives the Mercator projection. This property\nis rather curious, since it allows to make such a sophisticated one-to-one\nmapping as the Mercator projection using simple tools available in the\nworkroom.", "field": "Computer Science", "categories": "math.NA,astro-ph.EP,cs.NA"}, {"arxiv_id": "2401.12992", "title": "TranSentence: Speech-to-speech Translation via Language-agnostic\n  Sentence-level Speech Encoding without Language-parallel Data", "abstract": "Although there has been significant advancement in the field of\nspeech-to-speech translation, conventional models still require\nlanguage-parallel speech data between the source and target languages for\ntraining. In this paper, we introduce TranSentence, a novel speech-to-speech\ntranslation without language-parallel speech data. To achieve this, we first\nadopt a language-agnostic sentence-level speech encoding that captures the\nsemantic information of speech, irrespective of language. We then train our\nmodel to generate speech based on the encoded embedding obtained from a\nlanguage-agnostic sentence-level speech encoder that is pre-trained with\nvarious languages. With this method, despite training exclusively on the target\nlanguage's monolingual data, we can generate target language speech in the\ninference stage using language-agnostic speech embedding from the source\nlanguage speech. Furthermore, we extend TranSentence to multilingual\nspeech-to-speech translation. The experimental results demonstrate that\nTranSentence is superior to other models.", "field": "Computer Science", "categories": "cs.CL,cs.SD,eess.AS"}, {"arxiv_id": "2401.12993", "title": "Estimating the severity of dental and oral problems via sentiment\n  classification over clinical reports", "abstract": "Analyzing authors' sentiments in texts as a technique for identifying text\npolarity can be practical and useful in various fields, including medicine and\ndentistry. Currently, due to factors such as patients' limited knowledge about\ntheir condition, difficulties in accessing specialist doctors, or fear of\nillness, particularly in pandemic conditions, there might be a delay between\nreceiving a radiology report and consulting a doctor. In some cases, this delay\ncan pose significant risks to the patient, making timely decision-making\ncrucial. Having an automatic system that can inform patients about the\ndeterioration of their condition by analyzing the text of radiology reports\ncould greatly impact timely decision-making. In this study, a dataset\ncomprising 1,134 cone-beam computed tomography (CBCT) photo reports was\ncollected from the Shiraz University of Medical Sciences. Each case was\nexamined, and an expert labeled a severity level for the patient's condition on\neach document. After preprocessing all the text data, a deep learning model\nbased on Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)\nnetwork architecture, known as CNN-LSTM, was developed to detect the severity\nlevel of the patient's problem based on sentiment analysis in the radiologist's\nreport. The model's performance was evaluated on two datasets, each with two\nand four classes, in both imbalanced and balanced scenarios. Finally, to\ndemonstrate the effectiveness of our model, we compared its performance with\nthat of other classification models. The results, along with one-way ANOVA and\nTukey's test, indicated that our proposed model (CNN-LSTM) performed the best\naccording to precision, recall, and f-measure criteria. This suggests that it\ncan be a reliable model for estimating the severity of oral and dental\ndiseases, thereby assisting patients.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12994", "title": "Automated Scoring of Clinical Patient Notes using Advanced NLP and\n  Pseudo Labeling", "abstract": "Clinical patient notes are critical for documenting patient interactions,\ndiagnoses, and treatment plans in medical practice. Ensuring accurate\nevaluation of these notes is essential for medical education and certification.\nHowever, manual evaluation is complex and time-consuming, often resulting in\nvariability and resource-intensive assessments. To tackle these challenges,\nthis research introduces an approach leveraging state-of-the-art Natural\nLanguage Processing (NLP) techniques, specifically Masked Language Modeling\n(MLM) pretraining, and pseudo labeling. Our methodology enhances efficiency and\neffectiveness, significantly reducing training time without compromising\nperformance. Experimental results showcase improved model performance,\nindicating a potential transformation in clinical note assessment.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12995", "title": "Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed\n  Response Generation in Dialogues", "abstract": "Code-mixing, the blending of multiple languages within a single conversation,\nintroduces a distinctive challenge, particularly in the context of response\ngeneration. Capturing the intricacies of code-mixing proves to be a formidable\ntask, given the wide-ranging variations influenced by individual speaking\nstyles and cultural backgrounds. In this study, we explore response generation\nwithin code-mixed conversations. We introduce a novel approach centered on\nharnessing the Big Five personality traits acquired in an unsupervised manner\nfrom the conversations to bolster the performance of response generation. These\ninferred personality attributes are seamlessly woven into the fabric of the\ndialogue context, using a novel fusion mechanism, PA3. It uses an effective\ntwo-step attention formulation to fuse the dialogue and personality\ninformation. This fusion not only enhances the contextual relevance of\ngenerated responses but also elevates the overall performance of the model. Our\nexperimental results, grounded in a dataset comprising of multi-party\nHindi-English code-mix conversations, highlight the substantial advantages\noffered by personality-infused models over their conventional counterparts.\nThis is evident in the increase observed in ROUGE and BLUE scores for the\nresponse generation task when the identified personality is seamlessly\nintegrated into the dialogue context. Qualitative assessment for personality\nidentification and response generation aligns well with our quantitative\nresults.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12996", "title": "A Comparison of Veterans with Problematic Opioid Use Identified through\n  Natural Language Processing of Clinical Notes versus Using Diagnostic Codes", "abstract": "Background: Electronic health records (EHRs) are a data source for opioid\nresearch. Opioid use disorder is known to be under-coded as a diagnosis, yet\nproblematic opioid use can be documented in clinical notes.\n  Objectives: Our goals were 1) to identify problematic opioid use from a full\nrange of clinical notes; and 2) to compare the characteristics of patients\nidentified as having problematic opioid use, exclusively documented in clinical\nnotes, to those having documented ICD opioid use disorder diagnostic codes.\n  Materials and Methods: We developed and applied a natural language processing\n(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two\nVeteran Affairs service regions to identify patients with problematic opioid\nuse. We also used a set of ICD diagnostic codes to identify patients with\nopioid use disorder from the same cohort. We compared the demographic and\nclinical characteristics of patients identified only through NLP, to those of\npatients identified through ICD codes.\n  Results: NLP exclusively identified 57,331 patients; 6,997 patients had\npositive ICD code identifications. Patients exclusively identified through NLP\nwere more likely to be women. Those identified through ICD codes were more\nlikely to be male, younger, have concurrent benzodiazepine prescriptions, more\ncomorbidities, more care encounters, and less likely to be married. Patients in\nthe NLP and ICD groups had substantially elevated comorbidity levels compared\nto patients not documented as experiencing problematic opioid use.\n  Conclusions: NLP is a feasible approach for identifying problematic opioid\nuse not otherwise recorded by ICD codes. Clinicians may be reluctant to code\nfor opioid use disorder. It is therefore incumbent on the healthcare team to\nsearch for documentation of opioid concerns within clinical notes.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG,J.3"}, {"arxiv_id": "2401.12997", "title": "Progressive Distillation Based on Masked Generation Feature Method for\n  Knowledge Graph Completion", "abstract": "In recent years, knowledge graph completion (KGC) models based on pre-trained\nlanguage model (PLM) have shown promising results. However, the large number of\nparameters and high computational cost of PLM models pose challenges for their\napplication in downstream tasks. This paper proposes a progressive distillation\nmethod based on masked generation features for KGC task, aiming to\nsignificantly reduce the complexity of pre-trained models. Specifically, we\nperform pre-distillation on PLM to obtain high-quality teacher models, and\ncompress the PLM network to obtain multi-grade student models. However,\ntraditional feature distillation suffers from the limitation of having a single\nrepresentation of information in teacher models. To solve this problem, we\npropose masked generation of teacher-student features, which contain richer\nrepresentation information. Furthermore, there is a significant gap in\nrepresentation ability between teacher and student. Therefore, we design a\nprogressive distillation method to distill student models at each grade level,\nenabling efficient knowledge transfer from teachers to students. The\nexperimental results demonstrate that the model in the pre-distillation stage\nsurpasses the existing state-of-the-art methods. Furthermore, in the\nprogressive distillation stage, the model significantly reduces the model\nparameters while maintaining a certain level of performance. Specifically, the\nmodel parameters of the lower-grade student model are reduced by 56.7\\%\ncompared to the baseline.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.12998", "title": "Evaluating and Enhancing Large Language Models Performance in\n  Domain-specific Medicine: Osteoarthritis Management with DocOA", "abstract": "The efficacy of large language models (LLMs) in domain-specific medicine,\nparticularly for managing complex diseases such as osteoarthritis (OA), remains\nlargely unexplored. This study focused on evaluating and enhancing the clinical\ncapabilities of LLMs in specific domains, using osteoarthritis (OA) management\nas a case study. A domain specific benchmark framework was developed, which\nevaluate LLMs across a spectrum from domain-specific knowledge to clinical\napplications in real-world clinical scenarios. DocOA, a specialized LLM\ntailored for OA management that integrates retrieval-augmented generation (RAG)\nand instruction prompts, was developed. The study compared the performance of\nGPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human\nevaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less\neffective in the specialized domain of OA management, particularly in providing\npersonalized treatment recommendations. However, DocOA showed significant\nimprovements. This study introduces a novel benchmark framework which assesses\nthe domain-specific abilities of LLMs in multiple aspects, highlights the\nlimitations of generalized LLMs in clinical contexts, and demonstrates the\npotential of tailored approaches for developing domain-specific medical LLMs.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13001", "title": "PatternPortrait: Draw Me Like One of Your Scribbles", "abstract": "This paper introduces a process for generating abstract portrait drawings\nfrom pictures. Their unique style is created by utilizing single freehand\npattern sketches as references to generate unique patterns for shading. The\nmethod involves extracting facial and body features from images and\ntransforming them into vector lines. A key aspect of the research is the\ndevelopment of a graph neural network architecture designed to learn sketch\nstroke representations in vector form, enabling the generation of diverse\nstroke variations. The combination of these two approaches creates joyful\nabstract drawings that are realized via a pen plotter. The presented process\ngarnered positive feedback from an audience of approximately 280 participants.", "field": "Computer Science", "categories": "cs.GR,cs.AI,cs.LG"}, {"arxiv_id": "2401.13002", "title": "Theorem Discovery Amongst Cyclic Polygons", "abstract": "We examine a class of geometric theorems on cyclic 2n-gons. We prove that if\nwe take n disjoint pairs of sides, each pair separated by an even number of\npolygon sides, then there is a linear combination of the angles between those\nsides which is constant. We present a formula for the linear combination, which\nprovides a theorem statement in terms of those angles. We describe a program\nwhich uses this result to generate new geometry proof problems and their\nsolutions.", "field": "Computer Science", "categories": "cs.CG,cs.AI"}, {"arxiv_id": "2401.13006", "title": "CIMGEN: Controlled Image Manipulation by Finetuning Pretrained\n  Generative Models on Limited Data", "abstract": "Content creation and image editing can benefit from flexible user controls. A\ncommon intermediate representation for conditional image generation is a\nsemantic map, that has information of objects present in the image. When\ncompared to raw RGB pixels, the modification of semantic map is much easier.\nOne can take a semantic map and easily modify the map to selectively insert,\nremove, or replace objects in the map. The method proposed in this paper takes\nin the modified semantic map and alter the original image in accordance to the\nmodified map. The method leverages traditional pre-trained image-to-image\ntranslation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a\nlimited dataset of reference images associated with the semantic maps. We\ndiscuss the qualitative and quantitative performance of our technique to\nillustrate its capacity and possible applications in the fields of image\nforgery and image editing. We also demonstrate the effectiveness of the\nproposed image forgery technique in thwarting the numerous deep learning-based\nimage forensic techniques, highlighting the urgent need to develop robust and\ngeneralizable image forensic tools in the fight against the spread of fake\nmedia.", "field": "Computer Science", "categories": "cs.AI,cs.LG,eess.IV"}, {"arxiv_id": "2401.13009", "title": "Comparative Study of Causal Discovery Methods for Cyclic Models with\n  Hidden Confounders", "abstract": "Nowadays, the need for causal discovery is ubiquitous. A better understanding\nof not just the stochastic dependencies between parts of a system, but also the\nactual cause-effect relations, is essential for all parts of science. Thus, the\nneed for reliable methods to detect causal directions is growing constantly. In\nthe last 50 years, many causal discovery algorithms have emerged, but most of\nthem are applicable only under the assumption that the systems have no feedback\nloops and that they are causally sufficient, i.e. that there are no unmeasured\nsubsystems that can affect multiple measured variables. This is unfortunate\nsince those restrictions can often not be presumed in practice. Feedback is an\nintegral feature of many processes, and real-world systems are rarely\ncompletely isolated and fully measured. Fortunately, in recent years, several\ntechniques, that can cope with cyclic, causally insufficient systems, have been\ndeveloped. And with multiple methods available, a practical application of\nthose algorithms now requires knowledge of the respective strengths and\nweaknesses. Here, we focus on the problem of causal discovery for sparse linear\nmodels which are allowed to have cycles and hidden confounders. We have\nprepared a comprehensive and thorough comparative study of four causal\ndiscovery techniques: two versions of the LLC method [10] and two variants of\nthe ASP-based algorithm [11]. The evaluation investigates the performance of\nthose techniques for various experiments with multiple interventional setups\nand different dataset sizes.", "field": "Computer Science", "categories": "cs.LG,stat.ME,stat.ML"}, {"arxiv_id": "2401.13011", "title": "CCA: Collaborative Competitive Agents for Image Editing", "abstract": "This paper presents a novel generative model, Collaborative Competitive\nAgents (CCA), which leverages the capabilities of multiple Large Language\nModels (LLMs) based agents to execute complex tasks. Drawing inspiration from\nGenerative Adversarial Networks (GANs), the CCA system employs two equal-status\ngenerator agents and a discriminator agent. The generators independently\nprocess user instructions and generate results, while the discriminator\nevaluates the outputs, and provides feedback for the generator agents to\nfurther reflect and improve the generation results. Unlike the previous\ngenerative model, our system can obtain the intermediate steps of generation.\nThis allows each generator agent to learn from other successful executions due\nto its transparency, enabling a collaborative competition that enhances the\nquality and robustness of the system's results. The primary focus of this study\nis image editing, demonstrating the CCA's ability to handle intricate\ninstructions robustly. The paper's main contributions include the introduction\nof a multi-agent-based generative model with controllable intermediate steps\nand iterative optimization, a detailed examination of agent relationships, and\ncomprehensive experiments on image editing. Code is available at\n\\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13014", "title": "A Novel Policy Iteration Algorithm for Nonlinear Continuous-Time\n  H$\\infty$ Control Problem", "abstract": "H{\\infty} control of nonlinear continuous-time system depends on the solution\nof the Hamilton-Jacobi-Isaacs (HJI) equation, which has been proved impossible\nto obtain a closed-form solution due to the nonlinearity of HJI equation. In\norder to solve HJI equation, many iterative algorithms were proposed, and most\nof the algorithms were essentially Newton method when the fixed-point equation\nwas constructed in a Banach space. Newton method is a local optimization\nmethod, it has small convergence region and needs the initial guess to be\nsufficiently close to the solution. Whereas damped Newton method enhances the\nrobustness with respect to initial condition and has larger convergence region.\nIn this paper, a novel reinforcement learning method which is named\n{\\alpha}-policy iteration ({\\alpha}-PI) is introduced for solving HJI equation.\nFirst, by constructing a damped Newton iteration operator equation, a\ngeneralized Bellman equation (GBE) is obtained. The GBE is an extension of\nbellman equation. And then, by iterating on the GBE, an on-policy {\\alpha}-PI\nreinforcement learning method without using knowledge regarding to the system\ninternal dynamics is proposed. Third, based on the on-policy {\\alpha}-PI\nreinforcement learning method, we develop an off-policy {\\alpha}-PI\nreinforcement learning method without requiring any knowledge of the system\ndynamics. Finally, the neural-network based adaptive critic implementation\nschemes of on-policy and off-policy {\\alpha}-PI algorithms are derived\nrespectively, and the batch least-squares method is used for calculating the\nweight parameters of neural networks. The effectiveness of the off-policy\n{\\alpha}-PI algorithm is verified through computer simulation.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13019", "title": "White-box validation of quantitative product lines by statistical model\n  checking and process mining", "abstract": "We propose a novel methodology for validating software product line (PL)\nmodels by integrating Statistical Model Checking (SMC) with Process Mining\n(PM). Our approach focuses on the feature-oriented language QFLan in the PL\nengineering domain, allowing modeling of PLs with rich cross-tree and\nquantitative constraints, as well as aspects of dynamic PLs like staged\nconfigurations. This richness leads to models with infinite state-space,\nrequiring simulation-based analysis techniques like SMC. For instance, we\nillustrate with a running example involving infinite state space. SMC involves\ngenerating samples of system dynamics to estimate properties such as event\nprobabilities or expected values. On the other hand, PM uses data-driven\ntechniques on execution logs to identify and reason about the underlying\nexecution process. In this paper, we propose, for the first time, applying PM\ntechniques to SMC simulations' byproducts to enhance the utility of SMC\nanalyses. Typically, when SMC results are unexpected, modelers must determine\nwhether they stem from actual system characteristics or model bugs in a\nblack-box manner. We improve on this by using PM to provide a white-box\nperspective on the observed system dynamics. Samples from SMC are fed into PM\ntools, producing a compact graphical representation of observed dynamics. The\nmined PM model is then transformed into a QFLan model, accessible to PL\nengineers. Using two well-known PL models, we demonstrate the effectiveness and\nscalability of our methodology in pinpointing issues and suggesting fixes.\nAdditionally, we show its generality by applying it to the security domain.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.1302", "title": "A Safe Reinforcement Learning Algorithm for Supervisory Control of Power\n  Plants", "abstract": "Traditional control theory-based methods require tailored engineering for\neach system and constant fine-tuning. In power plant control, one often needs\nto obtain a precise representation of the system dynamics and carefully design\nthe control scheme accordingly. Model-free Reinforcement learning (RL) has\nemerged as a promising solution for control tasks due to its ability to learn\nfrom trial-and-error interactions with the environment. It eliminates the need\nfor explicitly modeling the environment's dynamics, which is potentially\ninaccurate. However, the direct imposition of state constraints in power plant\ncontrol raises challenges for standard RL methods. To address this, we propose\na chance-constrained RL algorithm based on Proximal Policy Optimization for\nsupervisory control. Our method employs Lagrangian relaxation to convert the\nconstrained optimization problem into an unconstrained objective, where\ntrainable Lagrange multipliers enforce the state constraints. Our approach\nachieves the smallest distance of violation and violation rate in a load-follow\nmaneuver for an advanced Nuclear Power Plant design.", "field": "Computer Science", "categories": "cs.SY,cs.LG"}, {"arxiv_id": "2401.13034", "title": "Locality Sensitive Sparse Encoding for Learning World Models Online", "abstract": "Acquiring an accurate world model online for model-based reinforcement\nlearning (MBRL) is challenging due to data nonstationarity, which typically\ncauses catastrophic forgetting for neural networks (NNs). From the online\nlearning perspective, a Follow-The-Leader (FTL) world model is desirable, which\noptimally fits all previous experiences at each round. Unfortunately, NN-based\nmodels need re-training on all accumulated data at every interaction step to\nachieve FTL, which is computationally expensive for lifelong agents. In this\npaper, we revisit models that can achieve FTL with incremental updates.\nSpecifically, our world model is a linear regression model supported by\nnonlinear random features. The linear part ensures efficient FTL update while\nthe nonlinear random feature empowers the fitting of complex environments. To\nbest trade off model capacity and computation efficiency, we introduce a\nlocality sensitive sparse encoding, which allows us to conduct efficient sparse\nupdates even with very high dimensional nonlinear features. We validate the\nrepresentation power of our encoding and verify that it allows efficient online\nlearning under data covariate shift. We also show, in the Dyna MBRL setting,\nthat our world models learned online using a single pass of trajectory data\neither surpass or match the performance of deep world models trained with\nreplay and other continual learning methods.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13044", "title": "Deterministic Collision-Free Exploration of Unknown Anonymous Graphs", "abstract": "We consider the fundamental task of network exploration. A network is modeled\nas a simple connected undirected n-node graph with unlabeled nodes, and all\nports at any node of degree d are arbitrarily numbered 0,.....,d-1. Each of two\nidentical mobile agents, initially situated at distinct nodes, has to visit all\nnodes and stop. Agents execute the same deterministic algorithm and move in\nsynchronous rounds: in each round, an agent can either remain at the same node\nor move to an adjacent node. Exploration must be collision-free: in every round\nat most one agent can be at any node. We assume that agents have vision of\nradius 2: an awake agent situated at a node v can see the subgraph induced by\nall nodes at a distance at most 2 from v, sees all port numbers in this\nsubgraph, and the agents located at these nodes. Agents do not know the entire\ngraph but they know an upper bound n on its size. The time of an exploration is\nthe number of rounds since the wakeup of the later agent to the termination by\nboth agents. We show a collision-free exploration algorithm working in time\npolynomial in n, for arbitrary graphs of size larger than 2. Moreover, we show\nthat if agents have only vision of radius 1, then collision-free exploration is\nimpossible, e.g., in any tree of diameter 2.", "field": "Computer Science", "categories": "cs.DC"}, {"arxiv_id": "2401.13051", "title": "PA-SAM: Prompt Adapter SAM for High-Quality Image Segmentation", "abstract": "The Segment Anything Model (SAM) has exhibited outstanding performance in\nvarious image segmentation tasks. Despite being trained with over a billion\nmasks, SAM faces challenges in mask prediction quality in numerous scenarios,\nespecially in real-world contexts. In this paper, we introduce a novel\nprompt-driven adapter into SAM, namely Prompt Adapter Segment Anything Model\n(PA-SAM), aiming to enhance the segmentation mask quality of the original SAM.\nBy exclusively training the prompt adapter, PA-SAM extracts detailed\ninformation from images and optimizes the mask decoder feature at both sparse\nand dense prompt levels, improving the segmentation performance of SAM to\nproduce high-quality masks. Experimental results demonstrate that our PA-SAM\noutperforms other SAM-based methods in high-quality, zero-shot, and open-set\nsegmentation. We're making the source code and models available at\nhttps://github.com/xzz2/pa-sam.", "field": "Computer Science", "categories": "cs.CV,eess.IV"}, {"arxiv_id": "2401.13053", "title": "Data Exchange Markets via Utility Balancing", "abstract": "This paper explores the design of a balanced data-sharing marketplace for\nentities with heterogeneous datasets and machine learning models that they seek\nto refine using data from other agents. The goal of the marketplace is to\nencourage participation for data sharing in the presence of such heterogeneity.\nOur market design approach for data sharing focuses on interim utility balance,\nwhere participants contribute and receive equitable utility from refinement of\ntheir models. We present such a market model for which we study computational\ncomplexity, solution existence, and approximation algorithms for welfare\nmaximization and core stability. We finally support our theoretical insights\nwith simulations on a mean estimation task inspired by road traffic delay\nestimation.", "field": "Computer Science", "categories": "cs.GT,cs.DS"}, {"arxiv_id": "2401.13054", "title": "Frustrated Random Walks: A Fast Method to Compute Node Distances on\n  Hypergraphs", "abstract": "A hypergraph is a generalization of a graph that arises naturally when\nattribute-sharing among entities is considered. Although a hypergraph can be\nconverted into a graph by expanding its hyperedges into fully connected\nsubgraphs, going the reverse way is computationally complex and NP-complete. We\ntherefore hypothesize that a hypergraph contains more information than a graph.\nIn addition, it is more convenient to manipulate a hypergraph directly, rather\nthan expand it into a graph. An open problem in hypergraphs is how to\naccurately and efficiently calculate their node distances. Estimating node\ndistances enables us to find a node's nearest neighbors, and perform label\npropagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this\npaper, we propose a novel approach based on random walks to achieve label\npropagation on hypergraphs. We estimate node distances as the expected hitting\ntimes of random walks. We note that simple random walks (SRW) cannot accurately\ndescribe highly complex real-world hypergraphs, which motivates us to introduce\nfrustrated random walks (FRW) to better describe them. We further benchmark our\nmethod against DeepWalk, and show that while the latter can achieve comparable\nresults, FRW has a distinct computational advantage in cases where the number\nof targets is fairly small. For such cases, we show that FRW runs in\nsignificantly shorter time than DeepWalk. Finally, we analyze the time\ncomplexity of our method, and show that for large and sparse hypergraphs, the\ncomplexity is approximately linear, rendering it superior to the DeepWalk\nalternative.", "field": "Computer Science", "categories": "cs.SI,cs.DM,cs.LG"}, {"arxiv_id": "2401.1306", "title": "TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced\n  Transformer-based Ensemble Approach for Qur'anic QA", "abstract": "In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks\nA and B. To address the challenge of low-resourced training data, we rely on\ntransfer learning together with a voting ensemble to improve prediction\nstability across multiple runs. Additionally, we employ different architectures\nand learning mechanisms for a range of Arabic pre-trained transformer-based\nmodels for both tasks. To identify unanswerable questions, we propose using a\nthresholding mechanism. Our top-performing systems greatly surpass the baseline\nperformance on the hidden split, achieving a MAP score of 25.05% for task A and\na partial Average Precision (pAP) of 57.11% for task B.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13062", "title": "Force sensing to reconstruct potential energy landscapes for cluttered\n  large obstacle traversal", "abstract": "Visual sensing of environmental geometry allows robots to use artificial\npotential fields to avoid sparse obstacles. Yet robots must further traverse\ncluttered large obstacles for applications like search and rescue through\nrubble and planetary exploration across Martain rocks. Recent studies\ndiscovered that to traverse cluttered large obstacles, multi-legged insects and\ninsect-inspired robots make strenuous transitions across locomotor modes with\nmajor changes in body orientation. When viewed on a potential energy landscape\nresulting from locomotor-obstacle physical interaction, these are\nbarrier-crossing transitions across landscape basins. This potential energy\nlandscape approach may provide a modeling framework for cluttered large\nobstacle traversal. Here, we take the next step toward this vision by testing\nwhether force sensing allows the reconstruction of the potential energy\nlandscape. We developed a cockroach-inspired, minimalistic robot capable of\nsensing obstacle contact forces and torques around its body as it propelled\nforward against a pair of cluttered grass-like beam obstacles. We performed\nmeasurements over many traverses with systematically varied body orientations.\nDespite the forces and torques not being fully conservative, they well-matched\nthe potential energy landscape gradients and the landscape reconstructed from\nthem well-matched ground truth. In addition, inspired by cockroach\nobservations, we found that robot head oscillation during traversal further\nimproved the accuracies of force sensing and landscape reconstruction. We still\nneed to study how to reconstruct landscape during a single traverse, as in\napplications, robots have little chance to use multiple traverses to sample the\nenvironment systematically and how to find landscape saddles for least-effort\ntransitions to traverse.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY,physics.bio-ph"}, {"arxiv_id": "2401.13066", "title": "Predictability and Randomness", "abstract": "Algorithmic theories of randomness can be related to theories of\nprobabilistic sequence prediction through the notion of a predictor, defined as\na function which supplies lower bounds on initial-segment probabilities of\ninfinite sequences. An infinite binary sequence $z$ is called unpredictable iff\nits initial-segment \"redundancy\" $n+\\log p(z(n))$ remains sufficiently low\nrelative to every effective predictor $p$. A predictor which maximizes the\ninitial-segment redundancy of a sequence is called optimal for that sequence.\nIt turns out that a sequence is random iff it is unpredictable. More generally,\na sequence is random relative to an arbitrary computable distribution iff the\ndistribution is itself an optimal predictor for the sequence. Here \"random\" can\nbe taken in the sense of Martin-L\\\"{o}f by using weak criteria of\neffectiveness, or in the sense of Schnorr by using stronger criteria of\neffectiveness. Under the weaker criteria of effectiveness it is possible to\nconstruct a universal predictor which is optimal for all infinite sequences.\nThis predictor assigns nonvanishing limit probabilities precisely to the\nrecursive sequences. Under the stronger criteria of effectiveness it is\npossible to establish a law of large numbers for sequences random relative to a\ncomputable distribution, which may be useful as a criterion of \"rationality\"\nfor methods of probabilistic prediction. A remarkable feature of effective\npredictors is the fact that they are expressible in the special form first\nproposed by Solomonoff. In this form sequence prediction reduces to assigning\nhigh probabilities to initial segments with short and/or numerous encodings.\nThis fact provides the link between theories of randomness and Solomonoff's\ntheory of prediction.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13068", "title": "Local Background Estimation for Improved Gas Plume Identification in\n  Hyperspectral Images", "abstract": "Deep learning identification models have shown promise for identifying gas\nplumes in Longwave IR hyperspectral images of urban scenes, particularly when a\nlarge library of gases are being considered. Because many gases have similar\nspectral signatures, it is important to properly estimate the signal from a\ndetected plume. Typically, a scene's global mean spectrum and covariance matrix\nare estimated to whiten the plume's signal, which removes the background's\nsignature from the gas signature. However, urban scenes can have many different\nbackground materials that are spatially and spectrally heterogeneous. This can\nlead to poor identification performance when the global background estimate is\nnot representative of a given local background material. We use image\nsegmentation, along with an iterative background estimation algorithm, to\ncreate local estimates for the various background materials that reside\nunderneath a gas plume. Our method outperforms global background estimation on\na set of simulated and real gas plumes. This method shows promise in increasing\ndeep learning identification confidence, while being simple and easy to tune\nwhen considering diverse plumes.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13076", "title": "SemanticSLAM: Learning based Semantic Map Construction and Robust Camera\n  Localization", "abstract": "Current techniques in Visual Simultaneous Localization and Mapping (VSLAM)\nestimate camera displacement by comparing image features of consecutive scenes.\nThese algorithms depend on scene continuity, hence requires frequent camera\ninputs. However, processing images frequently can lead to significant memory\nusage and computation overhead. In this study, we introduce SemanticSLAM, an\nend-to-end visual-inertial odometry system that utilizes semantic features\nextracted from an RGB-D sensor. This approach enables the creation of a\nsemantic map of the environment and ensures reliable camera localization.\nSemanticSLAM is scene-agnostic, which means it doesn't require retraining for\ndifferent environments. It operates effectively in indoor settings, even with\ninfrequent camera input, without prior knowledge. The strength of SemanticSLAM\nlies in its ability to gradually refine the semantic map and improve pose\nestimation. This is achieved by a convolutional long-short-term-memory\n(ConvLSTM) network, trained to correct errors during map construction. Compared\nto existing VSLAM algorithms, SemanticSLAM improves pose estimation by 17%. The\nresulting semantic map provides interpretable information about the environment\nand can be easily applied to various downstream tasks, such as path planning,\nobstacle avoidance, and robot navigation. The code will be publicly available\nat https://github.com/Leomingyangli/SemanticSLAM", "field": "Computer Science", "categories": "cs.RO,cs.CV"}, {"arxiv_id": "2401.13078", "title": "Open-Source, Cost-Aware Kinematically Feasible Planning for Mobile and\n  Surface Robotics", "abstract": "This paper introduces the Smac Planner, an openly available search-based\nplanning framework with multiple algorithm implementations including 2D-A*,\nHybrid-A*, and State Lattice planners. This work is motivated by the lack of\nperformant and available feasible planners for mobile and surface robotics\nresearch.\n  This paper contains three main contributions. First, it briefly describes a\nminimal open-source software framework where search-based planners may be\neasily added. Further, this paper characterizes new variations on the feasible\nplanners - dubbed Cost-Aware - specific to mobile roboticist's needs. This\nfills the gap of missing kinematically feasible implementations suitable for\nacademic, extension, and deployed use. Finally, we provide baseline\nbenchmarking against other standard planning frameworks.\n  Smac Planner has further significance by becoming the standard open-source\nplanning system within ROS 2's Nav2 framework which powers thousands of robots\nin research and industry.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13079", "title": "No AI After Auschwitz? Bridging AI and Memory Ethics in the Context of\n  Information Retrieval of Genocide-Related Information", "abstract": "The growing application of artificial intelligence (AI) in the field of\ninformation retrieval (IR) affects different domains, including cultural\nheritage. By facilitating organisation and retrieval of large volumes of\nheritage-related content, AI-driven IR systems inform users about a broad range\nof historical phenomena, including genocides (e.g. the Holocaust). However, it\nis currently unclear to what degree IR systems are capable of dealing with\nmultiple ethical challenges associated with the curation of genocide-related\ninformation. To address this question, this chapter provides an overview of\nethical challenges associated with the human curation of genocide-related\ninformation using a three-part framework inspired by Belmont criteria (i.e.\ncuration challenges associated with respect for individuals, beneficence and\njustice/fairness). Then, the chapter discusses to what degree the\nabove-mentioned challenges are applicable to the ways in which AI-driven IR\nsystems deal with genocide-related information and what can be the potential\nways of bridging AI and memory ethics in this context.", "field": "Computer Science", "categories": "cs.CY"}, {"arxiv_id": "2401.13081", "title": "Free Form Medical Visual Question Answering in Radiology", "abstract": "Visual Question Answering (VQA) in the medical domain presents a unique,\ninterdisciplinary challenge, combining fields such as Computer Vision, Natural\nLanguage Processing, and Knowledge Representation. Despite its importance,\nresearch in medical VQA has been scant, only gaining momentum since 2018.\nAddressing this gap, our research delves into the effective representation of\nradiology images and the joint learning of multimodal representations,\nsurpassing existing methods. We innovatively augment the SLAKE dataset,\nenabling our model to respond to a more diverse array of questions, not limited\nto the immediate content of radiology or pathology images. Our model achieves a\ntop-1 accuracy of 79.55\\% with a less complex architecture, demonstrating\ncomparable performance to current state-of-the-art models. This research not\nonly advances medical VQA but also opens avenues for practical applications in\ndiagnostic settings.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13082", "title": "PlaceFormer: Transformer-based Visual Place Recognition using\n  Multi-Scale Patch Selection and Fusion", "abstract": "Visual place recognition is a challenging task in the field of computer\nvision, and autonomous robotics and vehicles, which aims to identify a location\nor a place from visual inputs. Contemporary methods in visual place recognition\nemploy convolutional neural networks and utilize every region within the image\nfor the place recognition task. However, the presence of dynamic and\ndistracting elements in the image may impact the effectiveness of the place\nrecognition process. Therefore, it is meaningful to focus on task-relevant\nregions of the image for improved recognition. In this paper, we present\nPlaceFormer, a novel transformer-based approach for visual place recognition.\nPlaceFormer employs patch tokens from the transformer to create global image\ndescriptors, which are then used for image retrieval. To re-rank the retrieved\nimages, PlaceFormer merges the patch tokens from the transformer to form\nmulti-scale patches. Utilizing the transformer's self-attention mechanism, it\nselects patches that correspond to task-relevant areas in an image. These\nselected patches undergo geometric verification, generating similarity scores\nacross different patch sizes. Subsequently, spatial scores from each patch size\nare fused to produce a final similarity score. This score is then used to\nre-rank the images initially retrieved using global image descriptors.\nExtensive experiments on benchmark datasets demonstrate that PlaceFormer\noutperforms several state-of-the-art methods in terms of accuracy and\ncomputational efficiency, requiring less time and memory.", "field": "Computer Science", "categories": "cs.CV,cs.RO"}, {"arxiv_id": "2401.13085", "title": "IndiText Boost: Text Augmentation for Low Resource India Languages", "abstract": "Text Augmentation is an important task for low-resource languages. It helps\ndeal with the problem of data scarcity. A data augmentation strategy is used to\ndeal with the problem of data scarcity. Through the years, much work has been\ndone on data augmentation for the English language. In contrast, very less work\nhas been done on Indian languages. This is contrary to the fact that data\naugmentation is used to deal with data scarcity. In this work, we focus on\nimplementing techniques like Easy Data Augmentation, Back Translation,\nParaphrasing, Text Generation using LLMs, and Text Expansion using LLMs for\ntext classification on different languages. We focus on 6 Indian languages\nnamely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to\nour knowledge, no such work exists for text augmentation on Indian languages.\nWe carry out binary as well as multi-class text classification to make our\nresults more comparable. We get surprising results as basic data augmentation\ntechniques surpass LLMs.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.13086", "title": "Towards Trustable Language Models: Investigating Information Quality of\n  Large Language Models", "abstract": "Large language models (LLM) are generating information at a rapid pace,\nrequiring users to increasingly rely and trust the data. Despite remarkable\nadvances of LLM, Information generated by LLM is not completely trustworthy,\ndue to challenges in information quality. Specifically, integrity of\nInformation quality decreases due to unreliable, biased, tokenization during\npre-training of LLM. Moreover, due to decreased information quality issues, has\nled towards hallucination, fabricated information. Unreliable information can\nlead towards flawed decisions in businesses, which impacts economic activity.\nIn this work, we introduce novel mathematical information quality evaluation of\nLLM, we furthermore analyze and highlight information quality challenges,\nscaling laws to systematically scale language models.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.13087", "title": "Open-source data pipeline for street-view images: a case study on\n  community mobility during COVID-19 pandemic", "abstract": "Street View Images (SVI) are a common source of valuable data for\nresearchers. Researchers have used SVI data for estimating pedestrian volumes,\ndemographic surveillance, and to better understand built and natural\nenvironments in cityscapes. However, the most common source of publicly\navailable SVI data is Google Street View. Google Street View images are\ncollected infrequently, making temporal analysis challenging, especially in low\npopulation density areas. Our main contribution is the development of an\nopen-source data pipeline for processing 360-degree video recorded from a\ncar-mounted camera. The video data is used to generate SVIs, which then can be\nused as an input for temporal analysis. We demonstrate the use of the pipeline\nby collecting a SVI dataset over a 38-month longitudinal survey of Seattle, WA,\nUSA during the COVID-19 pandemic. The output of our pipeline is validated\nthrough statistical analyses of pedestrian traffic in the images. We confirm\nknown results in the literature and provide new insights into outdoor\npedestrian traffic patterns. This study demonstrates the feasibility and value\nof collecting and using SVI for research purposes beyond what is possible with\ncurrently available SVI data. Limitations and future improvements on the data\npipeline and case study are also discussed.", "field": "Computer Science", "categories": "cs.CV,stat.AP"}, {"arxiv_id": "2401.13096", "title": "Probabilistic Demand Forecasting with Graph Neural Networks", "abstract": "Demand forecasting is a prominent business use case that allows retailers to\noptimize inventory planning, logistics, and core business decisions. One of the\nkey challenges in demand forecasting is accounting for relationships and\ninteractions between articles. Most modern forecasting approaches provide\nindependent article-level predictions that do not consider the impact of\nrelated articles. Recent research has attempted addressing this challenge using\nGraph Neural Networks (GNNs) and showed promising results. This paper builds on\nprevious research on GNNs and makes two contributions. First, we integrate a\nGNN encoder into a state-of-the-art DeepAR model. The combined model produces\nprobabilistic forecasts, which are crucial for decision-making under\nuncertainty. Second, we propose to build graphs using article attribute\nsimilarity, which avoids reliance on a pre-defined graph structure. Experiments\non three real-world datasets show that the proposed approach consistently\noutperforms non-graph benchmarks. We also show that our approach produces\narticle embeddings that encode article similarity and demand dynamics and are\nuseful for other downstream business tasks beyond forecasting.", "field": "Computer Science", "categories": "cs.LG,stat.ML"}, {"arxiv_id": "2401.13097", "title": "Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in\n  Deep Learning Systems", "abstract": "Computer-based scene understanding has influenced fields ranging from urban\nplanning to autonomous vehicle performance, yet little is known about how well\nthese technologies work across social differences. We investigate the biases of\ndeep convolutional neural networks (dCNNs) in scene classification, using\nnearly one million images from global and US sources, including user-submitted\nhome photographs and Airbnb listings. We applied statistical models to quantify\nthe impact of socioeconomic indicators such as family income, Human Development\nIndex (HDI), and demographic factors from public data sources (CIA and US\nCensus) on dCNN performance. Our analyses revealed significant socioeconomic\nbias, where pretrained dCNNs demonstrated lower classification accuracy, lower\nclassification confidence, and a higher tendency to assign labels that could be\noffensive when applied to homes (e.g., \"ruin\", \"slum\"), especially in images\nfrom homes with lower socioeconomic status (SES). This trend is consistent\nacross two datasets of international images and within the diverse economic and\nracial landscapes of the United States. This research contributes to\nunderstanding biases in computer vision, emphasizing the need for more\ninclusive and representative training datasets. By mitigating the bias in the\ncomputer vision pipelines, we can ensure fairer and more equitable outcomes for\napplied computer vision, including home valuation and smart home security\nsystems. There is urgency in addressing these biases, which can significantly\nimpact critical decisions in urban development and resource allocation. Our\nfindings also motivate the development of AI systems that better understand and\nserve diverse communities, moving towards technology that equitably benefits\nall sectors of society.", "field": "Computer Science", "categories": "cs.CV,cs.AI,68-02,I.2.m"}, {"arxiv_id": "2401.13098", "title": "Gravity-Informed Deep Learning Framework for Predicting Ship Traffic\n  Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge", "abstract": "Invasive species in water bodies pose a major threat to the environment and\nbiodiversity globally. Due to increased transportation and trade, non-native\nspecies have been introduced to new environments, causing damage to ecosystems\nand leading to economic losses in agriculture, forestry, and fisheries.\nTherefore, there is a pressing need for risk assessment and management\ntechniques to mitigate the impact of these invasions. This study aims to\ndevelop a new physics-inspired model to forecast maritime shipping traffic and\nthus inform risk assessment of invasive species spread through global\ntransportation networks. Inspired by the gravity model for international\ntrades, our model considers various factors that influence the likelihood and\nimpact of vessel activities, such as shipping flux density, distance between\nports, trade flow, and centrality measures of transportation hubs.\nAdditionally, by analyzing the risk network of invasive species, we provide a\ncomprehensive framework for assessing the invasion threat level given a pair of\norigin and destination. Accordingly, this paper introduces transformers to\ngravity models to rebuild the short- and long-term dependencies that make the\nrisk analysis feasible. Thus, we introduce a physics-inspired framework that\nachieves an 89% segmentation accuracy for existing and non-existing\ntrajectories and an 84.8% accuracy for the number of vessels flowing between\nkey port areas, representing more than 10% improvement over the traditional\ndeep-gravity model. Along these lines, this research contributes to a better\nunderstanding of invasive species risk assessment. It allows policymakers,\nconservationists, and stakeholders to prioritize management actions by\nidentifying high-risk invasion pathways. Besides, our model is versatile and\ncan include new data sources, making it suitable for assessing species invasion\nrisks in a changing global landscape.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.SI,stat.AP"}, {"arxiv_id": "2401.13099", "title": "Sparse identification of nonlinear dynamics in the presence of library\n  and system uncertainty", "abstract": "The SINDy algorithm has been successfully used to identify the governing\nequations of dynamical systems from time series data. However, SINDy assumes\nthe user has prior knowledge of the variables in the system and of a function\nlibrary that can act as a basis for the system. In this paper, we demonstrate\non real world data how the Augmented SINDy algorithm outperforms SINDy in the\npresence of system variable uncertainty. We then show SINDy can be further\naugmented to perform robustly when both kinds of uncertainty are present.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.131", "title": "Bayesian sampling using interacting particles", "abstract": "Bayesian sampling is an important task in statistics and machine learning.\nOver the past decade, many ensemble-type sampling methods have been proposed.\nIn contrast to the classical Markov chain Monte Carlo methods, these new\nmethods deploy a large number of interactive samples, and the communication\nbetween these samples is crucial in speeding up the convergence. To justify the\nvalidity of these sampling strategies, the concept of interacting particles\nnaturally calls for the mean-field theory. The theory establishes a\ncorrespondence between particle interactions encoded in a set of coupled\nODEs/SDEs and a PDE that characterizes the evolution of the underlying\ndistribution. This bridges numerical algorithms with the PDE theory used to\nshow convergence in time. Many mathematical machineries are developed to\nprovide the mean-field analysis, and we showcase two such examples: The\ncoupling method and the compactness argument built upon the martingale\nstrategy. The former has been deployed to show the convergence of ensemble\nKalman sampler and ensemble Kalman inversion, and the latter will be shown to\nbe immensely powerful in proving the validity of the Vlasov-Boltzmann\nsimulator.", "field": "Computer Science", "categories": "math.NA,cs.NA,math.AP"}, {"arxiv_id": "2401.13103", "title": "Self-organizing Nervous Systems for Robot Swarms", "abstract": "The system architecture controlling a group of robots is generally set before\ndeployment and can be either centralized or decentralized. This dichotomy is\nhighly constraining, because decentralized systems are typically fully\nself-organized and therefore difficult to design analytically, whereas\ncentralized systems have single points of failure and limited scalability. To\naddress this dichotomy, we present the Self-organizing Nervous System (SoNS), a\nnovel robot swarm architecture based on self-organized hierarchy. The SoNS\napproach enables robots to autonomously establish, maintain, and reconfigure\ndynamic multi-level system architectures. For example, a robot swarm consisting\nof $n$ independent robots could transform into a single $n$-robot SoNS and then\ninto several independent smaller SoNSs, where each SoNS uses a temporary and\ndynamic hierarchy. Leveraging the SoNS approach, we show that sensing,\nactuation, and decision-making can be coordinated in a locally centralized way,\nwithout sacrificing the benefits of scalability, flexibility, and fault\ntolerance, for which swarm robotics is usually studied. In several\nproof-of-concept robot missions -- including binary decision-making and\nsearch-and-rescue -- we demonstrate that the capabilities of the SoNS approach\ngreatly advance the state of the art in swarm robotics. The missions are\nconducted with a real heterogeneous aerial-ground robot swarm, using a\ncustom-developed quadrotor platform. We also demonstrate the scalability of the\nSoNS approach in swarms of up to 250 robots in a physics-based simulator, and\ndemonstrate several types of system fault tolerance in simulation and reality.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13105", "title": "Smart Grids: A Comprehensive Survey of Challenges, Industry\n  Applications, and Future Trends", "abstract": "With the increased energy demands of the 21st century, there is a clear need\nfor developing a more sustainable method of energy generation, distribution,\nand transmission. The popularity of Smart Grid continues to grow as it presents\nits benefits, including interconnectivity, improved efficiency, the ability to\nintegrate renewable energy sources, and many more. However, it is not without\nits challenges. This survey aims to provide an introductory background of smart\ngrids, detail some of the main aspects and current challenges, and review the\nmost recent papers and proposed solutions. It will also highlight the current\nstate of implementation of the smart grid by describing various prototypes, as\nwell as various countries and continents implementation plans and projects.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13107", "title": "Development of a Causal Model for Improving Rural Seniors'\n  Accessibility: Data Evidences", "abstract": "Seniors residing in rural areas often encounter limited accessibility to\nopportunities, resources, and services. This paper introduces a model proposing\nthat both aging and rural residency are factors contributing to the restricted\naccessibility faced by rural seniors. Leveraging data from the 2017 National\nHousehold Travel Survey, the study examines three hypotheses pertaining to this\ncausal model. Multiple causal pathways emerge in the data analysis, with\nmobility identified as a mediator in one of them. The study further identifies\nspecific challenges faced by rural seniors, such as the reduced accessibility\nin reaching medical services and assisting others. These challenges stem\nprimarily from aging and geographic obstacles that not only diminish their\nwillingness to travel but also restrict more in the group from choosing\ntransportation modes with higher mobility. The insights gained from this study\nserve as a foundation for devising effective methods to enhance transportation\naccessibility for seniors in rural areas.", "field": "Computer Science", "categories": "cs.SI"}, {"arxiv_id": "2401.1311", "title": "XAI for All: Can Large Language Models Simplify Explainable AI?", "abstract": "The field of Explainable Artificial Intelligence (XAI) often focuses on users\nwith a strong technical background, making it challenging for non-experts to\nunderstand XAI methods. This paper presents \"x-[plAIn]\", a new approach to make\nXAI more accessible to a wider audience through a custom Large Language Model\n(LLM), developed using ChatGPT Builder. Our goal was to design a model that can\ngenerate clear, concise summaries of various XAI methods, tailored for\ndifferent audiences, including business professionals and academics. The key\nfeature of our model is its ability to adapt explanations to match each\naudience group's knowledge level and interests. Our approach still offers\ntimely insights, facilitating the decision-making process by the end users.\nResults from our use-case studies show that our model is effective in providing\neasy-to-understand, audience-specific explanations, regardless of the XAI\nmethod used. This adaptability improves the accessibility of XAI, bridging the\ngap between complex AI technologies and their practical applications. Our\nfindings indicate a promising direction for LLMs in making advanced AI concepts\nmore accessible to a diverse range of users.", "field": "Computer Science", "categories": "cs.AI,cs.HC"}, {"arxiv_id": "2401.13112", "title": "DISCOUNT: Distributional Counterfactual Explanation With Optimal\n  Transport", "abstract": "Counterfactual Explanations (CE) is the de facto method for providing insight\nand interpretability in black-box decision-making models by identifying\nalternative input instances that lead to different outcomes. This paper extends\nthe concept of CEs to a distributional context, broadening the scope from\nindividual data points to entire input and output distributions, named\nDistributional Counterfactual Explanation (DCE). In DCE, our focus shifts to\nanalyzing the distributional properties of the factual and counterfactual,\ndrawing parallels to the classical approach of assessing individual instances\nand their resulting decisions. We leverage Optimal Transport (OT) to frame a\nchance-constrained optimization problem, aiming to derive a counterfactual\ndistribution that closely aligns with its factual counterpart, substantiated by\nstatistical confidence. Our proposed optimization method, DISCOUNT,\nstrategically balances this confidence across both input and output\ndistributions. This algorithm is accompanied by an analysis of its convergence\nrate. The efficacy of our proposed method is substantiated through a series of\nillustrative case studies, highlighting its potential in providing deep\ninsights into decision-making models.", "field": "Computer Science", "categories": "cs.AI,stat.ML"}, {"arxiv_id": "2401.13115", "title": "Contractive Diffusion Probabilistic Models", "abstract": "Diffusion probabilistic models (DPMs) have emerged as a promising technology\nin generative modeling. The success of DPMs relies on two ingredients: time\nreversal of Markov diffusion processes and score matching. Most existing work\nimplicitly assumes that score matching is close to perfect, while this\nassumption is questionable. In view of possibly unguaranteed score matching, we\npropose a new criterion -- the contraction of backward sampling in the design\nof DPMs. This leads to a novel class of contractive DPMs (CDPMs), including\ncontractive Ornstein-Uhlenbeck (OU) processes and contractive sub-variance\npreserving (sub-VP) stochastic differential equations (SDEs). The key insight\nis that the contraction in the backward process narrows score matching errors,\nas well as discretization error. Thus, the proposed CDPMs are robust to both\nsources of error. Our proposal is supported by theoretical results, and is\ncorroborated by experiments. Notably, contractive sub-VP shows the best\nperformance among all known SDE-based DPMs on the CIFAR-10 dataset.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13127", "title": "Generalization of Heterogeneous Multi-Robot Policies via Awareness and\n  Communication of Capabilities", "abstract": "Recent advances in multi-agent reinforcement learning (MARL) are enabling\nimpressive coordination in heterogeneous multi-robot teams. However, existing\napproaches often overlook the challenge of generalizing learned policies to\nteams of new compositions, sizes, and robots. While such generalization might\nnot be important in teams of virtual agents that can retrain policies\non-demand, it is pivotal in multi-robot systems that are deployed in the\nreal-world and must readily adapt to inevitable changes. As such, multi-robot\npolicies must remain robust to team changes -- an ability we call adaptive\nteaming. In this work, we investigate if awareness and communication of robot\ncapabilities can provide such generalization by conducting detailed experiments\ninvolving an established multi-robot test bed. We demonstrate that shared\ndecentralized policies, that enable robots to be both aware of and communicate\ntheir capabilities, can achieve adaptive teaming by implicitly capturing the\nfundamental relationship between collective capabilities and effective\ncoordination. Videos of trained policies can be viewed at:\nhttps://sites.google.com/view/cap-comm", "field": "Computer Science", "categories": "cs.RO,cs.MA"}, {"arxiv_id": "2401.13128", "title": "Polynomial Lyapunov Functions and Invariant Sets from a New Hierarchy of\n  Quadratic Lyapunov Functions for LTV Systems", "abstract": "We introduce a new class of quadratic functions based on a hierarchy of\nlinear time-varying (LTV) dynamical systems. These quadratic functions in the\nhigher order space can be also seen as a non-homogeneous polynomial Lyapunov\nfunctions for the original system, i.e the first system in the hierarchy. These\nnon-homogeneous polynomials are used to obtain accurate outer approximation for\nthe reachable set given the initial condition and less conservative bounds for\nthe impulse response peak of linear, possibly time-varying systems. In\naddition, we pose an extension to the presented approach to construct invariant\nsets that are not necessarily Lyapunov functions. The introduced methods are\nbased on elementary linear systems theory and offer very much flexibility in\ndefining arbitrary polynomial Lyapunov functions and invariant sets for LTV\nsystems.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13129", "title": "Seed-Guided Fine-Grained Entity Typing in Science and Engineering\n  Domains", "abstract": "Accurately typing entity mentions from text segments is a fundamental task\nfor various natural language processing applications. Many previous approaches\nrely on massive human-annotated data to perform entity typing. Nevertheless,\ncollecting such data in highly specialized science and engineering domains\n(e.g., software engineering and security) can be time-consuming and costly,\nwithout mentioning the domain gaps between training and inference data if the\nmodel needs to be applied to confidential datasets. In this paper, we study the\ntask of seed-guided fine-grained entity typing in science and engineering\ndomains, which takes the name and a few seed entities for each entity type as\nthe only supervision and aims to classify new entity mentions into both seen\nand unseen types (i.e., those without seed entities). To solve this problem, we\npropose SEType which first enriches the weak supervision by finding more\nentities for each seen type from an unlabeled corpus using the contextualized\nrepresentations of pre-trained language models. It then matches the enriched\nentities to unlabeled text to get pseudo-labeled samples and trains a textual\nentailment model that can make inferences for both seen and unseen types.\nExtensive experiments on two datasets covering four domains demonstrate the\neffectiveness of SEType in comparison with various baselines.", "field": "Computer Science", "categories": "cs.CL,cs.SE"}, {"arxiv_id": "2401.13133", "title": "Analyzing COVID-19 Vaccination Sentiments in Nigerian Cyberspace:\n  Insights from a Manually Annotated Twitter Dataset", "abstract": "Numerous successes have been achieved in combating the COVID-19 pandemic,\ninitially using various precautionary measures like lockdowns, social\ndistancing, and the use of face masks. More recently, various vaccinations have\nbeen developed to aid in the prevention or reduction of the severity of the\nCOVID-19 infection. Despite the effectiveness of the precautionary measures and\nthe vaccines, there are several controversies that are massively shared on\nsocial media platforms like Twitter. In this paper, we explore the use of\nstate-of-the-art transformer-based language models to study people's acceptance\nof vaccines in Nigeria. We developed a novel dataset by crawling multi-lingual\ntweets using relevant hashtags and keywords. Our analysis and visualizations\nrevealed that most tweets expressed neutral sentiments about COVID-19 vaccines,\nwith some individuals expressing positive views, and there was no strong\npreference for specific vaccine types, although Moderna received slightly more\npositive sentiment. We also found out that fine-tuning a pre-trained LLM with\nan appropriate dataset can yield competitive results, even if the LLM was not\ninitially pre-trained on the specific language of that dataset.", "field": "Computer Science", "categories": "cs.CL,cs.SI"}, {"arxiv_id": "2401.13136", "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in\n  Multilingual Contexts", "abstract": "As the influence of large language models (LLMs) spans across global\ncommunities, their safety challenges in multilingual settings become paramount\nfor alignment research. This paper examines the variations in safety challenges\nfaced by LLMs across different languages and discusses approaches to\nalleviating such concerns. By comparing how state-of-the-art LLMs respond to\nthe same set of malicious prompts written in higher- vs. lower-resource\nlanguages, we observe that (1) LLMs tend to generate unsafe responses much more\noften when a malicious prompt is written in a lower-resource language, and (2)\nLLMs tend to generate more irrelevant responses to malicious prompts in\nlower-resource languages. To understand where the discrepancy can be\nattributed, we study the effect of instruction tuning with reinforcement\nlearning from human feedback (RLHF) or supervised finetuning (SFT) on the\nHH-RLHF dataset. Surprisingly, while training with high-resource languages\nimproves model alignment, training in lower-resource languages yields minimal\nimprovement. This suggests that the bottleneck of cross-lingual alignment is\nrooted in the pretraining stage. Our findings highlight the challenges in\ncross-lingual LLM safety, and we hope they inform future research in this\ndirection.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13138", "title": "Visibility into AI Agents", "abstract": "Increased delegation of commercial, scientific, governmental, and personal\nactivities to AI agents -- systems capable of pursuing complex goals with\nlimited supervision -- may exacerbate existing societal risks and introduce new\nrisks. Understanding and mitigating these risks involves critically evaluating\nexisting governance structures, revising and adapting these structures where\nneeded, and ensuring accountability of key stakeholders. Information about\nwhere, why, how, and by whom certain AI agents are used, which we refer to as\n\\textbf{visibility}, is critical to these objectives. In this paper, we assess\nthree categories of measures to increase visibility into AI agents:\n\\textbf{agent identifiers}, \\textbf{real-time monitoring}, and \\textbf{activity\nlogging}. For each, we outline potential implementations that vary in\nintrusiveness and informativeness. We analyze how the measures apply across a\nspectrum of centralized through decentralized deployment contexts, accounting\nfor various actors in the supply chain including hardware and software service\nproviders. Finally, we discuss the implications of our measures for privacy and\nconcentration of power. Further work into understanding the measures and\nmitigating their negative impacts can help to build a foundation for the\ngovernance of AI agents.", "field": "Computer Science", "categories": "cs.CY,cs.AI"}, {"arxiv_id": "2401.13142", "title": "Unsocial Intelligence: a Pluralistic, Democratic, and Participatory\n  Investigation of AGI Discourse", "abstract": "Dreams of machines that rival human intelligence have shaped the field of AI\nsince its inception. Yet there remains no agreed-upon conception of what\nhuman-level AI or artificial general intelligence (AGI) means. We investigate\nkey social, political, and ethical assumptions made by influential conceptions\nof AGI and human-level AI. We then draw on feminist, STS, and social science\nscholarship on the political and social character of intelligence in both\nhumans and machines to defend a pluralistic, democratic, and participatory\nconception of the topic. We argue that framing AGI or human-level AI as a\ntechnical or value-neutral topic leads to political, ethical, and epistemic\nharm. AGI should not be developed without explicit attention to the values they\nencode, the people they include or exclude, and a view toward epistemic\njustice.", "field": "Computer Science", "categories": "cs.CY"}, {"arxiv_id": "2401.13148", "title": "NLBAC: A Neural Ordinary Differential Equations-based Framework for\n  Stable and Safe Reinforcement Learning", "abstract": "Reinforcement learning (RL) excels in applications such as video games and\nrobotics, but ensuring safety and stability remains challenging when using RL\nto control real-world systems where using model-free algorithms suffering from\nlow sample efficiency might be prohibitive. This paper first provides safety\nand stability definitions for the RL system, and then introduces a Neural\nordinary differential equations-based Lyapunov-Barrier Actor-Critic (NLBAC)\nframework that leverages Neural Ordinary Differential Equations (NODEs) to\napproximate system dynamics and integrates the Control Barrier Function (CBF)\nand Control Lyapunov Function (CLF) frameworks with the actor-critic method to\nassist in maintaining the safety and stability for the system. Within this\nframework, we employ the augmented Lagrangian method to update the RL-based\ncontroller parameters. Additionally, we introduce an extra backup controller in\nsituations where CBF constraints for safety and the CLF constraint for\nstability cannot be satisfied simultaneously. Simulation results demonstrate\nthat the framework leads the system to approach the desired state and allows\nfewer violations of safety constraints with better sample efficiency compared\nto other methods.", "field": "Computer Science", "categories": "cs.LG,cs.RO,cs.SY,eess.SY"}, {"arxiv_id": "2401.1315", "title": "Automated Programmatic Performance Analysis of Parallel Programs", "abstract": "Developing efficient parallel applications is critical to advancing\nscientific development but requires significant performance analysis and\noptimization. Performance analysis tools help developers manage the increasing\ncomplexity and scale of performance data, but often rely on the user to\nmanually explore low-level data and are rigid in how the data can be\nmanipulated. We propose a Python-based API, Chopper, which provides high-level\nand flexible performance analysis for both single and multiple executions of\nparallel applications. Chopper facilitates performance analysis and reduces\ndeveloper effort by providing configurable high-level methods for common\nperformance analysis tasks such as calculating load imbalance, hot paths,\nscalability bottlenecks, correlation between metrics and CCT nodes, and causes\nof performance variability within a robust and mature Python environment that\nprovides fluid access to lower-level data manipulations. We demonstrate how\nChopper allows developers to quickly and succinctly explore performance and\nidentify issues across applications such as AMG, Laghos, LULESH, Quicksilver\nand Tortuga.", "field": "Computer Science", "categories": "cs.DC,cs.PF"}, {"arxiv_id": "2401.13154", "title": "MATRYOSHKA: Non-Exclusive Memory Tiering via Transactional Page\n  Migration", "abstract": "With the advent of byte-addressable memory devices, such as CXL memory,\npersistent memory, and storage-class memory, tiered memory systems have become\na reality. Page migration is the de facto method within operating systems for\nmanaging tiered memory. It aims to bring hot data whenever possible into fast\nmemory to optimize the performance of data accesses while using slow memory to\naccommodate data spilled from fast memory. While the existing research has\ndemonstrated the effectiveness of various optimizations on page migration, it\nfalls short of addressing a fundamental question: Is exclusive memory tiering,\nin which a page is either present in fast memory or slow memory, but not both\nsimultaneously, the optimal strategy for tiered memory management?\n  We demonstrate that page migration-based exclusive memory tiering suffers\nsignificant performance degradation when fast memory is under pressure. In this\npaper, we propose non-exclusive memory tiering, a page management strategy that\nretains a copy of pages recently promoted from slow memory to fast memory to\nmitigate memory thrashing. To enable non-exclusive memory tiering, we develop\nMATRYOSHKA, a new mechanism that features transactional page migration and page\nshadowing. MATRYOSHKA removes page migration off the program's critical path\nand makes migration asynchronous. Evaluations with microbenchmarks and\nrealworld applications show that MATRYOSHKA achieves 6x performance improvement\nover the state-of-the-art transparent page placement (TPP) approach under\nmemory pressure. We also compare MATRYOSHKA with a recently proposed\nsampling-based migration approach and demonstrate MATRYOSHKA's strengths and\npotential weaknesses in various scenarios. Through the evaluations, we discover\na serious issue facing all tested approaches, unfortunately including\nMATRYOSHKA, and call for further research on tiered memory-aware memory\nallocation.", "field": "Computer Science", "categories": "cs.OS"}, {"arxiv_id": "2401.13157", "title": "Time-Aware Knowledge Representations of Dynamic Objects with\n  Multidimensional Persistence", "abstract": "Learning time-evolving objects such as multivariate time series and dynamic\nnetworks requires the development of novel knowledge representation mechanisms\nand neural network architectures, which allow for capturing implicit\ntime-dependent information contained in the data. Such information is typically\nnot directly observed but plays a key role in the learning task performance. In\nturn, lack of time dimension in knowledge encoding mechanisms for\ntime-dependent data leads to frequent model updates, poor learning performance,\nand, as a result, subpar decision-making. Here we propose a new approach to a\ntime-aware knowledge representation mechanism that notably focuses on implicit\ntime-dependent topological information along multiple geometric dimensions. In\nparticular, we propose a new approach, named \\textit{Temporal MultiPersistence}\n(TMP), which produces multidimensional topological fingerprints of the data by\nusing the existing single parameter topological summaries. The main idea behind\nTMP is to merge the two newest directions in topological representation\nlearning, that is, multi-persistence which simultaneously describes data shape\nevolution along multiple key parameters, and zigzag persistence to enable us to\nextract the most salient data shape information over time. We derive\ntheoretical guarantees of TMP vectorizations and show its utility, in\napplication to forecasting on benchmark traffic flow, Ethereum blockchain, and\nelectrocardiogram datasets, demonstrating the competitive performance,\nespecially, in scenarios of limited data records. In addition, our TMP method\nimproves the computational efficiency of the state-of-the-art multipersistence\nsummaries up to 59.5 times.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.1316", "title": "SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced\n  Token Detection", "abstract": "Pre-training large language models is known to be extremely resource\nintensive and often times inefficient, under-utilizing the information\nencapsulated in the training text sequences. In this paper, we present SpacTor,\na new training procedure consisting of (1) a hybrid objective combining span\ncorruption (SC) and token replacement detection (RTD), and (2) a two-stage\ncurriculum that optimizes the hybrid objective over the initial $\\tau$\niterations, then transitions to standard SC loss. We show empirically that the\neffectiveness of the hybrid objective is tied to the two-stage pre-training\nschedule, and provide extensive analysis on why this is the case. In our\nexperiments with encoder-decoder architectures (T5) on a variety of NLP tasks,\nSpacTor-T5 yields the same downstream performance as standard SC pre-training,\nwhile enabling a 50% reduction in pre-training iterations and 40% reduction in\ntotal FLOPs. Alternatively, given the same amount of computing budget, we find\nthat SpacTor results in significantly improved downstream benchmark\nperformance.", "field": "Computer Science", "categories": "cs.LG,cs.CL"}, {"arxiv_id": "2401.13161", "title": "A Generalized Multiscale Bundle-Based Hyperspectral Sparse Unmixing\n  Algorithm", "abstract": "In hyperspectral sparse unmixing, a successful approach employs spectral\nbundles to address the variability of the endmembers in the spatial domain.\nHowever, the regularization penalties usually employed aggregate substantial\ncomputational complexity, and the solutions are very noise-sensitive. We\ngeneralize a multiscale spatial regularization approach to solve the unmixing\nproblem by incorporating group sparsity-inducing mixed norms. Then, we propose\na noise-robust method that can take advantage of the bundle structure to deal\nwith endmember variability while ensuring inter- and intra-class sparsity in\nabundance estimation with reasonable computational cost. We also present a\ngeneral heuristic to select the \\emph{most representative} abundance estimation\nover multiple runs of the unmixing process, yielding a solution that is robust\nand highly reproducible. Experiments illustrate the robustness and consistency\nof the results when compared to related methods.", "field": "Computer Science", "categories": "cs.CV,eess.SP"}, {"arxiv_id": "2401.13165", "title": "Misgendering and Assuming Gender in Machine Translation when Working\n  with Low-Resource Languages", "abstract": "This chapter focuses on gender-related errors in machine translation (MT) in\nthe context of low-resource languages. We begin by explaining what low-resource\nlanguages are, examining the inseparable social and computational factors that\ncreate such linguistic hierarchies. We demonstrate through a case study of our\nmother tongue Bengali, a global language spoken by almost 300 million people\nbut still classified as low-resource, how gender is assumed and inferred in\ntranslations to and from the high(est)-resource English when no such\ninformation is provided in source texts. We discuss the postcolonial and\nsocietal impacts of such errors leading to linguistic erasure and\nrepresentational harms, and conclude by discussing potential solutions towards\nuplifting languages by providing them more agency in MT conversations.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13169", "title": "A Repository-Level Dataset For Detecting, Classifying and Repairing\n  Software Vulnerabilities", "abstract": "Open-Source Software (OSS) vulnerabilities bring great challenges to the\nsoftware security and pose potential risks to our society. Enormous efforts\nhave been devoted into automated vulnerability detection, among which deep\nlearning (DL)-based approaches have proven to be the most effective. However,\nthe current labeled data present the following limitations: (1) \\textbf{Tangled\nPatches}: Developers may submit code changes unrelated to vulnerability fixes\nwithin patches, leading to tangled patches. (2) \\textbf{Lacking\nInter-procedural Vulnerabilities}: The existing vulnerability datasets\ntypically contain function-level and file-level vulnerabilities, ignoring the\nrelations between functions, thus rendering the approaches unable to detect the\ninter-procedural vulnerabilities. (3) \\textbf{Outdated Patches}: The existing\ndatasets usually contain outdated patches, which may bias the model during\ntraining.\n  To address the above limitations, in this paper, we propose an automated data\ncollection framework and construct the first repository-level high-quality\nvulnerability dataset named \\textbf{ReposVul}. The proposed framework mainly\ncontains three modules: (1) A vulnerability untangling module, aiming at\ndistinguishing vulnerability-fixing related code changes from tangled patches,\nin which the Large Language Models (LLMs) and static analysis tools are jointly\nemployed. (2) A multi-granularity dependency extraction module, aiming at\ncapturing the inter-procedural call relationships of vulnerabilities, in which\nwe construct multiple-granularity information for each vulnerability patch,\nincluding repository-level, file-level, function-level, and line-level. (3) A\ntrace-based filtering module, aiming at filtering the outdated patches, which\nleverages the file path trace-based filter and commit time trace-based filter\nto construct an up-to-date dataset.", "field": "Computer Science", "categories": "cs.CR,cs.SE"}, {"arxiv_id": "2401.1317", "title": "CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert\n  Judgments For Open-Domain Question Answering", "abstract": "Question answering (QA) can only make progress if we know if an answer is\ncorrect, but for many of the most challenging and interesting QA examples,\ncurrent evaluation metrics to determine answer equivalence (AE) often do not\nalign with human judgments, particularly more verbose, free-form answers from\nlarge language models (LLM). There are two challenges: a lack of data and that\nmodels are too big: LLM-based scorers can correlate better with human judges,\nbut this task has only been tested on limited QA datasets, and even when\navailable, update of the model is limited because LLMs are large and often\nexpensive. We rectify both of these issues by providing clear and consistent\nguidelines for evaluating AE in machine QA adopted from professional human QA\ncontests. We also introduce a combination of standard evaluation and a more\nefficient, robust, and lightweight discriminate AE classifier-based matching\nmethod (CFMatch, smaller than 1 MB), trained and validated to more accurately\nevaluate answer correctness in accordance with adopted expert AE rules that are\nmore aligned with human judgments.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13171", "title": "Compositional Generative Inverse Design", "abstract": "Inverse design, where we seek to design input variables in order to optimize\nan underlying objective function, is an important problem that arises across\nfields such as mechanical engineering to aerospace engineering. Inverse design\nis typically formulated as an optimization problem, with recent works\nleveraging optimization across learned dynamics models. However, as models are\noptimized they tend to fall into adversarial modes, preventing effective\nsampling. We illustrate that by instead optimizing over the learned energy\nfunction captured by the diffusion model, we can avoid such adversarial\nexamples and significantly improve design performance. We further illustrate\nhow such a design system is compositional, enabling us to combine multiple\ndifferent diffusion models representing subcomponents of our desired system to\ndesign systems with every specified component. In an N-body interaction task\nand a challenging 2D multi-airfoil design task, we demonstrate that by\ncomposing the learned diffusion model at test time, our method allows us to\ndesign initial states and boundary shapes that are more complex than those in\nthe training data. Our method outperforms state-of-the-art neural inverse\ndesign method by an average of 41.5% in prediction MAE and 14.3% in design\nobjective for the N-body dataset and discovers formation flying to minimize\ndrag in the multi-airfoil design task. Project website and code can be found at\nhttps://github.com/AI4Science-WestlakeU/cindm.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CE"}, {"arxiv_id": "2401.13172", "title": "ADMap: Anti-disturbance framework for reconstructing online vectorized\n  HD map", "abstract": "In the field of autonomous driving, online high-definition (HD) map\nreconstruction is crucial for planning tasks. Recent research has developed\nseveral high-performance HD map reconstruction models to meet this necessity.\nHowever, the point sequences within the instance vectors may be jittery or\njagged due to prediction bias, which can impact subsequent tasks. Therefore,\nthis paper proposes the Anti-disturbance Map reconstruction framework (ADMap).\nTo mitigate point-order jitter, the framework consists of three modules:\nMulti-Scale Perception Neck, Instance Interactive Attention (IIA), and Vector\nDirection Difference Loss (VDDL). By exploring the point-order relationships\nbetween and within instances in a cascading manner, the model can monitor the\npoint-order prediction process more effectively. ADMap achieves\nstate-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive\nresults demonstrate its ability to produce stable and reliable map elements in\ncomplex and changing driving scenarios. Code and more demos are available at\nhttps://github.com/hht1996ok/ADMap.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13174", "title": "Boundary and Relation Distillation for Semantic Segmentation", "abstract": "Recently, it has been revealed that small semantic segmentation (SS) models\nexhibit a tendency to make errors in maintaining boundary region completeness\nand preserving target region connectivity, despite their effective segmentation\nof the main object regions. To address these errors, we propose a targeted\nboundary and relation distillation (BRD) strategy using knowledge distillation\nfrom large teacher models to small student models. Specifically, the boundary\ndistillation extracts explicit object boundaries from the hierarchical feature\nmaps of the backbone network, subsequently enhancing the student model's mask\nquality in boundary regions. Concurrently, the relation distillation transfers\nimplicit relations from the teacher model to the student model using\npixel-level self-relation as a bridge, ensuring that the student's mask has\nstrong target region connectivity. The proposed BRD is designed concretely for\nSS and is characterized by simplicity and efficiency. Through experimental\nevaluations on multiple SS datasets, including Pascal VOC 2012, Cityscapes,\nADE20K, and COCO-Stuff 10K, we demonstrated that BRD significantly surpasses\nthe current methods without increasing the inference costs, generating crisp\nregion boundaries and smooth connecting regions that are challenging for small\nmodels.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13177", "title": "Deep Learning Model Reuse in the HuggingFace Community: Challenges,\n  Benefit and Trends", "abstract": "The ubiquity of large-scale Pre-Trained Models (PTMs) is on the rise,\nsparking interest in model hubs, and dedicated platforms for hosting PTMs.\nDespite this trend, a comprehensive exploration of the challenges that users\nencounter and how the community leverages PTMs remains lacking. To address this\ngap, we conducted an extensive mixed-methods empirical study by focusing on\ndiscussion forums and the model hub of HuggingFace, the largest public model\nhub. Based on our qualitative analysis, we present a taxonomy of the challenges\nand benefits associated with PTM reuse within this community. We then conduct a\nquantitative study to track model-type trends and model documentation evolution\nover time. Our findings highlight prevalent challenges such as limited guidance\nfor beginner users, struggles with model output comprehensibility in training\nor inference, and a lack of model understanding. We also identified interesting\ntrends among models where some models maintain high upload rates despite a\ndecline in topics related to them. Additionally, we found that despite the\nintroduction of model documentation tools, its quantity has not increased over\ntime, leading to difficulties in model comprehension and selection among users.\nOur study sheds light on new challenges in reusing PTMs that were not reported\nbefore and we provide recommendations for various stakeholders involved in PTM\nreuse.", "field": "Computer Science", "categories": "cs.SE,cs.CY,cs.LG"}, {"arxiv_id": "2401.13178", "title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents", "abstract": "Evaluating large language models (LLMs) as general-purpose agents is\nessential for understanding their capabilities and facilitating their\nintegration into practical applications. However, the evaluation process\npresents substantial challenges. A primary obstacle is the benchmarking of\nagent performance across diverse scenarios within a unified framework,\nespecially in maintaining partially-observable environments and ensuring\nmulti-round interactions. Moreover, current evaluation frameworks mostly focus\non the final success rate, revealing few insights during the process and\nfailing to provide a deep understanding of the model abilities. To address\nthese challenges, we introduce AgentBoard, a pioneering comprehensive benchmark\nand accompanied open-source evaluation framework tailored to analytical\nevaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric\nthat captures incremental advancements as well as a comprehensive evaluation\ntoolkit that features easy assessment of agents for multi-faceted analysis\nthrough interactive visualization. This not only sheds light on the\ncapabilities and limitations of LLM agents but also propels the\ninterpretability of their performance to the forefront. Ultimately, AgentBoard\nserves as a significant step towards demystifying agent behaviors and\naccelerating the development of stronger LLM agents.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.13182", "title": "A Market-Clearing-based Sensitivity Model for Locational Marginal and\n  Average Carbon Emission", "abstract": "This letter proposes a market-clearing-based locational marginal carbon\nemission (LMCE) metric to assess the marginal carbon emission effect of nodal\nload demand. Unlike the prevalent carbon emission flow (CEF) method that relies\non a hypothetical power-flow tracking process, the proposed LMCE metric depends\non a novel sensitivity analysis of market-clearing results, capable of\nrevealing both energy-dependent and network-dependent impacts on emissions.\nAdditionally, we introduce a locational average carbon emission (LACE) metric,\nderived from LMCE, to effectively measure the general emission effect. It\noffers insights into demand-side carbon emission effects, such as a negative\nLMCE and LACE indicating emission reduction even as load increases. It can also\nprevent excessive demand-side emission allocations. Overall, the proposed\nmethod provides a clear perspective for the ongoing decarbonization policies.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13185", "title": "Shortcutting Cross-Validation: Efficiently Deriving Column-Wise Centered\n  and Scaled Training Set $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and\n  $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ Without Full Recomputation of Matrix\n  Products or Statistical Moments", "abstract": "Cross-validation is a widely used technique for assessing the performance of\npredictive models on unseen data. Many predictive models, such as Kernel-Based\nPartial Least-Squares (PLS) models, require the computation of\n$\\mathbf{X}^{\\mathbf{T}}\\mathbf{X}$ and $\\mathbf{X}^{\\mathbf{T}}\\mathbf{Y}$\nusing only training set samples from the input and output matrices,\n$\\mathbf{X}$ and $\\mathbf{Y}$, respectively. In this work, we present three\nalgorithms that efficiently compute these matrices. The first one allows no\ncolumn-wise preprocessing. The second one allows column-wise centering around\nthe training set means. The third one allows column-wise centering and\ncolumn-wise scaling around the training set means and standard deviations.\nDemonstrating correctness and superior computational complexity, they offer\nsignificant cross-validation speedup compared with straight-forward\ncross-validation and previous work on fast cross-validation - all without data\nleakage. Their suitability for parallelization is highlighted with an\nopen-source Python implementation combining our algorithms with Improved Kernel\nPLS.", "field": "Computer Science", "categories": "cs.LG,cs.DS"}, {"arxiv_id": "2401.1319", "title": "A Comparison Between Lie Group- and Lie Algebra- Based Potential\n  Functions for Geometric Impedance Control", "abstract": "In this paper, a comparison analysis between geometric impedance controls\n(GICs) derived from two different potential functions on SE(3) for robotic\nmanipulators is presented. The first potential function is defined on the Lie\ngroup, utilizing the Frobenius norm of the configuration error matrix. The\nsecond potential function is defined utilizing the Lie algebra, i.e., log-map\nof the configuration error. Using a differential geometric approach, the\ndetailed derivation of the distance metric and potential function on SE(3) is\nintroduced. The GIC laws are respectively derived from the two potential\nfunctions, followed by extensive comparison analyses. In the qualitative\nanalysis, the properties of the error function and control laws are analyzed,\nwhile the performances of the controllers are quantitatively compared using\nnumerical simulation.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY"}, {"arxiv_id": "2401.13191", "title": "Towards Multi-domain Face Landmark Detection with Synthetic Data from\n  Diffusion model", "abstract": "Recently, deep learning-based facial landmark detection for in-the-wild faces\nhas achieved significant improvement. However, there are still challenges in\nface landmark detection in other domains (e.g. cartoon, caricature, etc). This\nis due to the scarcity of extensively annotated training data. To tackle this\nconcern, we design a two-stage training approach that effectively leverages\nlimited datasets and the pre-trained diffusion model to obtain aligned pairs of\nlandmarks and face in multiple domains. In the first stage, we train a\nlandmark-conditioned face generation model on a large dataset of real faces. In\nthe second stage, we fine-tune the above model on a small dataset of\nimage-landmark pairs with text prompts for controlling the domain. Our new\ndesigns enable our method to generate high-quality synthetic paired datasets\nfrom multiple domains while preserving the alignment between landmarks and\nfacial features. Finally, we fine-tuned a pre-trained face landmark detection\nmodel on the synthetic dataset to achieve multi-domain face landmark detection.\nOur qualitative and quantitative results demonstrate that our method\noutperforms existing methods on multi-domain face landmark detection.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13192", "title": "Generative Design of Crystal Structures by Point Cloud Representations\n  and Diffusion Model", "abstract": "Efficiently generating energetically stable crystal structures has long been\na challenge in material design, primarily due to the immense arrangement of\natoms in a crystal lattice. To facilitate the discovery of stable material, we\npresent a framework for the generation of synthesizable materials, leveraging a\npoint cloud representation to encode intricate structural information. At the\nheart of this framework lies the introduction of a diffusion model as its\nfoundational pillar. To gauge the efficacy of our approach, we employ it to\nreconstruct input structures from our training datasets, rigorously validating\nits high reconstruction performance. Furthermore, we demonstrate the profound\npotential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely\nnew materials, emphasizing their synthesizability. Our research stands as a\nnoteworthy contribution to the advancement of materials design and synthesis\nthrough the cutting-edge avenue of generative design instead of the\nconventional substitution or experience-based discovery.", "field": "Computer Science", "categories": "cs.AI,cond-mat.mtrl-sci,cs.LG,physics.comp-ph"}, {"arxiv_id": "2401.13193", "title": "Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN", "abstract": "Deep learning has made significant advances in computer vision, particularly\nin image classification tasks. Despite their high accuracy on training data,\ndeep learning models often face challenges related to complexity and\noverfitting. One notable concern is that the model often relies heavily on a\nlimited subset of filters for making predictions. This dependency can result in\ncompromised generalization and an increased vulnerability to minor variations.\nWhile regularization techniques like weight decay, dropout, and data\naugmentation are commonly used to address this issue, they may not directly\ntackle the reliance on specific filters. Our observations reveal that the heavy\nreliance problem gets severe when slow-learning filters are deprived of\nlearning opportunities due to fast-learning filters. Drawing inspiration from\nimage augmentation research that combats over-reliance on specific image\nregions by removing and replacing parts of images, our idea is to mitigate the\nproblem of over-reliance on strong filters by substituting highly activated\nfeatures. To this end, we present a novel method called Catch-up Mix, which\nprovides learning opportunities to a wide range of filters during training,\nfocusing on filters that may lag behind. By mixing activation maps with\nrelatively lower norms, Catch-up Mix promotes the development of more diverse\nrepresentations and reduces reliance on a small subset of filters. Experimental\nresults demonstrate the superiority of our method in various vision\nclassification datasets, providing enhanced robustness.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13196", "title": "Stable numerics for finite-strain elasticity", "abstract": "A backward stable numerical calculation of a function with condition number\n$\\kappa$ will have a relative accuracy of $\\kappa\\epsilon_{\\text{machine}}$.\nStandard formulations and software implementations of finite-strain elastic\nmaterials models make use of the deformation gradient $\\boldsymbol F = I +\n\\partial \\boldsymbol u/\\partial \\boldsymbol X$ and Cauchy-Green tensors. These\nformulations are not numerically stable, leading to loss of several digits of\naccuracy when used in the small strain regime, and often precluding the use of\nsingle precision floating point arithmetic. We trace the source of this\ninstability to specific points of numerical cancellation, interpretable as\nill-conditioned steps. We show how to compute various strain measures in a\nstable way and how to transform common constitutive models to their stable\nrepresentations, formulated in either initial or current configuration. The\nstable formulations all provide accuracy of order $\\epsilon_{\\text{machine}}$.\nIn many cases, the stable formulations have elegant representations in terms of\nappropriate strain measures and offer geometric intuition that is lacking in\ntheir standard representation. We show that algorithmic differentiation can\nstably compute stresses so long as the strain energy is expressed stably, and\ngive principles for stable computation that can be applied to inelastic\nmaterials.", "field": "Computer Science", "categories": "math.NA,cs.CE,cs.NA,74B20, 90-08, 90C90, 65K99,G.1.8; G.4; J.2; J.6"}, {"arxiv_id": "2401.13199", "title": "Why People Still Fall for Phishing Emails: An Empirical Investigation\n  into How Users Make Email Response Decisions", "abstract": "Despite technical and non-technical countermeasures, humans continue to be\ntricked by phishing emails. How users make email response decisions is a\nmissing piece in the puzzle to identifying why people still fall for phishing\nemails. We conducted an empirical study using a think-aloud method to\ninvestigate how people make 'response decisions' while reading emails. The\ngrounded theory analysis of the in-depth qualitative data has enabled us to\nidentify different elements of email users' decision-making that influence\ntheir email response decisions. Furthermore, we developed a theoretical model\nthat explains how people could be driven to respond to emails based on the\nidentified elements of users' email decision-making processes and the\nrelationships uncovered from the data. The findings provide deeper insights\ninto phishing email susceptibility due to people's email response\ndecision-making behavior. We also discuss the implications of our findings for\ndesigners and researchers working in anti-phishing training, education, and\nawareness interventions", "field": "Computer Science", "categories": "cs.CR,cs.CY,cs.HC"}, {"arxiv_id": "2401.132", "title": "Topology-aware Embedding Memory for Learning on Expanding Graphs", "abstract": "Memory replay based techniques have shown great success for continual\nlearning with incrementally accumulated Euclidean data. Directly applying them\nto continually expanding graphs, however, leads to the potential memory\nexplosion problem due to the need to buffer representative nodes and their\nassociated topological neighborhood structures. To this end, we systematically\nanalyze the key challenges in the memory explosion problem, and present a\ngeneral framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs)\nwith Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed\nframework not only reduces the memory space complexity from $\\mathcal{O}(nd^L)$\nto $\\mathcal{O}(n)$~\\footnote{$n$: memory budget, $d$: average node degree,\n$L$: the radius of the GNN receptive field}, but also fully utilizes the\ntopological information for memory replay. Specifically, PDGNNs decouple\ntrainable parameters from the computation ego-subgraph via\n\\textit{Topology-aware Embeddings} (TEs), which compress ego-subgraphs into\ncompact vectors (i.e., TEs) to reduce the memory consumption. Based on this\nframework, we discover a unique \\textit{pseudo-training effect} in continual\nlearning on expanding graphs and this effect motivates us to develop a novel\n\\textit{coverage maximization sampling} strategy that can enhance the\nperformance with a tight memory budget. Thorough empirical studies demonstrate\nthat, by tackling the memory explosion problem and incorporating topological\ninformation into memory replay, PDGNNs with TEM significantly outperform\nstate-of-the-art techniques, especially in the challenging class-incremental\nsetting.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13201", "title": "MLLMReID: Multimodal Large Language Model-based Person Re-identification", "abstract": "Multimodal large language models (MLLM) have achieved satisfactory results in\nmany tasks. However, their performance in the task of person re-identification\n(ReID) has not been explored to date. This paper will investigate how to adapt\nthem for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID\nimage-text datasets, and then use their visual encoder as a backbone for ReID.\nHowever, there still exist two apparent issues: (1) Designing instructions for\nReID, MLLMs may overfit specific instructions, and designing a variety of\ninstructions will lead to higher costs. (2) Latent image feature vectors from\nLLMs are not involved in loss computation. Instructional learning, aligning\nimage-text features, results in indirect optimization and a learning objective\nthat inadequately utilizes features, limiting effectiveness in person feature\nlearning. To address these problems, this paper proposes MLLMReID: Multimodal\nLarge Language Model-based ReID. Firstly, we proposed Common Instruction, a\nsimple approach that leverages the essence ability of LLMs to continue writing,\navoiding complex and diverse instruction design. Secondly, we proposed\nDirectReID, which effectively employs the latent image feature vectors of\nimages outputted by LLMs in ReID tasks. The experimental results demonstrate\nthe superiority of our method. We will open-source the code on GitHub.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.CL"}, {"arxiv_id": "2401.13202", "title": "PAC Learnability for Reliable Communication over Discrete Memoryless\n  Channels", "abstract": "In practical communication systems, knowledge of channel models is often\nabsent, and consequently, transceivers need be designed based on empirical\ndata. In this work, we study data-driven approaches to reliably choosing\ndecoding metrics and code rates that facilitate reliable communication over\nunknown discrete memoryless channels (DMCs). Our analysis is inspired by the\nPAC learning theory and does not rely on any assumptions on the statistical\ncharacteristics of DMCs. We show that a naive plug-in algorithm for choosing\ndecoding metrics is likely to fail for finite training sets. We propose an\nalternative algorithm called the virtual sample algorithm and establish a\nnon-asymptotic lower bound on its performance. The virtual sample algorithm is\nthen used as a building block for constructing a learning algorithm that\nchooses a decoding metric and a code rate using which a transmitter and a\nreceiver can reliably communicate at a rate arbitrarily close to the channel\nmutual information. Therefore, we conclude that DMCs are PAC learnable.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13203", "title": "Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects", "abstract": "Controllable 3D indoor scene synthesis stands at the forefront of\ntechnological progress, offering various applications like gaming, film, and\naugmented/virtual reality. The capability to stylize and de-couple objects\nwithin these scenarios is a crucial factor, providing an advanced level of\ncontrol throughout the editing process. This control extends not just to\nmanipulating geometric attributes like translation and scaling but also\nincludes managing appearances, such as stylization. Current methods for scene\nstylization are limited to applying styles to the entire scene, without the\nability to separate and customize individual objects. Addressing the\nintricacies of this challenge, we introduce a unique pipeline designed for\nsynthesis 3D indoor scenes. Our approach involves strategically placing objects\nwithin the scene, utilizing information from professionally designed bounding\nboxes. Significantly, our pipeline prioritizes maintaining style consistency\nacross multiple objects within the scene, ensuring a cohesive and visually\nappealing result aligned with the desired aesthetic. The core strength of our\npipeline lies in its ability to generate 3D scenes that are not only visually\nimpressive but also exhibit features like photorealism, multi-view consistency,\nand diversity. These scenes are crafted in response to various natural language\nprompts, demonstrating the versatility and adaptability of our model.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13205", "title": "Boosting the Transferability of Adversarial Examples via Local Mixup and\n  Adaptive Step Size", "abstract": "Adversarial examples are one critical security threat to various visual\napplications, where injected human-imperceptible perturbations can confuse the\noutput.Generating transferable adversarial examples in the black-box setting is\ncrucial but challenging in practice. Existing input-diversity-based methods\nadopt different image transformations, but may be inefficient due to\ninsufficient input diversity and an identical perturbation step size. Motivated\nby the fact that different image regions have distinctive weights in\nclassification, this paper proposes a black-box adversarial generative\nframework by jointly designing enhanced input diversity and adaptive step\nsizes. We design local mixup to randomly mix a group of transformed adversarial\nimages, strengthening the input diversity. For precise adversarial generation,\nwe project the perturbation into the $tanh$ space to relax the boundary\nconstraint. Moreover, the step sizes of different regions can be dynamically\nadjusted by integrating a second-order momentum.Extensive experiments on\nImageNet validate that our framework can achieve superior transferability\ncompared to state-of-the-art baselines.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13206", "title": "Self-Improving Interference Management Based on Deep Learning With\n  Uncertainty Quantification", "abstract": "This paper presents a groundbreaking self-improving interference management\nframework tailored for wireless communications, integrating deep learning with\nuncertainty quantification to enhance overall system performance. Our approach\naddresses the computational challenges inherent in traditional\noptimization-based algorithms by harnessing deep learning models to predict\noptimal interference management solutions. A significant breakthrough of our\nframework is its acknowledgment of the limitations inherent in data-driven\nmodels, particularly in scenarios not adequately represented by the training\ndataset. To overcome these challenges, we propose a method for uncertainty\nquantification, accompanied by a qualifying criterion, to assess the\ntrustworthiness of model predictions. This framework strategically alternates\nbetween model-generated solutions and traditional algorithms, guided by a\ncriterion that assesses the prediction credibility based on quantified\nuncertainties. Experimental results validate the framework's efficacy,\ndemonstrating its superiority over traditional deep learning models, notably in\nscenarios underrepresented in the training dataset. This work marks a\npioneering endeavor in harnessing self-improving deep learning for interference\nmanagement, through the lens of uncertainty quantification.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13209", "title": "Symmetric, Optimization-based, Cross-element Compatible Nodal\n  Distributions for High-order Finite Elements", "abstract": "In high-order and high-dimensional finite elements, ill-conditioned nodal\ndistributions are often computationally cost-prohibitive. As a result, uniform\ndistributions quickly fall apart. For tensor-product like elements,\nGauss-Legendre-Lobatto (GLL) nodal distributions are often used as a\nsubstitute. Besides these, other efficient nodal distributions are difficult to\ncreate due to a desired symmetry within elements and conformity with\nneighboring elements. In this paper, we provide a general framework to\nconstruct symmetric, well-conditioned, cross-element compatible nodal\ndistributions which can be used for high-order and high-dimensional finite\nelements. Starting from the inherent symmetries in any potential element, the\nframework is used to build up nodal groups in a structured and efficient manner\nutilizing the natural coordinates of each element, while ensuring nodes stay\nwithin the elements. By constructing constrained symmetry groups, the vertices,\nedges, and faces, of all elements are required to conform to their respective\nlower-dimensional distributions. Optimizing over these groups yields the\ndesired optimized nodal distributions. We demonstrate the strength of this\nframework by creating and comparing optimized nodal distributions with GLL\ndistributions (in elements such as the line, quadrilateral, and hexahedron),\nand its robustness by generating optimized nodal distributions for otherwise\ndifficult elements (such as the triangle, tetrahedron, and triangular prism).", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.1321", "title": "Multitask Active Learning for Graph Anomaly Detection", "abstract": "In the web era, graph machine learning has been widely used on ubiquitous\ngraph-structured data. As a pivotal component for bolstering web security and\nenhancing the robustness of graph-based applications, the significance of graph\nanomaly detection is continually increasing. While Graph Neural Networks (GNNs)\nhave demonstrated efficacy in supervised and semi-supervised graph anomaly\ndetection, their performance is contingent upon the availability of sufficient\nground truth labels. The labor-intensive nature of identifying anomalies from\ncomplex graph structures poses a significant challenge in real-world\napplications. Despite that, the indirect supervision signals from other tasks\n(e.g., node classification) are relatively abundant. In this paper, we propose\na novel MultItask acTIve Graph Anomaly deTEction framework, namely MITIGATE.\nFirstly, by coupling node classification tasks, MITIGATE obtains the capability\nto detect out-of-distribution nodes without known anomalies. Secondly, MITIGATE\nquantifies the informativeness of nodes by the confidence difference across\ntasks, allowing samples with conflicting predictions to provide informative yet\nnot excessively challenging information for subsequent training. Finally, to\nenhance the likelihood of selecting representative nodes that are distant from\nknown patterns, MITIGATE adopts a masked aggregation mechanism for distance\nmeasurement, considering both inherent features of nodes and current labeled\nstatus. Empirical studies on four datasets demonstrate that MITIGATE\nsignificantly outperforms the state-of-the-art methods for anomaly detection.\nOur code is publicly available at: https://github.com/AhaChang/MITIGATE.", "field": "Computer Science", "categories": "cs.LG,cs.SI"}, {"arxiv_id": "2401.13212", "title": "AdCorDA: Classifier Refinement via Adversarial Correction and Domain\n  Adaptation", "abstract": "This paper describes a simple yet effective technique for refining a\npretrained classifier network. The proposed AdCorDA method is based on\nmodification of the training set and making use of the duality between network\nweights and layer inputs. We call this input space training. The method\nconsists of two stages - adversarial correction followed by domain adaptation.\nAdversarial correction uses adversarial attacks to correct incorrect\ntraining-set classifications. The incorrectly classified samples of the\ntraining set are removed and replaced with the adversarially corrected samples\nto form a new training set, and then, in the second stage, domain adaptation is\nperformed back to the original training set. Extensive experimental validations\nshow significant accuracy boosts of over 5% on the CIFAR-100 dataset. The\ntechnique can be straightforwardly applied to refinement of weight-quantized\nneural networks, where experiments show substantial enhancement in performance\nover the baseline. The adversarial correction technique also results in\nenhanced robustness to adversarial attacks.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.LG"}, {"arxiv_id": "2401.13213", "title": "Common-Sense Bias Discovery and Mitigation for Classification Tasks", "abstract": "Machine learning model bias can arise from dataset composition: sensitive\nfeatures correlated to the learning target disturb the model decision rule and\nlead to performance differences along the features. Existing de-biasing work\ncaptures prominent and delicate image features which are traceable in model\nlatent space, like colors of digits or background of animals. However, using\nthe latent space is not sufficient to understand all dataset feature\ncorrelations. In this work, we propose a framework to extract feature clusters\nin a dataset based on image descriptions, allowing us to capture both subtle\nand coarse features of the images. The feature co-occurrence pattern is\nformulated and correlation is measured, utilizing a human-in-the-loop for\nexamination. The analyzed features and correlations are human-interpretable, so\nwe name the method Common-Sense Bias Discovery (CSBD). Having exposed sensitive\ncorrelations in a dataset, we demonstrate that downstream model bias can be\nmitigated by adjusting image sampling weights, without requiring a sensitive\ngroup label supervision. Experiments show that our method discovers novel\nbiases on multiple classification tasks for two benchmark image datasets, and\nthe intervention outperforms state-of-the-art unsupervised bias mitigation\nmethods.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13214", "title": "AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical\n  Attention Network", "abstract": "Recently, methods based on deep learning have been successfully applied to\nship detection for synthetic aperture radar (SAR) images. Despite the\ndevelopment of numerous ship detection methodologies, detecting small and\ncoastal ships remains a significant challenge due to the limited features and\nclutter in coastal environments. For that, a novel adaptive multi-hierarchical\nattention module (AMAM) is proposed to learn multi-scale features and\nadaptively aggregate salient features from various feature layers, even in\ncomplex environments. Specifically, we first fuse information from adjacent\nfeature layers to enhance the detection of smaller targets, thereby achieving\nmulti-scale feature enhancement. Then, to filter out the adverse effects of\ncomplex backgrounds, we dissect the previously fused multi-level features on\nthe channel, individually excavate the salient regions, and adaptively\namalgamate features originating from different channels. Thirdly, we present a\nnovel adaptive multi-hierarchical attention network (AMANet) by embedding the\nAMAM between the backbone network and the feature pyramid network (FPN).\nBesides, the AMAM can be readily inserted between different frameworks to\nimprove object detection. Lastly, extensive experiments on two large-scale SAR\nship detection datasets demonstrate that our AMANet method is superior to\nstate-of-the-art methods.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.LG,68T45,I.2.10"}, {"arxiv_id": "2401.13216", "title": "On Principled Local Optimization Methods for Federated Learning", "abstract": "Federated Learning (FL), a distributed learning paradigm that scales\non-device learning collaboratively, has emerged as a promising approach for\ndecentralized AI applications. Local optimization methods such as Federated\nAveraging (FedAvg) are the most prominent methods for FL applications. Despite\ntheir simplicity and popularity, the theoretical understanding of local\noptimization methods is far from clear. This dissertation aims to advance the\ntheoretical foundation of local methods in the following three directions.\n  First, we establish sharp bounds for FedAvg, the most popular algorithm in\nFederated Learning. We demonstrate how FedAvg may suffer from a notion we call\niterate bias, and how an additional third-order smoothness assumption may\nmitigate this effect and lead to better convergence rates. We explain this\nphenomenon from a Stochastic Differential Equation (SDE) perspective.\n  Second, we propose Federated Accelerated Stochastic Gradient Descent (FedAc),\nthe first principled acceleration of FedAvg, which provably improves the\nconvergence rate and communication efficiency. Our technique uses on a\npotential-based perturbed iterate analysis, a novel stability analysis of\ngeneralized accelerated SGD, and a strategic tradeoff between acceleration and\nstability.\n  Third, we study the Federated Composite Optimization problem, which extends\nthe classic smooth setting by incorporating a shared non-smooth regularizer. We\nshow that direct extensions of FedAvg may suffer from the \"curse of primal\naveraging,\" resulting in slow convergence. As a solution, we propose a new\nprimal-dual algorithm, Federated Dual Averaging, which overcomes the curse of\nprimal averaging by employing a novel inter-client dual averaging procedure.", "field": "Computer Science", "categories": "cs.LG,cs.DC,math.OC,stat.ML"}, {"arxiv_id": "2401.13218", "title": "ULTRA: Unleash LLMs' Potential for Event Argument Extraction through\n  Hierarchical Modeling and Pair-wise Refinement", "abstract": "Structural extraction of events within discourse is critical since it avails\na deeper understanding of communication patterns and behavior trends. Event\nargument extraction (EAE), at the core of event-centric understanding, is the\ntask of identifying role-specific text spans (i.e., arguments) for a given\nevent. Document-level EAE (DocEAE) focuses on arguments that are scattered\nacross an entire document. In this work, we explore the capabilities of open\nsource Large Language Models (LLMs), i.e., Flan-UL2, for the DocEAE task. To\nthis end, we propose ULTRA, a hierarchical framework that extracts event\narguments more cost-effectively -- the method needs as few as 50 annotations\nand doesn't require hitting costly API endpoints. Further, it alleviates the\npositional bias issue intrinsic to LLMs. ULTRA first sequentially reads text\nchunks of a document to generate a candidate argument set, upon which ULTRA\nlearns to drop non-pertinent candidates through self-refinement. We further\nintroduce LEAFER to address the challenge LLMs face in locating the exact\nboundary of an argument span. ULTRA outperforms strong baselines, which include\nstrong supervised models and ChatGPT, by 9.8% when evaluated by the exact match\n(EM) metric.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13221", "title": "Unified-Width Adaptive Dynamic Network for All-In-One Image Restoration", "abstract": "In contrast to traditional image restoration methods, all-in-one image\nrestoration techniques are gaining increased attention for their ability to\nrestore images affected by diverse and unknown corruption types and levels.\nHowever, contemporary all-in-one image restoration methods omit task-wise\ndifficulties and employ the same networks to reconstruct images afflicted by\ndiverse degradations. This practice leads to an underestimation of the task\ncorrelations and suboptimal allocation of computational resources. To elucidate\ntask-wise complexities, we introduce a novel concept positing that intricate\nimage degradation can be represented in terms of elementary degradation.\nBuilding upon this foundation, we propose an innovative approach, termed the\nUnified-Width Adaptive Dynamic Network (U-WADN), consisting of two pivotal\ncomponents: a Width Adaptive Backbone (WAB) and a Width Selector (WS). The WAB\nincorporates several nested sub-networks with varying widths, which facilitates\nthe selection of the most apt computations tailored to each task, thereby\nstriking a balance between accuracy and computational efficiency during\nruntime. For different inputs, the WS automatically selects the most\nappropriate sub-network width, taking into account both task-specific and\nsample-specific complexities. Extensive experiments across a variety of image\nrestoration tasks demonstrate that the proposed U-WADN achieves better\nperformance while simultaneously reducing up to 32.3\\% of FLOPs and providing\napproximately 15.7\\% real-time acceleration. The code has been made available\nat \\url{https://github.com/xuyimin0926/U-WADN}.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13222", "title": "It's About Time: Incorporating Temporality in Retrieval Augmented\n  Language Models", "abstract": "The web serves as a global repository of knowledge, used by billions of\npeople to search for information. Ensuring that users receive the most relevant\nand up-to-date information, especially in the presence of multiple versions of\nweb content from different time points remains a critical challenge for\ninformation retrieval. This challenge has recently been compounded by the\nincreased use of question answering tools trained on Wikipedia or web content\nand powered by large language models (LLMs) \\citep{chatgpt} which have been\nfound to make up information (or hallucinate), and in addition have been shown\nto struggle with the temporal dimensions of information. Even Retriever\nAugmented Language Models (RALMs) which incorporate a document database to\nreduce LLM hallucination are unable to handle temporal queries correctly. This\nleads to instances where RALMs respond to queries such as \"Who won the\nWimbledon Championship?\", by retrieving document passages related to Wimbledon\nbut without the ability to differentiate between them based on how recent they\nare.\n  In this paper, we propose and evaluate, TempRALM, a temporally-aware\nRetriever Augmented Language Model (RALM) with few-shot learning extensions,\nwhich takes into account both semantically and temporally relevant documents\nrelative to a given query, rather than relying on semantic similarity alone. We\nshow that our approach results in up to 74\\% improvement in performance over\nthe baseline RALM model, without requiring model pre-training, recalculating or\nreplacing the RALM document index, or adding other computationally intensive\nelements.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13223", "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over\n  Tabular and Textual Data", "abstract": "In this work, we address question answering (QA) over a hybrid of tabular and\ntextual data that are very common content on the Web (e.g. SEC filings), where\ndiscrete reasoning capabilities are often required. Recently, large language\nmodels (LLMs) like GPT-4 have demonstrated strong multi-step reasoning\ncapabilities. We then consider harnessing the amazing power of LLMs to solve\nour task. We abstract a Step-wise Pipeline for tabular and textual QA, which\nconsists of three key steps, including Extractor, Reasoner and Executor, and\ninitially design an instruction to instantiate the pipeline and validate that\nGPT-4 outperforms all existing methods. However, utilizing an online LLM like\nGPT-4 holds various challenges in terms of cost, latency, and data security\nrisk, which motivates us to specialize smaller LLMs in this task. We develop a\nTAT-LLM language model by fine-tuning LLaMA 2 with the training data generated\nautomatically from existing expert-annotated datasets following the Step-wise\nPipeline. The experimental results have verified that our TAT-LLM model can\noutperform all baseline models, including the previous best fine-tuned models\nand very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.\nWe hope our work can serve as a pioneering example of specializing smaller\nlanguage models for specific tasks.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13227", "title": "Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large\n  Language Models", "abstract": "Exploring the application of large-scale language models to graph learning is\na novel endeavor. However, the vast amount of information inherent in large\ngraphs poses significant challenges to this process. This paper focuses on the\nlink prediction task and introduces LPNL (Link Prediction via Natural\nLanguage), a framework based on a large language model designed for scalable\nlink prediction on large-scale heterogeneous graphs.We design novel prompts for\nlink prediction that articulate graph details in natural language. We propose a\ntwo-stage sampling pipeline to extract crucial information from large-scale\nheterogeneous graphs, and a divide-and-conquer strategy to control the input\ntoken count within predefined limits, addressing the challenge of overwhelming\ninformation. We fine-tune a T5 model based on our self-supervised learning\ndesigned for for link prediction. Extensive experiments on a large public\nheterogeneous graphs demonstrate that LPNL outperforms various advanced\nbaselines, highlighting its remarkable performance in link prediction tasks on\nlarge-scale graphs.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG,cs.SI"}, {"arxiv_id": "2401.13229", "title": "From Random to Informed Data Selection: A Diversity-Based Approach to\n  Optimize Human Annotation and Few-Shot Learning", "abstract": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.LG"}, {"arxiv_id": "2401.13231", "title": "DittoGym: Learning to Control Soft Shape-Shifting Robots", "abstract": "Robot co-design, where the morphology of a robot is optimized jointly with a\nlearned policy to solve a specific task, is an emerging area of research. It\nholds particular promise for soft robots, which are amenable to novel\nmanufacturing techniques that can realize learned morphologies and actuators.\nInspired by nature and recent novel robot designs, we propose to go a step\nfurther and explore the novel reconfigurable robots, defined as robots that can\nchange their morphology within their lifetime. We formalize control of\nreconfigurable soft robots as a high-dimensional reinforcement learning (RL)\nproblem. We unify morphology change, locomotion, and environment interaction in\nthe same action space, and introduce an appropriate, coarse-to-fine curriculum\nthat enables us to discover policies that accomplish fine-grained control of\nthe resulting robots. We also introduce DittoGym, a comprehensive RL benchmark\nfor reconfigurable soft robots that require fine-grained morphology changes to\naccomplish the tasks. Finally, we evaluate our proposed coarse-to-fine\nalgorithm on DittoGym and demonstrate robots that learn to change their\nmorphology several times within a sequence, uniquely enabled by our RL\nalgorithm. More results are available at https://dittogym.github.io.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13232", "title": "Distributed Source Coding Using Constrained-Random-Number Generators", "abstract": "This paper investigates the general distributed lossless/lossy source coding\nformulated by Jana and Blahut. Their multi-letter rate-distortion region, an\nalternative to the region derived by Yang and Qin, is characterized by entropy\nfunctions for arbitrary general correlated sources. Achievability is shown by\nconstructing a code based on constrained-random number generators.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13236", "title": "How to Collaborate: Towards Maximizing the Generalization Performance in\n  Cross-Silo Federated Learning", "abstract": "Federated learning (FL) has attracted vivid attention as a privacy-preserving\ndistributed learning framework. In this work, we focus on cross-silo FL, where\nclients become the model owners after training and are only concerned about the\nmodel's generalization performance on their local data. Due to the data\nheterogeneity issue, asking all the clients to join a single FL training\nprocess may result in model performance degradation. To investigate the\neffectiveness of collaboration, we first derive a generalization bound for each\nclient when collaborating with others or when training independently. We show\nthat the generalization performance of a client can be improved only by\ncollaborating with other clients that have more training data and similar data\ndistribution. Our analysis allows us to formulate a client utility maximization\nproblem by partitioning clients into multiple collaborating groups. A\nhierarchical clustering-based collaborative training (HCCT) scheme is then\nproposed, which does not need to fix in advance the number of groups. We\nfurther analyze the convergence of HCCT for general non-convex loss functions\nwhich unveils the effect of data similarity among clients. Extensive\nsimulations show that HCCT achieves better generalization performance than\nbaseline schemes, whereas it degenerates to independent training and\nconventional FL in specific scenarios.", "field": "Computer Science", "categories": "cs.LG,cs.DC"}, {"arxiv_id": "2401.13239", "title": "Adaptive Crowdsourcing Via Self-Supervised Learning", "abstract": "Common crowdsourcing systems average estimates of a latent quantity of\ninterest provided by many crowdworkers to produce a group estimate. We develop\na new approach -- just-predict-others -- that leverages self-supervised\nlearning and a novel aggregation scheme. This approach adapts weights assigned\nto crowdworkers based on estimates they provided for previous quantities. When\nskills vary across crowdworkers or their estimates correlate, the weighted sum\noffers a more accurate group estimate than the average. Existing algorithms\nsuch as expectation maximization can, at least in principle, produce similarly\naccurate group estimates. However, their computational requirements become\nonerous when complex models, such as neural networks, are required to express\nrelationships among crowdworkers. Just-predict-others accommodates such\ncomplexity as well as many other practical challenges. We analyze the efficacy\nof just-predict-others through theoretical and computational studies. Among\nother things, we establish asymptotic optimality as the number of engagements\nper crowdworker grows.", "field": "Computer Science", "categories": "cs.LG,cs.HC"}, {"arxiv_id": "2401.13244", "title": "Automating Unrealizability Logic: Hoare-style Proof Synthesis for\n  Infinite Sets of Programs", "abstract": "Unrealizability logic (UL) was proposed by Kim et al. as the first\nHoare-style proof system for proving properties that hold for an infinite set\nof programs (defined by a regular tree grammar). The goal of our work is to\nautomate reasoning and proof generation for UL. A key ingredient in UL is the\nnotion of nonterminal summaries-inductive facts that characterize recursive\nnonterminals in the grammar that defines the set of programs. They are\nanalogous to procedure summaries in Hoare logic. The goal of automating UL led\nus to reformulate the inference rules-in particular, introducing a unified rule\nfor nonterminal summaries, called the rule of adaptation, which draws\ninspiration from how procedure summaries are handled in Hoare logic. In the\nsame way that verification conditions can be used to synthesize loop invariants\nfor Hoare logic proofs, our reformulation of UL reduces the problem of\nsynthesizing a nonterminal summary to a Syntax-Guided Synthesis problem. We\nimplement Wuldo, the first checker and synthesizer for UL. Wuldo can express\nproofs beyond the reach of existing tools, including proofs that establish how\ninfinitely many programs behave on infinitely many inputs, and in some cases\nWuldo can even synthesize the needed nonterminal summaries.", "field": "Computer Science", "categories": "cs.PL"}, {"arxiv_id": "2401.13245", "title": "GraphiMind: LLM-centric Interface for Information Graphics Design", "abstract": "Information graphics are pivotal in effective information dissemination and\nstorytelling. However, creating such graphics is extremely challenging for\nnon-professionals, since the design process requires multifaceted skills and\ncomprehensive knowledge. Thus, despite the many available authoring tools, a\nsignificant gap remains in enabling non-experts to produce compelling\ninformation graphics seamlessly, especially from scratch. Recent breakthroughs\nshow that Large Language Models (LLMs), especially when tool-augmented, can\nautonomously engage with external tools, making them promising candidates for\nenabling innovative graphic design applications. In this work, we propose a\nLLM-centric interface with the agent GraphiMind for automatic generation,\nrecommendation, and composition of information graphics design resources, based\non user intent expressed through natural language. Our GraphiMind integrates a\nTextual Conversational Interface, powered by tool-augmented LLM, with a\ntraditional Graphical Manipulation Interface, streamlining the entire design\nprocess from raw resource curation to composition and refinement. Extensive\nevaluations highlight our tool's proficiency in simplifying the design process,\nopening avenues for its use by non-professional users. Moreover, we spotlight\nthe potential of LLMs in reshaping the domain of information graphics design,\noffering a blend of automation, versatility, and user-centric interactivity.", "field": "Computer Science", "categories": "cs.HC"}, {"arxiv_id": "2401.13246", "title": "SEER: Facilitating Structured Reasoning and Explanation via\n  Reinforcement Learning", "abstract": "Elucidating the reasoning process with structured explanations from question\nto answer is fundamentally crucial, as it significantly enhances the\ninterpretability and trustworthiness of question-answering (QA) systems.\nHowever, structured explanations demand models to perform intricate structured\nreasoning, which poses great challenges. Most existing methods focus on\nsingle-step reasoning through supervised learning, ignoring logical\ndependencies between steps. Meanwhile, existing reinforcement learning\n(RL)-based methods overlook the structured relationships, impeding RL's\npotential in structured reasoning. In this paper, we propose SEER, a novel\nmethod that maximizes a structure-based return to facilitate structured\nreasoning and explanation. Our proposed structure-based return precisely\ndescribes the hierarchical and branching structure inherent in structured\nreasoning, effectively capturing the intricate relationships between states. We\nalso introduce a fine-grained reward function to meticulously delineate diverse\nreasoning steps. Extensive experiments show that SEER significantly outperforms\nstate-of-the-art methods, achieving an absolute improvement of 6.9% over\nRL-based methods on EntailmentBank, a 4.4% average improvement on STREET\nbenchmark, and exhibiting outstanding efficiency and cross-dataset\ngeneralization performance.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13247", "title": "A Human-Centered Review of Algorithms in Homelessness Research", "abstract": "Homelessness is a humanitarian challenge affecting an estimated 1.6 billion\npeople worldwide. In the face of rising homeless populations in developed\nnations and a strain on social services, government agencies are increasingly\nadopting data-driven models to determine one's risk of experiencing\nhomelessness and assigning scarce resources to those in need. We conducted a\nsystematic literature review of 57 papers to understand the evolution of these\ndecision-making algorithms. We investigated trends in computational methods,\npredictor variables, and target outcomes used to develop the models using a\nhuman-centered lens and found that only 9 papers (15.7%) investigated model\nfairness and bias. We uncovered tensions between explainability and ecological\nvalidity wherein predictive risk models (53.4%) focused on reductive\nexplainability while resource allocation models (25.9%) were dependent on\nunrealistic assumptions and simulated data that are not useful in practice.\nFurther, we discuss research challenges and opportunities for developing\nhuman-centered algorithms in this area.", "field": "Computer Science", "categories": "cs.HC"}, {"arxiv_id": "2401.13248", "title": "\"Here's Your Evidence\": False Consensus in Public Twitter Discussions of\n  COVID-19 Science", "abstract": "The COVID-19 pandemic brought about an extraordinary rate of scientific\npapers on the topic that were discussed among the general public, although\noften in biased or misinformed ways. In this paper, we present a mixed-methods\nanalysis aimed at examining whether public discussions were commensurate with\nthe scientific consensus on several COVID-19 issues. We estimate scientific\nconsensus based on samples of abstracts from preprint servers and compare\nagainst the volume of public discussions on Twitter mentioning these papers. We\nfind that anti-consensus posts and users, though overall less numerous than\npro-consensus ones, are vastly over-represented on Twitter, thus producing a\nfalse consensus effect. This transpires with favorable papers being\ndisproportionately amplified, along with an influx of new anti-consensus user\nsign-ups. Finally, our content analysis highlights that anti-consensus users\nmisrepresent scientific findings or question scientists' integrity in their\nefforts to substantiate their claims.", "field": "Computer Science", "categories": "cs.CY,cs.SI"}, {"arxiv_id": "2401.13254", "title": "A modular architecture for IMU-based data gloves", "abstract": "The flexibility and range of motion in human hands play a crucial role in\nhuman interaction with the environment and have been studied across different\nfields. Researchers explored various technological solutions for gathering\ninformation from the hands. These solutions include tracking hand motion\nthrough cameras or wearable sensors and using wearable sensors to measure the\nposition and pressure of contact points. Data gloves can collect both types of\ninformation by utilizing inertial measurement units, flex sensors, magnetic\ntrackers for motion tracking, and force resistors or touch sensors for contact\nmeasurement. Although there are commercially available data gloves, researchers\noften create custom data gloves to achieve the desired flexibility and control\nover the hardware. However, the existing literature lacks standardization and\nthe reuse of previously designed data gloves. As a result, many gloves with\nunclear characteristics exist, which makes replication challenging and\nnegatively impacts the reproducibility of studies. This work proposes a\nmodular, open hardware and software architecture for creating customized data\ngloves based on IMU technology. We also provide an architecture implementation\nalong with an experimental protocol to evaluate device performance.", "field": "Computer Science", "categories": "cs.AR"}, {"arxiv_id": "2401.13255", "title": "Constructing a fully homomorphic encryption scheme with the Yoneda Lemma", "abstract": "This paper redefines the foundations of asymmetric cryptography's homomorphic\ncryptosystems through the application of the Yoneda Lemma. It explicitly\nillustrates that widely adopted systems, including ElGamal, RSA, Benaloh,\nRegev's LWE, and NTRUEncrypt, directly derive from the principles of the Yoneda\nLemma. This synthesis gives rise to a holistic homomorphic encryption framework\nnamed the Yoneda Encryption Scheme. Within this scheme, encryption is\nelucidated through the bijective maps of the Yoneda Lemma Isomorphism, and\ndecryption seamlessly follows from the naturality of these maps. This\nunification suggests a conjecture for a unified model theory framework,\nproviding a basis for reasoning about both homomorphic and fully homomorphic\nencryption (FHE) schemes. As a practical demonstration, the paper introduces an\nFHE scheme capable of processing arbitrary finite sequences of encrypted\nmultiplications and additions without the need for additional tweaking\ntechniques, such as squashing or bootstrapping. This not only underscores the\npractical implications of the proposed theoretical advancements but also\nintroduces new possibilities for leveraging model theory and forcing techniques\nin cryptography to facilitate the design of FHE schemes.", "field": "Computer Science", "categories": "cs.CR,math.CT,18A35 (Primary) 18C30, 18A05, 68P25 (Secondary)"}, {"arxiv_id": "2401.13256", "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n  Personalized Dialogue Systems", "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.1326", "title": "MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion,\n  ASR Error Detection, and ASR Error Correction", "abstract": "The prevalent approach in speech emotion recognition (SER) involves\nintegrating both audio and textual information to comprehensively identify the\nspeaker's emotion, with the text generally obtained through automatic speech\nrecognition (ASR). An essential issue of this approach is that ASR errors from\nthe text modality can worsen the performance of SER. Previous studies have\nproposed using an auxiliary ASR error detection task to adaptively assign\nweights of each word in ASR hypotheses. However, this approach has limited\nimprovement potential because it does not address the coherence of semantic\ninformation in the text. Additionally, the inherent heterogeneity of different\nmodalities leads to distribution gaps between their representations, making\ntheir fusion challenging. Therefore, in this paper, we incorporate two\nauxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to\nenhance the semantic coherence of ASR text, and further introduce a novel\nmulti-modal fusion (MF) method to learn shared representations across\nmodalities. We refer to our method as MF-AED-AEC. Experimental results indicate\nthat MF-AED-AEC significantly outperforms the baseline model by a margin of\n4.1\\%.", "field": "Computer Science", "categories": "cs.CL,cs.MM,cs.SD,eess.AS"}, {"arxiv_id": "2401.13262", "title": "Designing Redistribution Mechanisms for Reducing Transaction Fees in\n  Blockchains", "abstract": "Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user\ntransactions to include in blocks and determine their payments (i.e.,\ntransaction fees). Increasing demand and scarce block resources have led to\nhigh user transaction fees. As these blockchains are a public resource, it may\nbe preferable to reduce these transaction fees. To this end, we introduce\nTransaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG\npayments collected from such TFM as rebates to minimize transaction fees.\nClassic redistribution mechanisms (RMs) achieve this while ensuring Allocative\nEfficiency (AE) and User Incentive Compatibility (UIC). Our first result shows\nthe non-triviality of applying RM in TFMs. More concretely, we prove that it is\nimpossible to reduce transaction fees when (i) transactions that are not\nconfirmed do not receive rebates and (ii) the miner can strategically\nmanipulate the mechanism. Driven by this, we propose \\emph{Robust} TFRM\n(\\textsf{R-TFRM}): a mechanism that compromises on an honest miner's individual\nrationality to guarantee strictly positive rebates to the users. We then\nintroduce \\emph{robust} and \\emph{rational} TFRM (\\textsf{R}$^2$\\textsf{-TFRM})\nthat uses trusted on-chain randomness that additionally guarantees miner's\nindividual rationality (in expectation) and strictly positive rebates. Our\nresults show that TFRMs provide a promising new direction for reducing\ntransaction fees in public blockchains.", "field": "Computer Science", "categories": "cs.GT,cs.AI,cs.CR"}, {"arxiv_id": "2401.13264", "title": "Enhancing cross-domain detection: adaptive class-aware contrastive\n  transformer", "abstract": "Recently,the detection transformer has gained substantial attention for its\ninherent minimal post-processing requirement.However,this paradigm relies on\nabundant training data,yet in the context of the cross-domain\nadaptation,insufficient labels in the target domain exacerbate issues of class\nimbalance and model performance degradation.To address these challenges, we\npropose a novel class-aware cross domain detection transformer based on the\nadversarial learning and mean-teacher framework.First,considering the\ninconsistencies between the classification and regression tasks,we introduce an\nIoU-aware prediction branch and exploit the consistency of classification and\nlocation scores to filter and reweight pseudo labels.Second, we devise a\ndynamic category threshold refinement to adaptively manage model\nconfidence.Third,to alleviate the class imbalance,an instance-level class-aware\ncontrastive learning module is presented to encourage the generation of\ndiscriminative features for each class,particularly benefiting minority\nclasses.Experimental results across diverse domain-adaptive scenarios validate\nour method's effectiveness in improving performance and alleviating class\nimbalance issues,which outperforms the state-of-the-art transformer based\nmethods.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13266", "title": "SpecLLM: Exploring Generation and Review of VLSI Design Specification\n  with Large Language Model", "abstract": "The development of architecture specifications is an initial and fundamental\nstage of the integrated circuit (IC) design process. Traditionally,\narchitecture specifications are crafted by experienced chip architects, a\nprocess that is not only time-consuming but also error-prone. Mistakes in these\nspecifications may significantly affect subsequent stages of chip design.\nDespite the presence of advanced electronic design automation (EDA) tools,\neffective solutions to these specification-related challenges remain scarce.\nSince writing architecture specifications is naturally a natural language\nprocessing (NLP) task, this paper pioneers the automation of architecture\nspecification development with the advanced capabilities of large language\nmodels (LLMs). Leveraging our definition and dataset, we explore the\napplication of LLMs in two key aspects of architecture specification\ndevelopment: (1) Generating architecture specifications, which includes both\nwriting specifications from scratch and converting RTL code into detailed\nspecifications. (2) Reviewing existing architecture specifications. We got\npromising results indicating that LLMs may revolutionize how these critical\nspecification documents are developed in IC design nowadays. By reducing the\neffort required, LLMs open up new possibilities for efficiency and accuracy in\nthis crucial aspect of chip design.", "field": "Computer Science", "categories": "cs.AR"}, {"arxiv_id": "2401.13267", "title": "Dual-modal Dynamic Traceback Learning for Medical Report Generation", "abstract": "With increasing reliance on medical imaging in clinical practices, automated\nreport generation from medical images is in great demand. Existing report\ngeneration methods typically adopt an encoder-decoder deep learning framework\nto build a uni-directional image-to-report mapping. However, such a framework\nignores the bi-directional mutual associations between images and reports, thus\nincurring difficulties in associating the intrinsic medical meanings between\nthem. Recent generative representation learning methods have demonstrated the\nbenefits of dual-modal learning from both image and text modalities. However,\nthese methods exhibit two major drawbacks for medical report generation: 1)\nthey tend to capture morphological information and have difficulties in\ncapturing subtle pathological semantic information, and 2) they predict masked\ntext rely on both unmasked images and text, inevitably degrading performance\nwhen inference is based solely on images. In this study, we propose a new\nreport generation framework with dual-modal dynamic traceback learning (DTrace)\nto overcome the two identified drawbacks and enable dual-modal learning for\nmedical report generation. To achieve this, our DTrace introduces a traceback\nmechanism to control the semantic validity of generated content via\nself-assessment. Further, our DTrace introduces a dynamic learning strategy to\nadapt to various proportions of image and text input, enabling report\ngeneration without reliance on textual input during inference. Extensive\nexperiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that\nour DTrace outperforms state-of-the-art medical report generation methods.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13268", "title": "Loss Allocation in Submarine Armored Three-core HVAC Power Cables", "abstract": "Loss allocation of the three different components (conductor, sheaths and\narmor) of solidly bonded three-core separated lead-sheathed armored cables,\nfrequently employed in offshore wind farms, is challenging due to the lack of\naccurate enough analytical expressions in the IEC standard. Also, loss\nallocation through experimental tests leads to inaccurate results since it is\nbased on questionable assumptions. This paper improves both the IEC formulae\nand experimental methods by means of different analytical corrections in the\nconductor and sheath loss expressions. To this aim, an ad hoc application\ninterface (Virtual Lab) based on 3D numerical simulations (finite element\nmethod) has been developed. This tool virtualizes and automates different test\nsetups to emulate, in few seconds, the most employed experimental procedures in\nthis type of cable. The analytical corrections have been derived from an\nin-depth analysis of a first set of 368 cables, ranging from 30 to 275 kV. The\nnew loss expressions were successfully applied to a second set of 645 armored\ncables of quite diverse features (voltages from 10 to 275 kV, sections and\ndimensional parameters), hence bringing a general framework for any kind of\nthree-core armored cable.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.1327", "title": "Audio-Infused Automatic Image Colorization by Exploiting Audio Scene\n  Semantics", "abstract": "Automatic image colorization is inherently an ill-posed problem with\nuncertainty, which requires an accurate semantic understanding of scenes to\nestimate reasonable colors for grayscale images. Although recent\ninteraction-based methods have achieved impressive performance, it is still a\nvery difficult task to infer realistic and accurate colors for automatic\ncolorization. To reduce the difficulty of semantic understanding of grayscale\nscenes, this paper tries to utilize corresponding audio, which naturally\ncontains extra semantic information about the same scene. Specifically, a novel\naudio-infused automatic image colorization (AIAIC) network is proposed, which\nconsists of three stages. First, we take color image semantics as a bridge and\npretrain a colorization network guided by color image semantics. Second, the\nnatural co-occurrence of audio and video is utilized to learn the color\nsemantic correlations between audio and visual scenes. Third, the implicit\naudio semantic representation is fed into the pretrained network to finally\nrealize the audio-guided colorization. The whole process is trained in a\nself-supervised manner without human annotation. In addition, an audiovisual\ncolorization dataset is established for training and testing. Experiments\ndemonstrate that audio guidance can effectively improve the performance of\nautomatic colorization, especially for some scenes that are difficult to\nunderstand only from visual modality.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13274", "title": "An energy-stable parametric finite element method for the planar\n  Willmore flow", "abstract": "We propose an energy-stable parametric finite element method (PFEM) for the\nplanar Willmore flow and establish its unconditional energy stability of the\nfull discretization scheme. The key lies in the introduction of two novel\ngeometric identities to describe the planar Willmore flow: the first one\ninvolves the coupling of the outward unit normal vector $\\boldsymbol{n}$ and\nthe normal velocity $V$, and the second one concerns the time derivative of the\nmean curvature $\\kappa$. Based on them, we derive a set of new geometric\npartial differential equations for the planar Willmore flow, leading to our new\nfully-discretized and unconditionally energy-stable PFEM. Our stability\nanalysis is also based on the two new geometric identities. Extensive numerical\nexperiments are provided to illustrate its efficiency and validate its\nunconditional energy stability.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.13275", "title": "Can AI Assistants Know What They Don't Know?", "abstract": "Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.1328", "title": "DDI-CoCo: A Dataset For Understanding The Effect Of Color Contrast In\n  Machine-Assisted Skin Disease Detection", "abstract": "Skin tone as a demographic bias and inconsistent human labeling poses\nchallenges in dermatology AI. We take another angle to investigate color\ncontrast's impact, beyond skin tones, on malignancy detection in skin disease\ndatasets: We hypothesize that in addition to skin tones, the color difference\nbetween the lesion area and skin also plays a role in malignancy detection\nperformance of dermatology AI models. To study this, we first propose a robust\nlabeling method to quantify color contrast scores of each image and validate\nour method by showing small labeling variations. More importantly, applying our\nmethod to \\textit{the only} diverse-skin tone and pathologically-confirmed skin\ndisease dataset DDI, yields \\textbf{DDI-CoCo Dataset}, and we observe a\nperformance gap between the high and low color difference groups. This\ndisparity remains consistent across various state-of-the-art (SoTA) image\nclassification models, which supports our hypothesis. Furthermore, we study the\ninteraction between skin tone and color difference effects and suggest that\ncolor difference can be an additional reason behind model performance bias\nbetween skin tones. Our work provides a complementary angle to dermatology AI\nfor improving skin disease detection.", "field": "Computer Science", "categories": "cs.CV,cs.CE"}, {"arxiv_id": "2401.13282", "title": "RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing", "abstract": "Forecasting complex system dynamics, particularly for long-term predictions,\nis persistently hindered by error accumulation and computational burdens. This\nstudy presents RefreshNet, a multiscale framework developed to overcome these\nchallenges, delivering an unprecedented balance between computational\nefficiency and predictive accuracy. RefreshNet incorporates convolutional\nautoencoders to identify a reduced order latent space capturing essential\nfeatures of the dynamics, and strategically employs multiple recurrent neural\nnetwork (RNN) blocks operating at varying temporal resolutions within the\nlatent space, thus allowing the capture of latent dynamics at multiple temporal\nscales. The unique \"refreshing\" mechanism in RefreshNet allows coarser blocks\nto reset inputs of finer blocks, effectively controlling and alleviating error\naccumulation. This design demonstrates superiority over existing techniques\nregarding computational efficiency and predictive accuracy, especially in\nlong-term forecasting. The framework is validated using three benchmark\napplications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and\nKuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms\nstate-of-the-art methods in long-term forecasting accuracy and speed, marking a\nsignificant advancement in modeling complex systems and opening new avenues in\nunderstanding and predicting their behavior.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13285", "title": "Small Object Tracking in LiDAR Point Cloud: Learning the\n  Target-awareness Prototype and Fine-grained Search Region", "abstract": "Single Object Tracking in LiDAR point cloud is one of the most essential\nparts of environmental perception, in which small objects are inevitable in\nreal-world scenarios and will bring a significant barrier to the accurate\nlocation. However, the existing methods concentrate more on exploring universal\narchitectures for common categories and overlook the challenges that small\nobjects have long been thorny due to the relative deficiency of foreground\npoints and a low tolerance for disturbances. To this end, we propose a Siamese\nnetwork-based method for small object tracking in the LiDAR point cloud, which\nis composed of the target-awareness prototype mining (TAPM) module and the\nregional grid subdivision (RGS) module. The TAPM module adopts the\nreconstruction mechanism of the masked decoder to learn the prototype in the\nfeature space, aiming to highlight the presence of foreground points that will\nfacilitate the subsequent location of small objects. Through the above\nprototype is capable of accentuating the small object of interest, the\npositioning deviation in feature maps still leads to high tracking errors. To\nalleviate this issue, the RGS module is proposed to recover the fine-grained\nfeatures of the search region based on ViT and pixel shuffle layers. In\naddition, apart from the normal settings, we elaborately design a scaling\nexperiment to evaluate the robustness of the different trackers on small\nobjects. Extensive experiments on KITTI and nuScenes demonstrate that our\nmethod can effectively improve the tracking performance of small targets\nwithout affecting normal-sized objects.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13296", "title": "Visual Objectification in Films: Towards a New AI Task for Video\n  Interpretation", "abstract": "In film gender studies, the concept of 'male gaze' refers to the way the\ncharacters are portrayed on-screen as objects of desire rather than subjects.\nIn this article, we introduce a novel video-interpretation task, to detect\ncharacter objectification in films. The purpose is to reveal and quantify the\nusage of complex temporal patterns operated in cinema to produce the cognitive\nperception of objectification. We introduce the ObyGaze12 dataset, made of 1914\nmovie clips densely annotated by experts for objectification concepts\nidentified in film studies and psychology. We evaluate recent vision models,\nshow the feasibility of the task and where the challenges remain with concept\nbottleneck models. Our new dataset and code are made available to the\ncommunity.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13298", "title": "Towards Explainable Harmful Meme Detection through Multimodal Debate\n  between Large Language Models", "abstract": "The age of social media is flooded with Internet memes, necessitating a clear\ngrasp and effective identification of harmful ones. This task presents a\nsignificant challenge due to the implicit meaning embedded in memes, which is\nnot explicitly conveyed through the surface text and image. However, existing\nharmful meme detection methods do not present readable explanations that unveil\nsuch implicit meaning to support their detection decisions. In this paper, we\npropose an explainable approach to detect harmful memes, achieved through\nreasoning over conflicting rationales from both harmless and harmful positions.\nSpecifically, inspired by the powerful capacity of Large Language Models (LLMs)\non text generation and reasoning, we first elicit multimodal debate between\nLLMs to generate the explanations derived from the contradictory arguments.\nThen we propose to fine-tune a small language model as the debate judge for\nharmfulness inference, to facilitate multimodal fusion between the harmfulness\nrationales and the intrinsic multimodal information within memes. In this way,\nour model is empowered to perform dialectical reasoning over intricate and\nimplicit harm-indicative patterns, utilizing multimodal explanations\noriginating from both harmless and harmful arguments. Extensive experiments on\nthree public meme datasets demonstrate that our harmful meme detection approach\nachieves much better performance than state-of-the-art methods and exhibits a\nsuperior capacity for explaining the meme harmfulness of the model predictions.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13301", "title": "Classification of Radiologically Isolated Syndrome and Clinically\n  Isolated Syndrome with Machine-Learning Techniques", "abstract": "Background and purpose: The unanticipated detection by magnetic resonance\nimaging (MRI) in the brain of asymptomatic subjects of white matter lesions\nsuggestive of multiple sclerosis (MS) has been named radiologically isolated\nsyndrome (RIS). As the difference between early MS [i.e. clinically isolated\nsyndrome (CIS)] and RIS is the occurrence of a clinical event, it is logical to\nimprove detection of the subclinical form without interfering with MRI as there\nare radiological diagnostic criteria for that. Our objective was to use\nmachine-learning classification methods to identify morphometric measures that\nhelp to discriminate patients with RIS from those with CIS.\n  Methods: We used a multimodal 3-T MRI approach by combining MRI biomarkers\n(cortical thickness, cortical and subcortical grey matter volume, and white\nmatter integrity) of a cohort of 17 patients with RIS and 17 patients with CIS\nfor single-subject level classification.\n  Results: The best proposed models to predict the diagnosis of CIS and RIS\nwere based on the Naive Bayes, Bagging and Multilayer Perceptron classifiers\nusing only three features: the left rostral middle frontal gyrus volume and the\nfractional anisotropy values in the right amygdala and right lingual gyrus. The\nNaive Bayes obtained the highest accuracy [overall classification, 0.765; area\nunder the receiver operating characteristic (AUROC), 0.782].\n  Conclusions: A machine-learning approach applied to multimodal MRI data may\ndifferentiate between the earliest clinical expressions of MS (CIS and RIS)\nwith an accuracy of 78%.\n  Keywords: Bagging; Multilayer Perceptron; Naive Bayes classifier; clinically\nisolated syndrome; diffusion tensor imaging; machine-learning; magnetic\nresonance imaging; multiple sclerosis; radiologically isolated syndrome.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13302", "title": "A Lagrange-Newton Approach to Smoothing-and-Mapping", "abstract": "In this report we explore the application of the Lagrange-Newton method to\nthe SAM (smoothing-and-mapping) problem in mobile robotics. In Lagrange-Newton\nSAM, the angular component of each pose vector is expressed by orientation\nvectors and treated through Lagrange constraints. This is different from the\ntypical Gauss-Newton approach where variations need to be mapped back and forth\nbetween Euclidean space and a manifold suitable for rotational components. We\nderive equations for five different types of measurements between robot poses:\ntranslation, distance, and rotation from odometry in the plane, as well as\nhome-vector angle and compass angle from visual homing. We demonstrate the\nfeasibility of the Lagrange-Newton approach for a simple example related to a\ncleaning robot scenario.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13303", "title": "MaLA-500: Massive Language Adaptation of Large Language Models", "abstract": "Large language models have advanced the state of the art in natural language\nprocessing. However, their predominant design for English or a limited set of\nlanguages creates a substantial gap in their effectiveness for low-resource\nlanguages. To bridge this gap, we introduce MaLA-500, a novel large language\nmodel designed to cover an extensive range of 534 languages. To train MaLA-500,\nwe employ vocabulary extension and continued pretraining on LLaMA 2 with\nGlot500-c. Our experiments on SIB-200 show that MaLA-500 achieves\nstate-of-the-art in-context learning results. We release MaLA-500 at\nhttps://huggingface.co/MaLA-LM", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13306", "title": "POSTER: Towards Secure 5G Infrastructures for Production Systems", "abstract": "To meet the requirements of modern production, industrial communication\nincreasingly shifts from wired fieldbus to wireless 5G communication. Besides\ntremendous benefits, this shift introduces severe novel risks, ranging from\nlimited reliability over new security vulnerabilities to a lack of\naccountability. To address these risks, we present approaches to (i) prevent\nattacks through authentication and redundant communication, (ii) detect\nanomalies and jamming, and (iii) respond to detected attacks through device\nexclusion and accountability measures.", "field": "Computer Science", "categories": "cs.CR"}, {"arxiv_id": "2401.13307", "title": "ChatterBox: Multi-round Multimodal Referring and Grounding", "abstract": "In this study, we establish a baseline for a new task named multimodal\nmulti-round referring and grounding (MRG), opening up a promising direction for\ninstance-level multimodal dialogues. We present a new benchmark and an\nefficient vision-language model for this purpose. The new benchmark, named\nCB-300K, spans challenges including multi-round dialogue, complex spatial\nrelationships among multiple instances, and consistent reasoning, which are\nbeyond those shown in existing benchmarks. The proposed model, named\nChatterBox, utilizes a two-branch architecture to collaboratively handle vision\nand language tasks. By tokenizing instance regions, the language branch\nacquires the ability to perceive referential information. Meanwhile, ChatterBox\nfeeds a query embedding in the vision branch to a token receiver for visual\ngrounding. A two-stage optimization strategy is devised, making use of both\nCB-300K and auxiliary external data to improve the model's stability and\ncapacity for instance-level understanding. Experiments show that ChatterBox\noutperforms existing models in MRG both quantitatively and qualitatively,\npaving a new path towards multimodal dialogue scenarios with complicated and\nprecise interactions. Code, data, and model are available at:\nhttps://github.com/sunsmarterjie/ChatterBox.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.1331", "title": "Lessons Learned Migrating CUDA to SYCL: A HEP Case Study with ROOT\n  RDataFrame", "abstract": "The world's largest particle accelerator, located at CERN, produces petabytes\nof data that need to be analysed efficiently, to study the fundamental\nstructures of our universe. ROOT is an open-source C++ data analysis framework,\ndeveloped for this purpose. Its high-level data analysis interface, RDataFrame,\ncurrently only supports CPU parallelism. Given the increasing heterogeneity in\ncomputing facilities, it becomes crucial to efficiently support GPGPUs to take\nadvantage of the available resources. SYCL allows for a single-source\nimplementation, which enables support for different architectures. In this\npaper, we describe a CUDA implementation and the migration process to SYCL,\nfocusing on a core high energy physics operation in RDataFrame --\nhistogramming. We detail the challenges that we faced when integrating SYCL\ninto a large and complex code base. Furthermore, we perform an extensive\ncomparative performance analysis of two SYCL compilers, AdaptiveCpp and DPC++,\nand the reference CUDA implementation. We highlight the performance bottlenecks\nthat we encountered, and the methodology used to detect these. Based on our\nfindings, we provide actionable insights for developers of SYCL applications.", "field": "Computer Science", "categories": "cs.DC"}, {"arxiv_id": "2401.13311", "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in\n  Large Multimodal Models", "abstract": "Recent advancements in AI have led to the development of large multimodal\nmodels (LMMs) capable of processing complex tasks involving joint reasoning\nover text and visual content in the image (e.g., navigating maps in public\nplaces). This paper introduces ConTextual, a novel benchmark comprising\ninstructions designed explicitly to evaluate LMMs' ability to perform\ncontext-sensitive text-rich visual reasoning. ConTextual emphasizes diverse\nreal-world scenarios (e.g., time-reading, navigation, shopping and more)\ndemanding a deeper understanding of the interactions between textual and visual\nelements. Our findings reveal a significant performance gap of 30.8% between\nthe best-performing LMM, GPT-4V(ision), and human capabilities using human\nevaluation indicating substantial room for improvement in context-sensitive\ntext-rich visual reasoning. Notably, while GPT-4V excelled in abstract\ncategories like meme and quote interpretation, its overall performance still\nlagged behind humans. In addition to human evaluations, we also employed\nautomatic evaluation metrics using GPT-4, uncovering similar trends in\nperformance disparities. We also perform a fine-grained evaluation across\ndiverse visual contexts and provide qualitative analysis which provides a\nrobust framework for future advancements in the LMM design.\nhttps://con-textual.github.io/", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.LG"}, {"arxiv_id": "2401.13312", "title": "Evaluation of the power frequency magnetic field generated by three-core\n  armored cables through 3D finite element simulations", "abstract": "The great expansion in offshore power plants is raising the concern regarding\nthe cumulative effect of the electromagnetic field emissions caused by\nsubmarine power cables. In this sense, owners are required to predict these\nemissions during the permitting and consenting process of new power plants.\nThis is a challenging task, especially in the case of HVAC three-core armored\ncables due to their complex geometry. Customarily, 2D approaches based on the\nfinite element method (FEM) have been employed for evaluating the magnetic\nfield emissions caused by these cables. However, inaccurate results are\nobtained since the phase conductors and armor twisting is omitted. This work\ndevelops, for the first time in the literature, an in-depth analysis of the\nmagnetic field caused by this type of cable through an ultra-shortened 3D-FEM\nmodel, which is also faced to experimental measurements taken on an actual 132\nkV, 800 mm2 three-core armored cable. Relevant conclusions are derived\nregarding the impact of the cable design on the magnetic field emissions,\nincluding material properties, as well as single and double-layer armors,\npresenting the proposed model not only as a valuable tool for predicting\npurposes, but also for optimizing cable design in terms of magnetic field\nemissions.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13313", "title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document\n  Understanding with Instructions", "abstract": "We study the problem of completing various visual document understanding\n(VDU) tasks, e.g., question answering and information extraction, on real-world\ndocuments through human-written instructions. To this end, we propose\nInstructDoc, the first large-scale collection of 30 publicly available VDU\ndatasets, each with diverse instructions in a unified format, which covers a\nwide range of 12 tasks and includes open document types/formats. Furthermore,\nto enhance the generalization performance on VDU tasks, we design a new\ninstruction-based document reading and understanding model, InstructDr, that\nconnects document images, image encoders, and large language models (LLMs)\nthrough a trainable bridging module. Experiments demonstrate that InstructDr\ncan effectively adapt to new VDU datasets, tasks, and domains via given\ninstructions and outperforms existing multimodal LLMs and ChatGPT without\nspecific training.", "field": "Computer Science", "categories": "cs.CV,cs.CL"}, {"arxiv_id": "2401.1332", "title": "A Big Data Architecture for Early Identification and Categorization of\n  Dark Web Sites", "abstract": "The dark web has become notorious for its association with illicit activities\nand there is a growing need for systems to automate the monitoring of this\nspace. This paper proposes an end-to-end scalable architecture for the early\nidentification of new Tor sites and the daily analysis of their content. The\nsolution is built using an Open Source Big Data stack for data serving with\nKubernetes, Kafka, Kubeflow, and MinIO, continuously discovering onion\naddresses in different sources (threat intelligence, code repositories, web-Tor\ngateways, and Tor repositories), downloading the HTML from Tor and\ndeduplicating the content using MinHash LSH, and categorizing with the BERTopic\nmodeling (SBERT embedding, UMAP dimensionality reduction, HDBSCAN document\nclustering and c-TF-IDF topic keywords). In 93 days, the system identified\n80,049 onion services and characterized 90% of them, addressing the challenge\nof Tor volatility. A disproportionate amount of repeated content is found, with\nonly 6.1% unique sites. From the HTML files of the dark sites, 31 different\nlow-topics are extracted, manually labeled, and grouped into 11 high-level\ntopics. The five most popular included sexual and violent content,\nrepositories, search engines, carding, cryptocurrencies, and marketplaces.\nDuring the experiments, we identified 14 sites with 13,946 clones that shared a\nsuspiciously similar mirroring rate per day, suggesting an extensive common\nphishing network. Among the related works, this study is the most\nrepresentative characterization of onion services based on topics to date.", "field": "Computer Science", "categories": "cs.DC,cs.IR"}, {"arxiv_id": "2401.13322", "title": "Polynomial-free unisolvence of polyharmonic splines with odd exponent by\n  random sampling", "abstract": "In a recent paper almost sure unisolvence of RBF interpolation at random\npoints with no polynomial addition was proved, for Thin-Plate Splines and\nRadial Powers with noninteger exponent. The proving technique left unsolved the\ncase of odd exponents. In this short note we prove almost sure polynomial-free\nunisolvence in such instances, by a deeper analysis of the interpolation matrix\ndeterminant and fundamental properties of analytic functions.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.13324", "title": "Information That Matters: Exploring Information Needs of People Affected\n  by Algorithmic Decisions", "abstract": "Explanations of AI systems rarely address the information needs of people\naffected by algorithmic decision-making (ADM). This gap between conveyed\ninformation and information that matters to affected stakeholders can impede\nunderstanding and adherence to regulatory frameworks such as the AI Act. To\naddress this gap, we present the \"XAI Novice Question Bank\": A catalog of\naffected stakeholders' information needs in two ADM use cases (employment\nprediction and health monitoring), covering the categories data, system\ncontext, system usage, and system specifications. Information needs were\ngathered in an interview study where participants received explanations in\nresponse to their inquiries. Participants further reported their understanding\nand decision confidence, showing that while confidence tended to increase after\nreceiving explanations, participants also met understanding challenges, such as\nbeing unable to tell why their understanding felt incomplete. Explanations\nfurther influenced participants' perceptions of the systems' risks and\nbenefits, which they confirmed or changed depending on the use case. When risks\nwere perceived as high, participants expressed particular interest in\nexplanations about intention, such as why and to what end a system was put in\nplace. With this work, we aim to support the inclusion of affected stakeholders\ninto explainability by contributing an overview of information and challenges\nrelevant to them when deciding on the adoption of ADM systems. We close by\nsummarizing our findings in a list of six key implications that inform the\ndesign of future explanations for affected stakeholder audiences.", "field": "Computer Science", "categories": "cs.HC,cs.AI"}, {"arxiv_id": "2401.13325", "title": "Memory Consistency Guided Divide-and-Conquer Learning for Generalized\n  Category Discovery", "abstract": "Generalized category discovery (GCD) aims at addressing a more realistic and\nchallenging setting of semi-supervised learning, where only part of the\ncategory labels are assigned to certain training samples. Previous methods\ngenerally employ naive contrastive learning or unsupervised clustering scheme\nfor all the samples. Nevertheless, they usually ignore the inherent critical\ninformation within the historical predictions of the model being trained.\nSpecifically, we empirically reveal that a significant number of salient\nunlabeled samples yield consistent historical predictions corresponding to\ntheir ground truth category. From this observation, we propose a Memory\nConsistency guided Divide-and-conquer Learning framework (MCDL). In this\nframework, we introduce two memory banks to record historical prediction of\nunlabeled data, which are exploited to measure the credibility of each sample\nin terms of its prediction consistency. With the guidance of credibility, we\ncan design a divide-and-conquer learning strategy to fully utilize the\ndiscriminative information of unlabeled data while alleviating the negative\ninfluence of noisy labels. Extensive experimental results on multiple\nbenchmarks demonstrate the generality and superiority of our method, where our\nmethod outperforms state-of-the-art models by a large margin on both seen and\nunseen classes of the generic image recognition and challenging semantic shift\nsettings (i.e.,with +8.4% gain on CUB and +8.1% on Standford Cars).", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13327", "title": "Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable\n  Stress Detection", "abstract": "Smartwatch health sensor data is increasingly utilized in smart health\napplications and patient monitoring, including stress detection. However, such\nmedical data often comprises sensitive personal information and is\nresource-intensive to acquire for research purposes. In response to this\nchallenge, we introduce the privacy-aware synthetization of multi-sensor\nsmartwatch health readings related to moments of stress. Our method involves\nthe generation of synthetic sequence data through Generative Adversarial\nNetworks (GANs), coupled with the implementation of Differential Privacy (DP)\nsafeguards for protecting patient information during model training. To ensure\nthe integrity of our synthetic data, we employ a range of quality assessments\nand monitor the plausibility between synthetic and original data. To test the\nusefulness, we create private machine learning models on a commonly used,\nalbeit small, stress detection dataset, exploring strategies for enhancing the\nexisting data foundation with our synthetic data. Through our GAN-based\naugmentation methods, we observe improvements in model performance, both in\nnon-private (0.45% F1) and private (11.90-15.48% F1) training scenarios. We\nunderline the potential of differentially private synthetic data in optimizing\nutility-privacy trade-offs, especially with limited availability of real\ntraining samples.", "field": "Computer Science", "categories": "cs.LG,cs.CR"}, {"arxiv_id": "2401.13328", "title": "Rank-decreasing transductions", "abstract": "We propose to study transformations on graphs, and more generally structures,\nby looking at how the cut-rank (as introduced by Oum) of subsets is affected\nwhen going from the input structure to the output structure. We consider\ntransformations in which the underlying sets are the same for both the input\nand output, and so the cut-ranks of subsets can be easily compared. The purpose\nof this paper is to give a characterisation of logically defined transductions\nthat is expressed in purely structural terms, without referring to logic:\ntransformations which decrease the cut-rank, in the asymptotic sense, are\nexactly those that can be defined in monadic second-order logic. This\ncharacterisation assumes that the transduction has inputs of bounded treewidth;\nwe also show that the characterisation fails in the absence of any assumptions.", "field": "Computer Science", "categories": "cs.LO"}, {"arxiv_id": "2401.13329", "title": "Generative Video Diffusion for Unseen Cross-Domain Video Moment\n  Retrieval", "abstract": "Video Moment Retrieval (VMR) requires precise modelling of fine-grained\nmoment-text associations to capture intricate visual-language relationships.\nDue to the lack of a diverse and generalisable VMR dataset to facilitate\nlearning scalable moment-text associations, existing methods resort to joint\ntraining on both source and target domain videos for cross-domain applications.\nMeanwhile, recent developments in vision-language multimodal models pre-trained\non large-scale image-text and/or video-text pairs are only based on coarse\nassociations (weakly labelled). They are inadequate to provide fine-grained\nmoment-text correlations required for cross-domain VMR. In this work, we solve\nthe problem of unseen cross-domain VMR, where certain visual and textual\nconcepts do not overlap across domains, by only utilising target domain\nsentences (text prompts) without accessing their videos. To that end, we\nexplore generative video diffusion for fine-grained editing of source videos\ncontrolled by the target sentences, enabling us to simulate target domain\nvideos. We address two problems in video editing for optimising unseen domain\nVMR: (1) generation of high-quality simulation videos of different moments with\nsubtle distinctions, (2) selection of simulation videos that complement\nexisting source training videos without introducing harmful noise or\nunnecessary repetitions. On the first problem, we formulate a two-stage video\ndiffusion generation controlled simultaneously by (1) the original video\nstructure of a source video, (2) subject specifics, and (3) a target sentence\nprompt. This ensures fine-grained variations between video moments. On the\nsecond problem, we introduce a hybrid selection mechanism that combines two\nquantitative metrics for noise filtering and one qualitative metric for\nleveraging VMR prediction on simulation video selection.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.1333", "title": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit\n  Neural Networks", "abstract": "Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)\nwith Early Exit Classifiers (EECs), to provide predictions at intermediate\npoints of the processing when enough confidence in classification is achieved.\nThis leads to many benefits in terms of effectiveness and efficiency.\nCurrently, the design of EENNs is carried out manually by experts, a complex\nand time-consuming task that requires accounting for many aspects, including\nthe correct placement, the thresholding, and the computational overhead of the\nEECs. For this reason, the research is exploring the use of Neural Architecture\nSearch (NAS) to automatize the design of EENNs. Currently, few comprehensive\nNAS solutions for EENNs have been proposed in the literature, and a fully\nautomated, joint design strategy taking into consideration both the backbone\nand the EECs remains an open problem. To this end, this work presents Neural\nArchitecture Search for Hardware Constrained Early Exit Neural Networks\n(NACHOS), the first NAS framework for the design of optimal EENNs satisfying\nconstraints on the accuracy and the number of Multiply and Accumulate (MAC)\noperations performed by the EENNs at inference time. In particular, this\nprovides the joint design of backbone and EECs to select a set of admissible\n(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best\ntradeoff between the accuracy and number of MACs. The results show that the\nmodels designed by NACHOS are competitive with the state-of-the-art EENNs.\nAdditionally, this work investigates the effectiveness of two novel\nregularization terms designed for the optimization of the auxiliary classifiers\nof the EENN", "field": "Computer Science", "categories": "cs.LG,cs.CV"}, {"arxiv_id": "2401.13334", "title": "Explainable Bayesian Optimization", "abstract": "In industry, Bayesian optimization (BO) is widely applied in the human-AI\ncollaborative parameter tuning of cyber-physical systems. However, BO's\nsolutions may deviate from human experts' actual goal due to approximation\nerrors and simplified objectives, requiring subsequent tuning. The black-box\nnature of BO limits the collaborative tuning process because the expert does\nnot trust the BO recommendations. Current explainable AI (XAI) methods are not\ntailored for optimization and thus fall short of addressing this gap. To bridge\nthis gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based\nexplainability method that produces high quality explanations through\nmultiobjective optimization. Our evaluation of benchmark optimization problems\nand real-world hyperparameter optimization tasks demonstrates TNTRules'\nsuperiority over state-of-the-art XAI methods in generating high quality\nexplanations. This work contributes to the intersection of BO and XAI,\nproviding interpretable optimization techniques for real-world applications.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13341", "title": "Evaluation of depth perception in crowded volumes", "abstract": "Depth perception in volumetric visualization plays a crucial role in the\nunderstanding and interpretation of volumetric data. Numerous visualization\ntechniques, many of which rely on physically based optical effects, promise to\nimprove depth perception but often do so without considering camera movement or\nthe content of the volume. As a result, the findings from previous studies may\nnot be directly applicable to crowded volumes, where a large number of\ncontained structures disrupts spatial perception. Crowded volumes therefore\nrequire special analysis and visualization tools with sparsification\ncapabilities. Interactivity is an integral part of visualizing and exploring\ncrowded spaces, but has received little attention in previous studies. To\naddress this gap, we conducted a study to assess the impact of different\nrendering techniques on depth perception in crowded volumes, with a particular\nfocus on the effects of camera movement. The results show that depth perception\nconsidering camera motion depends much more on the content of the volume than\non the chosen visualization technique. Furthermore, we found that traditional\nrendering techniques, which have often performed poorly in previous studies,\nshowed comparable performance to physically based methods in our study.", "field": "Computer Science", "categories": "cs.GR"}, {"arxiv_id": "2401.13343", "title": "Lessons on Datasets and Paradigms in Machine Learning for Symbolic\n  Computation: A Case Study on CAD", "abstract": "Symbolic Computation algorithms and their implementation in computer algebra\nsystems often contain choices which do not affect the correctness of the output\nbut can significantly impact the resources required: such choices can benefit\nfrom having them made separately for each problem via a machine learning model.\nThis study reports lessons on such use of machine learning in symbolic\ncomputation, in particular on the importance of analysing datasets prior to\nmachine learning and on the different machine learning paradigms that may be\nutilised. We present results for a particular case study, the selection of\nvariable ordering for cylindrical algebraic decomposition, but expect that the\nlessons learned are applicable to other decisions in symbolic computation.\n  We utilise an existing dataset of examples derived from applications which\nwas found to be imbalanced with respect to the variable ordering decision. We\nintroduce an augmentation technique for polynomial systems problems that allows\nus to balance and further augment the dataset, improving the machine learning\nresults by 28\\% and 38\\% on average, respectively. We then demonstrate how the\nexisting machine learning methodology used for the problem $-$ classification\n$-$ might be recast into the regression paradigm. While this does not have a\nradical change on the performance, it does widen the scope in which the\nmethodology can be applied to make choices.", "field": "Computer Science", "categories": "cs.SC,cs.LG,68W30, 68T05, 03C10,I.2.6; I.1.0"}, {"arxiv_id": "2401.13345", "title": "Intelligent Traffic Light Controller using Verilog and Xilinx Spartan-3e", "abstract": "Traffic lights also known as stop-lights are signaling devices placed at road\ncrossings which control the competing flow of traffic and avoid collisions. The\ntraffic light controller uses a worldwide color code (red, yellow and green). A\ntraffic light controller can be implemented by using a microcontroller, Field\nProgrammable Gate Array or Application Specific Integrated Circuits. Use of\nField Programmable Gate Array is beneficial for a number of reasons viz number\nof Input/Output ports, performance compared to that of a microcontroller and\nalso it is less expensive as compared to Application Specific Integrated\nCircuits. In this paper, an efficient Traffic Light controller is designed\nusing Moore finite state machine. The circuit description is done in Verilog\nand the design is tested and simulated on FPGA board Spartan-3e.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13346", "title": "Past, Present, Future: A Comprehensive Exploration of AI Use Cases in\n  the UMBRELLA IoT Testbed", "abstract": "UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem\nincorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative\nrobots, and edge-intelligence-enabled devices. This paper provides a guide to\nthe implemented and prospective artificial intelligence (AI) capabilities of\nUMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are\npresented in detail: 1) An automated streetlight monitoring for detecting\nissues and triggering maintenance alerts; 2) A Digital twin of building\nenvironments providing enhanced air quality sensing with reduced cost; 3) A\nlarge-scale Federated Learning framework for reducing communication overhead;\nand 4) An intrusion detection for containerised applications identifying\nmalicious activities. Additionally, the potential of UMBRELLA is outlined for\nfuture smart city and multi-robot crowdsensing applications enhanced by\nsemantic communications and multi-agent planning. Finally, to realise the above\nuse-cases we discuss the need for a tailored MLOps platform to automate\nUMBRELLA model pipelines and establish trust.", "field": "Computer Science", "categories": "cs.NI,cs.AI"}, {"arxiv_id": "2401.13351", "title": "Predicting IR Personalization Performance using Pre-retrieval Query\n  Predictors", "abstract": "Personalization generally improves the performance of queries but in a few\ncases it may also harms it. If we are able to predict and therefore to disable\npersonalization for those situations, the overall performance will be higher\nand users will be more satisfied with personalized systems. We use some\nstate-of-the-art pre-retrieval query performance predictors and propose some\nothers including the user profile information for the previous purpose. We\nstudy the correlations among these predictors and the difference between the\npersonalized and the original queries. We also use classification and\nregression techniques to improve the results and finally reach a bit more than\none third of the maximum ideal performance. We think this is a good starting\npoint within this research line, which certainly needs more effort and\nimprovements.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13352", "title": "EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable\n  Endoscopic Tissues Reconstruction", "abstract": "The accurate 3D reconstruction of deformable soft body tissues from\nendoscopic videos is a pivotal challenge in medical applications such as VR\nsurgery and medical image analysis. Existing methods often struggle with\naccuracy and the ambiguity of hallucinated tissue parts, limiting their\npractical utility. In this work, we introduce EndoGaussians, a novel approach\nthat employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This\nmethod marks the first use of Gaussian Splatting in this context, overcoming\nthe limitations of previous NeRF-based techniques. Our method sets new\nstate-of-the-art standards, as demonstrated by quantitative assessments on\nvarious endoscope datasets. These advancements make our method a promising tool\nfor medical professionals, offering more reliable and efficient 3D\nreconstructions for practical applications in the medical field.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13354", "title": "Characterizing Network Requirements for GPU API Remoting in AI\n  Applications", "abstract": "GPU remoting is a promising technique for supporting AI applications.\nNetworking plays a key role in enabling remoting. However, for efficient\nremoting, the network requirements in terms of latency and bandwidth are\nunknown. In this paper, we take a GPU-centric approach to derive the minimum\nlatency and bandwidth requirements for GPU remoting, while ensuring no (or\nlittle) performance degradation for AI applications. Our study including\ntheoretical model demonstrates that, with careful remoting design, unmodified\nAI applications can run on the remoting setup using commodity networking\nhardware without any overhead or even with better performance, with low network\ndemands.", "field": "Computer Science", "categories": "cs.OS,cs.NI"}, {"arxiv_id": "2401.13355", "title": "Considering Capacitive Effects in Foil Winding Homogenization", "abstract": "In conventional finite element simulations, foil windings with a thin foil\nand many turns require many mesh elements. This renders models quickly\ncomputationally infeasible. With the use of homogenization approaches, the\nfinite element mesh does not need to resolve the small-scale structure of the\nfoil winding domain. Present homogenization approaches take resistive and\ninductive effects into account. With an increase of the operation frequency of\nfoil windings, however, capacitive effects between adjacent turns in the foil\nwinding become relevant. This paper presents an extension to the standard foil\nwinding model that covers the capacitive behavior of foil windings.", "field": "Computer Science", "categories": "cs.CE,35Q61,G.1.8; J.2; G.1.2"}, {"arxiv_id": "2401.13357", "title": "Linear Relative Pose Estimation Founded on Pose-only Imaging Geometry", "abstract": "How to efficiently and accurately handle image matching outliers is a\ncritical issue in two-view relative estimation. The prevailing RANSAC method\nnecessitates that the minimal point pairs be inliers. This paper introduces a\nlinear relative pose estimation algorithm for n $( n \\geq 6$) point pairs,\nwhich is founded on the recent pose-only imaging geometry to filter out\noutliers by proper reweighting. The proposed algorithm is able to handle planar\ndegenerate scenes, and enhance robustness and accuracy in the presence of a\nsubstantial ratio of outliers. Specifically, we embed the linear global\ntranslation (LiGT) constraint into the strategies of iteratively reweighted\nleast-squares (IRLS) and RANSAC so as to realize robust outlier removal.\nSimulations and real tests of the Strecha dataset show that the proposed\nalgorithm achieves relative rotation accuracy improvement of 2 $\\sim$ 10 times\nin face of as large as 80% outliers.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13358", "title": "Sequential solution strategies for the Cahn-Hilliard-Biot model", "abstract": "This paper presents a study of solution strategies for the Cahn-Hilliard-Biot\nequations, a complex mathematical model for understanding flow in deformable\nporous media with changing solid phases. Solving the Cahn-Hilliard-Biot system\nposes significant challenges due to its coupled, nonlinear and non-convex\nnature. We explore various solution algorithms, comparing monolithic and\nsplitting strategies, focusing on both their computational efficiency and\nrobustness.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.13359", "title": "Reconfigurable routing in data center networks", "abstract": "The Reconfigurable Routing Problem (RRP) in hybrid networks is, in short, the\nproblem of finding settings for optical switches augmenting a static network so\nas to achieve optimal delivery of some given workload. The problem has\npreviously been studied in various scenarios with both tractable and\nNP-hardness results obtained. However, the data center and interconnection\nnetworks to which the problem is most relevant are almost always such that the\nstatic network is highly structured whereas all previous results assume that\nthe static network can be arbitrary (which makes existing computational\nhardness results less technologically relevant and also easier to obtain). In\nthis paper, and for the first time, we prove various intractability results for\nRRP where the underlying static network is highly structured, for example\nconsisting of a hypercube, and also extend some existing tractability results.", "field": "Computer Science", "categories": "cs.CC,cs.DM,cs.NI"}, {"arxiv_id": "2401.1336", "title": "Debiased Sample Selection for Combating Noisy Labels", "abstract": "Learning with noisy labels aims to ensure model generalization given a\nlabel-corrupted training set. The sample selection strategy achieves promising\nperformance by selecting a label-reliable subset for model training. In this\npaper, we empirically reveal that existing sample selection methods suffer from\nboth data and training bias that are represented as imbalanced selected sets\nand accumulation errors in practice, respectively. However, only the training\nbias was handled in previous studies. To address this limitation, we propose a\nnoIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection.\nSpecifically, to mitigate the training bias, we design a robust network\narchitecture that integrates with multiple experts. Compared with the\nprevailing double-branch network, our network exhibits better performance of\nselection and prediction by ensembling these experts while training with fewer\nparameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling\nstrategy based on two weight-based data samplers. By training on the mixture of\ntwo class-discriminative mini-batches, the model mitigates the effect of the\nimbalanced training set while avoiding sparse representations that are easily\ncaused by sampling strategies. Extensive experiments and analyses demonstrate\nthe effectiveness of ITEM. Our code is available at this url\n\\href{https://github.com/1998v7/ITEM}{ITEM}.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13361", "title": "A note on the numerical approximation of Greeks for American-style\n  options", "abstract": "In this note we consider the approximation of the Greeks Delta and Gamma of\nAmerican-style options through the numerical solution of time-dependent partial\ndifferential complementarity problems (PDCPs). This approach is very attractive\nas it can yield accurate approximations to these Greeks at essentially no\nadditional computational cost during the numerical solution of the PDCP for the\npertinent option value function. For the temporal discretization, the\nCrank-Nicolson method is arguably the most popular method in computational\nfinance. It is well-known, however, that this method can have an undesirable\nconvergence behaviour in the approximation of the Greeks Delta and Gamma for\nAmerican-style options, even when backward Euler damping (Rannacher smoothing)\nis employed.\n  In this note we study for the temporal discretization an interesting family\nof diagonally implicit Runge-Kutta (DIRK) methods together with the two-stage\nLobatto IIIC method. Through ample numerical experiments for one- and two-asset\nAmerican-style options, it is shown that these methods can yield a regular\nsecond-order convergence behaviour for the option value as well as for the\nGreeks Delta and Gamma. A mutual comparison reveals that the DIRK method with\nsuitably chosen parameter $\\theta$ is preferable.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.13362", "title": "TraKDis: A Transformer-based Knowledge Distillation Approach for Visual\n  Reinforcement Learning with Application to Cloth Manipulation", "abstract": "Approaching robotic cloth manipulation using reinforcement learning based on\nvisual feedback is appealing as robot perception and control can be learned\nsimultaneously. However, major challenges result due to the intricate dynamics\nof cloth and the high dimensionality of the corresponding states, what shadows\nthe practicality of the idea. To tackle these issues, we propose TraKDis, a\nnovel Transformer-based Knowledge Distillation approach that decomposes the\nvisual reinforcement learning problem into two distinct stages. In the first\nstage, a privileged agent is trained, which possesses complete knowledge of the\ncloth state information. This privileged agent acts as a teacher, providing\nvaluable guidance and training signals for subsequent stages. The second stage\ninvolves a knowledge distillation procedure, where the knowledge acquired by\nthe privileged agent is transferred to a vision-based agent by leveraging\npre-trained state estimation and weight initialization. TraKDis demonstrates\nbetter performance when compared to state-of-the-art RL techniques, showing a\nhigher performance of 21.9%, 13.8%, and 8.3% in cloth folding tasks in\nsimulation. Furthermore, to validate robustness, we evaluate the agent in a\nnoisy environment; the results indicate its ability to handle and adapt to\nenvironmental uncertainties effectively. Real robot experiments are also\nconducted to showcase the efficiency of our method in real-world scenarios.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13363", "title": "Do You Guys Want to Dance: Zero-Shot Compositional Human Dance\n  Generation with Multiple Persons", "abstract": "Human dance generation (HDG) aims to synthesize realistic videos from images\nand sequences of driving poses. Despite great success, existing methods are\nlimited to generating videos of a single person with specific backgrounds,\nwhile the generalizability for real-world scenarios with multiple persons and\ncomplex backgrounds remains unclear. To systematically measure the\ngeneralizability of HDG models, we introduce a new task, dataset, and\nevaluation protocol of compositional human dance generation (cHDG). Evaluating\nthe state-of-the-art methods on cHDG, we empirically find that they fail to\ngeneralize to real-world scenarios. To tackle the issue, we propose a novel\nzero-shot framework, dubbed MultiDance-Zero, that can synthesize videos\nconsistent with arbitrary multiple persons and background while precisely\nfollowing the driving poses. Specifically, in contrast to straightforward DDIM\nor null-text inversion, we first present a pose-aware inversion method to\nobtain the noisy latent code and initialization text embeddings, which can\naccurately reconstruct the composed reference image. Since directly generating\nvideos from them will lead to severe appearance inconsistency, we propose a\ncompositional augmentation strategy to generate augmented images and utilize\nthem to optimize a set of generalizable text embeddings. In addition,\nconsistency-guided sampling is elaborated to encourage the background and\nkeypoints of the estimated clean image at each reverse step to be close to\nthose of the reference image, further improving the temporal consistency of\ngenerated videos. Extensive qualitative and quantitative results demonstrate\nthe effectiveness and superiority of our approach.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13365", "title": "Organizing Scientific Knowledge From Energy System Research Using the\n  Open Research Knowledge Graph", "abstract": "Engineering sciences, such as energy system research, play an important role\nin developing solutions to technical, environmental, economic, and social\nchallenges of our modern society. In this context, the transformation of energy\nsystems into climate-neutral systems is one of the key strategies for\nmitigating climate change. For the transformation of energy systems, engineers\nmodel, simulate and analyze scenarios and transformation pathways to initiate\ndebates about possible transformation strategies. For these debates and\nresearch in general, all steps of the research process must be traceable to\nguarantee the trustworthiness of published results, avoid redundancies, and\nensure their social acceptance. However, the analysis of energy systems is an\ninterdisciplinary field as the investigations of large, complex energy systems\noften require the use of different software applications and large amounts of\nheterogeneous data. Engineers must therefore communicate, understand, and\n(re)use heterogeneous scientific knowledge and data. Although the importance of\nFAIR scientific knowledge and data in the engineering sciences and energy\nsystem research is increasing, little research has been conducted on this\ntopic. When it comes to publishing scientific knowledge and data from\npublications, software, and datasets (such as models, scenarios, and\nsimulations) openly available and transparent, energy system research lags\nbehind other research domains. According to Schmitt et al. and Nie{\\ss}e et\nal., engineers need technical support in the form of infrastructures, services,\nand terminologies to improve communication, understanding, and (re)use of\nscientific knowledge and data.", "field": "Computer Science", "categories": "cs.DL"}, {"arxiv_id": "2401.13366", "title": "Mitigating System Bias in Resource Constrained Asynchronous Federated\n  Learning Systems", "abstract": "Federated learning (FL) systems face performance challenges in dealing with\nheterogeneous devices and non-identically distributed data across clients. We\npropose a dynamic global model aggregation method within Asynchronous Federated\nLearning (AFL) deployments to address these issues. Our aggregation method\nscores and adjusts the weighting of client model updates based on their upload\nfrequency to accommodate differences in device capabilities. Additionally, we\nalso immediately provide an updated global model to clients after they upload\ntheir local models to reduce idle time and improve training efficiency. We\nevaluate our approach within an AFL deployment consisting of 10 simulated\nclients with heterogeneous compute constraints and non-IID data. The simulation\nresults, using the FashionMNIST dataset, demonstrate over 10% and 19%\nimprovement in global model accuracy compared to state-of-the-art methods\nPAPAYA and FedAsync, respectively. Our dynamic aggregation method allows\nreliable global model training despite limiting client resources and\nstatistical data heterogeneity. This improves robustness and scalability for\nreal-world FL deployments.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13369", "title": "Dynamic Epistemic Logic of Resource Bounded Information Mining Agents", "abstract": "Logics for resource-bounded agents have been getting more and more attention\nin recent years since they provide us with more realistic tools for modelling\nand reasoning about multi-agent systems. While many existing approaches are\nbased on the idea of agents as imperfect reasoners, who must spend their\nresources to perform logical inference, this is not the only way to introduce\nresource constraints into logical settings. In this paper we study agents as\nperfect reasoners, who may purchase a new piece of information from a\ntrustworthy source. For this purpose we propose dynamic epistemic logic for\nsemi-public queries for resource-bounded agents. In this logic (groups of)\nagents can perform a query (ask a question) about whether some formula is true\nand receive a correct answer. These queries are called semi-public, because the\nvery fact of the query is public, while the answer is private. We also assume\nthat every query has a cost and every agent has a budget constraint. Finally,\nour framework allows us to reason about group queries, in which agents may\nshare resources to obtain a new piece of information together. We demonstrate\nthat our logic is complete, decidable and has an efficient model checking\nprocedure.", "field": "Computer Science", "categories": "cs.LO"}, {"arxiv_id": "2401.13371", "title": "SVARM-IQ: Efficient Approximation of Any-order Shapley Interactions\n  through Stratification", "abstract": "Addressing the limitations of individual attribution scores via the Shapley\nvalue (SV), the field of explainable AI (XAI) has recently explored intricate\ninteractions of features or data points. In particular,\n\\mbox{extensions}~of~the SV, such as the Shapley Interaction Index (SII), have\nbeen proposed as a measure to still benefit from the axiomatic basis of the SV.\nHowever, similar to the SV, their exact computation remains computationally\nprohibitive. Hence, we propose with SVARM-IQ a sampling-based approach to\nefficiently approximate Shapley-based interaction indices of any order.\nSVARM-IQ can be applied to a broad class of interaction indices, including the\nSII, by leveraging a novel stratified representation. We provide non-asymptotic\ntheoretical guarantees on its approximation quality and empirically demonstrate\nthat SVARM-IQ achieves state-of-the-art estimation results in practical XAI\nscenarios on different model classes and application domains.", "field": "Computer Science", "categories": "cs.GT"}, {"arxiv_id": "2401.13376", "title": "\\texttt{lymph}: discontinuous poLYtopal methods for Multi-PHysics\n  differential problems", "abstract": "We present the library \\texttt{lymph} for the finite element numerical\ndiscretization of coupled multi-physics problems. \\texttt{lymph} is a Matlab\nlibrary for the discretization of partial differential equations based on\nhigh-order discontinuous Galerkin methods on polytopal grids (PolyDG) for\nspatial discretization coupled with suitable finite-difference time marching\nschemes. The objective of the paper is to introduce the library by describing\nit in terms of installation, input/output data, and code structure,\nhighlighting -- when necessary -- key implementation aspects related to the\nmethod. A user guide, proceeding step-by-step in the implementation and\nsolution of a Poisson problem, is also provided. In the last part of the paper,\nwe show the results obtained for several differential problems, namely the\nPoisson problem, the heat equation, and the elastodynamics system. Through\nthese examples, we show the convergence properties and highlight some of the\nmain features of the proposed method, i.e. geometric flexibility, high-order\naccuracy, and robustness with respect to heterogeneous physical parameters.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.13382", "title": "A proof theory of right-linear (omega-)grammars via cyclic proofs", "abstract": "Right-linear (or left-linear) grammars are a well-known class of context-free\ngrammars computing just the regular languages. They may naturally be written as\nexpressions with (least) fixed points but with products restricted to letters\nas left arguments, giving an alternative to the syntax of regular expressions.\nIn this work, we investigate the resulting logical theory of this syntax.\nNamely, we propose a theory of right-linear algebras (RLA) over of this syntax\nand a cyclic proof system CRLA for reasoning about them.\n  We show that CRLA is sound and complete for the intended model of regular\nlanguages. From here we recover the same completeness result for RLA by\nextracting inductive invariants from cyclic proofs, rendering the model of\nregular languages the free right-linear algebra.\n  Finally, we extend system CRLA by greatest fixed points, nuCRLA, naturally\nmodelled by languages of omega-words thanks to right-linearity. We show a\nsimilar soundness and completeness result of (the guarded fragment of) nuCRLA\nfor the model of omega-regular languages, employing game theoretic techniques.", "field": "Computer Science", "categories": "cs.LO,cs.FL,math.LO"}, {"arxiv_id": "2401.13384", "title": "Randomized learning-augmented auctions with revenue guarantees", "abstract": "We consider the fundamental problem of designing a truthful single-item\nauction with the challenging objective of extracting a large fraction of the\nhighest agent valuation as revenue. Following a recent trend in algorithm\ndesign, we assume that the agent valuations belong to a known interval, and a\n(possibly erroneous) prediction for the highest valuation is available. Then,\nauction design aims for high consistency and robustness, meaning that, for\nappropriate pairs of values $\\gamma$ and $\\rho$, the extracted revenue should\nbe at least a $\\gamma$- or $\\rho$-fraction of the highest valuation when the\nprediction is correct for the input instance or not. We characterize all pairs\nof parameters $\\gamma$ and $\\rho$ so that a randomized $\\gamma$-consistent and\n$\\rho$-robust auction exists. Furthermore, for the setting in which robustness\ncan be a function of the prediction error, we give sufficient and necessary\nconditions for the existence of robust auctions and present randomized auctions\nthat extract a revenue that is only a polylogarithmic (in terms of the\nprediction error) factor away from the highest agent valuation.", "field": "Computer Science", "categories": "cs.GT"}, {"arxiv_id": "2401.13386", "title": "Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain", "abstract": "Face recognition technology has been deployed in various real-life\napplications. The most sophisticated deep learning-based face recognition\nsystems rely on training millions of face images through complex deep neural\nnetworks to achieve high accuracy. It is quite common for clients to upload\nface images to the service provider in order to access the model inference.\nHowever, the face image is a type of sensitive biometric attribute tied to the\nidentity information of each user. Directly exposing the raw face image to the\nservice provider poses a threat to the user's privacy. Current\nprivacy-preserving approaches to face recognition focus on either concealing\nvisual information on model input or protecting model output face embedding.\nThe noticeable drop in recognition accuracy is a pitfall for most methods. This\npaper proposes a hybrid frequency-color fusion approach to reduce the input\ndimensionality of face recognition in the frequency domain. Moreover, sparse\ncolor information is also introduced to alleviate significant accuracy\ndegradation after adding differential privacy noise. Besides, an\nidentity-specific embedding mapping scheme is applied to protect original face\nembedding by enlarging the distance among identities. Lastly, secure multiparty\ncomputation is implemented for safely computing the embedding distance during\nmodel inference. The proposed method performs well on multiple widely used\nverification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy\nthan the state-of-the-art in the 1:N verification scenario.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13387", "title": "A Mathematical Theory of Semantic Communication", "abstract": "The year 1948 witnessed the historic moment of the birth of classic\ninformation theory (CIT). Guided by CIT, modern communication techniques have\napproached the theoretic limitations, such as, entropy function $H(U)$, channel\ncapacity $C=\\max_{p(x)}I(X;Y)$ and rate-distortion function\n$R(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d(x,\\hat{x})\\leq D} I(X;\\hat{X})$. Semantic\ncommunication paves a new direction for future communication techniques whereas\nthe guided theory is missed. In this paper, we try to establish a systematic\nframework of semantic information theory (SIT). We investigate the behavior of\nsemantic communication and find that synonym is the basic feature so we define\nthe synonymous mapping between semantic information and syntactic information.\nStemming from this core concept, synonymous mapping, we introduce the measures\nof semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down\nsemantic mutual information $I^s(\\tilde{X};\\tilde{Y})$\n$(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity\n$C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion\nfunction\n$R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq\nD}I_s(\\tilde{X};\\hat{\\tilde{X}})$. Furthermore, we prove three coding theorems\nof SIT by using random coding and (jointly) typical decoding/encoding, that is,\nthe semantic source coding theorem, semantic channel coding theorem, and\nsemantic rate-distortion coding theorem. We find that the limits of SIT are\nextended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$,\n$C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of\nsemantic information theory. In addition, we discuss the semantic information\nmeasures in the continuous case. Especially, for band-limited Gaussian channel,\nwe obtain a new channel capacity formula,\n$C_s=B\\log\\left[S^4\\left(1+\\frac{P}{N_0B}\\right)\\right]$ with the synonymous\nlength $S$.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13388", "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional\n  Diffusion", "abstract": "Existing text-to-image diffusion models primarily generate images from text\nprompts. However, the inherent conciseness of textual descriptions poses\nchallenges in faithfully synthesizing images with intricate details, such as\nspecific entities or scenes. This paper presents \\textbf{UNIMO-G}, a simple\nmultimodal conditional diffusion framework that operates on multimodal prompts\nwith interleaved textual and visual inputs, which demonstrates a unified\nability for both text-driven and subject-driven image generation. UNIMO-G\ncomprises two core components: a Multimodal Large Language Model (MLLM) for\nencoding multimodal prompts, and a conditional denoising diffusion network for\ngenerating images based on the encoded multimodal input. We leverage a\ntwo-stage training strategy to effectively train the framework: firstly\npre-training on large-scale text-image pairs to develop conditional image\ngeneration capabilities, and then instruction tuning with multimodal prompts to\nachieve unified image generation proficiency. A well-designed data processing\npipeline involving language grounding and image segmentation is employed to\nconstruct multi-modal prompts. UNIMO-G excels in both text-to-image generation\nand zero-shot subject-driven synthesis, and is notably effective in generating\nhigh-fidelity images from complex multimodal prompts involving multiple image\nentities.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.1339", "title": "Memoryless Strategies in Stochastic Reachability Games", "abstract": "We study concurrent stochastic reachability games played on finite graphs.\nTwo players, Max and Min, seek respectively to maximize and minimize the\nprobability of reaching a set of target states. We prove that Max has a\nmemoryless strategy that is optimal from all states that have an optimal\nstrategy. Our construction provides an alternative proof of this result by\nBordais, Bouyer and Le Roux, and strengthens it, as we allow Max's action sets\nto be countably infinite.", "field": "Computer Science", "categories": "cs.LO,cs.GT,math.PR"}, {"arxiv_id": "2401.13391", "title": "Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely\n  on between-group metrics", "abstract": "Artificial Intelligence (AI) finds widespread applications across various\ndomains, sparking concerns about fairness in its deployment. While fairness in\nAI remains a central concern, the prevailing discourse often emphasizes\noutcome-based metrics without a nuanced consideration of the differential\nimpacts within subgroups. Bias mitigation techniques do not only affect the\nranking of pairs of instances across sensitive groups, but often also\nsignificantly affect the ranking of instances within these groups. Such changes\nare hard to explain and raise concerns regarding the validity of the\nintervention. Unfortunately, these effects largely remain under the radar in\nthe accuracy-fairness evaluation framework that is usually applied. This paper\nchallenges the prevailing metrics for assessing bias mitigation techniques,\narguing that they do not take into account the changes within-groups and that\nthe resulting prediction labels fall short of reflecting real-world scenarios.\nWe propose a paradigm shift: initially, we should focus on generating the most\nprecise ranking for each subgroup. Following this, individuals should be chosen\nfrom these rankings to meet both fairness standards and practical\nconsiderations.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13394", "title": "Determining hulls of generalized Reed-Solomon codes from algebraic\n  geometry codes", "abstract": "In this paper, we provide conditions that hulls of generalized Reed-Solomon\n(GRS) codes are also GRS codes from algebraic geometry codes. If the conditions\nare not satisfied, we provide a method of linear algebra to find the bases of\nhulls of GRS codes and give formulas to compute their dimensions. Besides, we\nexplain that the conditions are too good to be improved by some examples.\nMoreover, we show self-orthogonal and self-dual GRS codes.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13398", "title": "Text Categorization Can Enhance Domain-Agnostic Stopword Extraction", "abstract": "This paper investigates the role of text categorization in streamlining\nstopword extraction in natural language processing (NLP), specifically focusing\non nine African languages alongside French. By leveraging the MasakhaNEWS,\nAfrican Stopwords Project, and MasakhaPOS datasets, our findings emphasize that\ntext categorization effectively identifies domain-agnostic stopwords with over\n80% detection success rate for most examined languages. Nevertheless,\nlinguistic variances result in lower detection rates for certain languages.\nInterestingly, we find that while over 40% of stopwords are common across news\ncategories, less than 15% are unique to a single category. Uncommon stopwords\nadd depth to text but their classification as stopwords depends on context.\nTherefore combining statistical and linguistic approaches creates comprehensive\nstopword lists, highlighting the value of our hybrid method. This research\nenhances NLP for African languages and underscores the importance of text\ncategorization in stopword extraction.", "field": "Computer Science", "categories": "cs.CL,cs.LG"}, {"arxiv_id": "2401.134", "title": "On fixed point theory in partially ordered sets and an application to\n  asymptotic complexity of algorithms", "abstract": "The celebrated Kleene fixed point theorem is crucial in the mathematical\nmodelling of recursive specifications in Denotational Semantics. In this paper\nwe discuss whether the hypothesis of the aforementioned result can be weakened.\nAn affirmative answer to the aforesaid inquiry is provided so that a\ncharacterization of those properties that a self-mapping must satisfy in order\nto guarantee that its set of fixed points is non-empty when no notion of\ncompleteness are assumed to be satisfied by the partially ordered set.\nMoreover, the case in which the partially ordered set is coming from a\nquasi-metric space is treated in depth. Finally, an application of the exposed\ntheory is obtained. Concretely, a mathematical method to discuss the asymptotic\ncomplexity of those algorithms whose running time of computing fulfills a\nrecurrence equation is presented. Moreover, the aforesaid method retrieves the\nfixed point based methods that appear in the literature for asymptotic\ncomplexity analysis of algorithms. However, our new method improves the\naforesaid methods because it imposes fewer requirements than those that have\nbeen assumed in the literature and, in addition, it allows to state\nsimultaneously upper and lower asymptotic bounds for the running time\ncomputing.", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13405", "title": "Synthetic data enables faster annotation and robust segmentation for\n  multi-object grasping in clutter", "abstract": "Object recognition and object pose estimation in robotic grasping continue to\nbe significant challenges, since building a labelled dataset can be time\nconsuming and financially costly in terms of data collection and annotation. In\nthis work, we propose a synthetic data generation method that minimizes human\nintervention and makes downstream image segmentation algorithms more robust by\ncombining a generated synthetic dataset with a smaller real-world dataset\n(hybrid dataset). Annotation experiments show that the proposed synthetic scene\ngeneration can diminish labelling time dramatically. RGB image segmentation is\ntrained with hybrid dataset and combined with depth information to produce\npixel-to-point correspondence of individual segmented objects. The object to\ngrasp is then determined by the confidence score of the segmentation algorithm.\nPick-and-place experiments demonstrate that segmentation trained on our hybrid\ndataset (98.9%, 70%) outperforms the real dataset and a publicly available\ndataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping\nsuccess rate, respectively. Supplementary material is available at\nhttps://sites.google.com/view/synthetic-dataset-generation.", "field": "Computer Science", "categories": "cs.CV,cs.RO"}, {"arxiv_id": "2401.13407", "title": "Increasing, not Diminishing: Investigating the Returns of Highly\n  Maintainable Code", "abstract": "Understanding and effectively managing Technical Debt (TD) remains a vital\nchallenge in software engineering. While many studies on code-level TD have\nbeen published, few illustrate the business impact of low-quality source code.\nIn this study, we combine two publicly available datasets to study the\nassociation between code quality on the one hand, and defect count and\nimplementation time on the other hand. We introduce a value-creation model,\nderived from regression analyses, to explore relative changes from a baseline.\nOur results show that the associations vary across different intervals of code\nquality. Furthermore, the value model suggests strong non-linearities at the\nextremes of the code quality spectrum. Most importantly, the model suggests\namplified returns on investment in the upper end. We discuss the findings\nwithin the context of the \"broken windows\" theory and recommend organizations\nto diligently prevent the introduction of code smells in files with high churn.\nFinally, we argue that the value-creation model can be used to initiate\ndiscussions regarding the return on investment in refactoring efforts.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.13408", "title": "Causal Perception", "abstract": "Perception occurs when two individuals interpret the same information\ndifferently. Despite being a known phenomenon with implications for bias in\ndecision-making, as individuals' experience determines interpretation,\nperception remains largely overlooked in automated decision-making (ADM)\nsystems. In particular, it can have considerable effects on the fairness or\nfair usage of an ADM system, as fairness itself is context-specific and its\ninterpretation dependent on who is judging. In this work, we formalize\nperception under causal reasoning to capture the act of interpretation by an\nindividual. We also formalize individual experience as additional causal\nknowledge that comes with and is used by an individual. Further, we define and\ndiscuss loaded attributes, which are attributes prone to evoke perception.\nSensitive attributes, such as gender and race, are clear examples of loaded\nattributes. We define two kinds of causal perception, unfaithful and\ninconsistent, based on the causal properties of faithfulness and consistency.\nWe illustrate our framework through a series of decision-making examples and\ndiscuss relevant fairness applications. The goal of this work is to position\nperception as a parameter of interest, useful for extending the standard,\nsingle interpretation ADM problem formulation.", "field": "Computer Science", "categories": "cs.AI,cs.CY,cs.HC"}, {"arxiv_id": "2401.1341", "title": "How to Forget Clients in Federated Online Learning to Rank?", "abstract": "Data protection legislation like the European Union's General Data Protection\nRegulation (GDPR) establishes the \\textit{right to be forgotten}: a user\n(client) can request contributions made using their data to be removed from\nlearned models. In this paper, we study how to remove the contributions made by\na client participating in a Federated Online Learning to Rank (FOLTR) system.\nIn a FOLTR system, a ranker is learned by aggregating local updates to the\nglobal ranking model. Local updates are learned in an online manner at a\nclient-level using queries and implicit interactions that have occurred within\nthat specific client. By doing so, each client's local data is not shared with\nother clients or with a centralised search service, while at the same time\nclients can benefit from an effective global ranking model learned from\ncontributions of each client in the federation.\n  In this paper, we study an effective and efficient unlearning method that can\nremove a client's contribution without compromising the overall ranker\neffectiveness and without needing to retrain the global ranker from scratch. A\nkey challenge is how to measure whether the model has unlearned the\ncontributions from the client $c^*$ that has requested removal. For this, we\ninstruct $c^*$ to perform a poisoning attack (add noise to this client updates)\nand then we measure whether the impact of the attack is lessened when the\nunlearning process has taken place. Through experiments on four datasets, we\ndemonstrate the effectiveness and efficiency of the unlearning strategy under\ndifferent combinations of parameter settings.", "field": "Computer Science", "categories": "cs.CR,cs.IR,cs.LG"}, {"arxiv_id": "2401.13414", "title": "GTAutoAct: An Automatic Datasets Generation Framework Based on Game\n  Engine Redevelopment for Action Recognition", "abstract": "Current datasets for action recognition tasks face limitations stemming from\ntraditional collection and generation methods, including the constrained range\nof action classes, absence of multi-viewpoint recordings, limited diversity,\npoor video quality, and labor-intensive manually collection. To address these\nchallenges, we introduce GTAutoAct, a innovative dataset generation framework\nleveraging game engine technology to facilitate advancements in action\nrecognition. GTAutoAct excels in automatically creating large-scale,\nwell-annotated datasets with extensive action classes and superior video\nquality. Our framework's distinctive contributions encompass: (1) it\ninnovatively transforms readily available coordinate-based 3D human motion into\nrotation-orientated representation with enhanced suitability in multiple\nviewpoints; (2) it employs dynamic segmentation and interpolation of rotation\nsequences to create smooth and realistic animations of action; (3) it offers\nextensively customizable animation scenes; (4) it implements an autonomous\nvideo capture and processing pipeline, featuring a randomly navigating camera,\nwith auto-trimming and labeling functionalities. Experimental results\nunderscore the framework's robustness and highlights its potential to\nsignificantly improve action recognition model training.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13416", "title": "Characterizing Perspective Error in Voxel-Based Lidar Scan Matching", "abstract": "This paper quantifies an error source that limits the accuracy of lidar scan\nmatching, particularly for voxel-based methods. Lidar scan matching, which is\nused in dead reckoning (also known as lidar odometry) and mapping, computes the\nrotation and translation that best align a pair of point clouds. Perspective\nerrors occur when a scene is viewed from different angles, with different\nsurfaces becoming visible or occluded from each viewpoint. To explain\nperspective anomalies observed in data, this paper models perspective errors\nfor two objects representative of urban landscapes: a cylindrical column and a\ndual-wall corner. For each object, we provide an analytical model of the\nperspective error for voxel-based lidar scan matching. We then analyze how\nperspective errors accumulate as a lidar-equipped vehicle moves past these\nobjects.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13418", "title": "Serial fusion of multi-modal biometric systems", "abstract": "Serial, or sequential, fusion of multiple biometric matchers has been not\nthoroughly investigated so far. However, this approach exhibits some advantages\nwith respect to the widely adopted parallel approaches. In this paper, we\npropose a novel theoretical framework for the assessment of performance of such\nsystems, based on a previous work of the authors. Benefits in terms of\nperformance are theoretically evaluated, as well as estimation errors in the\nmodel parameters computation. Model is analyzed from the viewpoint of its pros\nand cons, by mean of preliminary experiments performed on NIST Biometric Score\nSet 1.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.1342", "title": "Distributed network for measuring climatic parameters in heterogeneous\n  environments: Application in a greenhouse", "abstract": "In Mediterranean countries of Southern Europe, the climatic conditions are\nusually favourable to cultivate greenhouse vegetables but not always for\nworkers. The aim of this study was to design a network of weather stations\ncapable of gathering data of environmental parameters related to the wellbeing\nof workers in greenhouses in south-eastern Spain. The unevenness of the thermal\nenvironment was studied both vertically as well as horizontally following\nguideline ISO 7726. The results indicate that the greenhouse should be\nconsidered a heterogeneous environment, implying that, for an evaluation of the\nenvironmental conditions related to thermal stress of the workers inside the\ngreenhouse, measurements should be taken at different points within the\ngreenhouse at three heights (ankle, abdomen, and head).", "field": "Computer Science", "categories": "cs.DC"}, {"arxiv_id": "2401.13428", "title": "Numerical Approximations and Convergence Analysis of Piecewise Diffusion\n  Markov Processes, with Application to Glioma Cell Migration", "abstract": "In this paper, we focus on numerical approximations of Piecewise Diffusion\nMarkov Processes (PDifMPs), particularly when the explicit flow maps are\nunavailable. Our approach is based on the thinning method for modelling the\njump mechanism and combines the Euler-Maruyama scheme to approximate the\nunderlying flow dynamics. For the proposed approximation schemes, we study both\nthe mean-square and weak convergence. Weak convergence of the algorithms is\nestablished by a martingale problem formulation. Moreover, we employ these\nresults to simulate the migration patterns exhibited by moving glioma cells at\nthe microscopic level. Further, we develop and implement a splitting method for\nthis PDifMP model and employ both the Thinned Euler-Maruyama and the splitting\nscheme in our simulation example, allowing us to compare both methods.", "field": "Computer Science", "categories": "math.NA,cs.NA,math.PR"}, {"arxiv_id": "2401.13429", "title": "Detection of Correlated Random Vectors", "abstract": "In this paper, we investigate the problem of deciding whether two standard\nnormal random vectors $\\mathsf{X}\\in\\mathbb{R}^{n}$ and\n$\\mathsf{Y}\\in\\mathbb{R}^{n}$ are correlated or not. This is formulated as a\nhypothesis testing problem, where under the null hypothesis, these vectors are\nstatistically independent, while under the alternative, $\\mathsf{X}$ and a\nrandomly and uniformly permuted version of $\\mathsf{Y}$, are correlated with\ncorrelation $\\rho$. We analyze the thresholds at which optimal testing is\ninformation-theoretically impossible and possible, as a function of $n$ and\n$\\rho$. To derive our information-theoretic lower bounds, we develop a novel\ntechnique for evaluating the second moment of the likelihood ratio using an\northogonal polynomials expansion, which among other things, reveals a\nsurprising connection to integer partition functions. We also study a\nmulti-dimensional generalization of the above setting, where rather than two\nvectors we observe two databases/matrices, and furthermore allow for partial\ncorrelations between these two.", "field": "Computer Science", "categories": "cs.IT,cs.LG,math.IT,math.ST,stat.TH"}, {"arxiv_id": "2401.13432", "title": "Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction\n  and Beyond", "abstract": "Thin-plate spline (TPS) is a principal warp that allows for representing\nelastic, nonlinear transformation with control point motions. With the increase\nof control points, the warp becomes increasingly flexible but usually\nencounters a bottleneck caused by undesired issues, e.g., content distortion.\nIn this paper, we explore generic applications of TPS in single-image-based\nwarping tasks, such as rotation correction, rectangling, and portrait\ncorrection. To break this bottleneck, we propose the coupled thin-plate spline\nmodel (CoupledTPS), which iteratively couples multiple TPS with limited control\npoints into a more flexible and powerful transformation. Concretely, we first\ndesign an iterative search to predict new control points according to the\ncurrent latent condition. Then, we present the warping flow as a bridge for the\ncoupling of different TPS transformations, effectively eliminating\ninterpolation errors caused by multiple warps. Besides, in light of the\nlaborious annotation cost, we develop a semi-supervised learning scheme to\nimprove warping quality by exploiting unlabeled data. It is formulated through\ndual transformation between the searched control points of unlabeled data and\nits graphic augmentation, yielding an implicit correction consistency\nconstraint. Finally, we collect massive unlabeled data to exhibit the benefit\nof our semi-supervised scheme in rotation correction. Extensive experiments\ndemonstrate the superiority and universality of CoupledTPS over the existing\nstate-of-the-art (SoTA) solutions for rotation correction and beyond. The code\nand data will be available at https://github.com/nie-lang/CoupledTPS.", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13434", "title": "Query Exposure Prediction for Groups of Documents in Rankings", "abstract": "The main objective of an Information Retrieval system is to provide a user\nwith the most relevant documents to the user's query. To do this, modern IR\nsystems typically deploy a re-ranking pipeline in which a set of documents is\nretrieved by a lightweight first-stage retrieval process and then re-ranked by\na more effective but expensive model. However, the success of a re-ranking\npipeline is heavily dependent on the performance of the first stage retrieval,\nsince new documents are not usually identified during the re-ranking stage.\nMoreover, this can impact the amount of exposure that a particular group of\ndocuments, such as documents from a particular demographic group, can receive\nin the final ranking. For example, the fair allocation of exposure becomes more\nchallenging or impossible if the first stage retrieval returns too few\ndocuments from certain groups, since the number of group documents in the\nranking affects the exposure more than the documents' positions. With this in\nmind, it is beneficial to predict the amount of exposure that a group of\ndocuments is likely to receive in the results of the first stage retrieval\nprocess, in order to ensure that there are a sufficient number of documents\nincluded from each of the groups. In this paper, we introduce the novel task of\nquery exposure prediction (QEP). Specifically, we propose the first approach\nfor predicting the distribution of exposure that groups of documents will\nreceive for a given query. Our new approach, called GEP, uses lexical\ninformation from individual groups of documents to estimate the exposure the\ngroups will receive in a ranking. Our experiments on the TREC 2021 and 2022\nFair Ranking Track test collections show that our proposed GEP approach results\nin exposure predictions that are up to 40 % more accurate than the predictions\nof adapted existing query performance prediction and resource allocation\napproaches.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13438", "title": "Keeping Energy-Neutral Devices Operational: a Coherent Massive\n  Beamforming Approach", "abstract": "Keeping the batteries on the shelf: this is the holy grail for low-cost\nInternet of Things (IoT) nodes. In this paper we study the potential of radio\nfrequency (RF)-based wireless power transfer implementing coherent beamforming\nwith many antennas to realize this ambitious target. We optimize the deployment\nof the antennas to charge electronic shelf labels (ESLs), considering actual\nregulatory constraints. The results confirm the feasibility to create power\nspots that are sufficient to keep the high density of battery-less devices\noperational.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13439", "title": "Model Predictive Wave Disturbance Rejection for Underwater Soft Robotic\n  Manipulators", "abstract": "Inspired by the octopus and other animals living in water, soft robots should\nnaturally lend themselves to underwater operations, as supported by encouraging\nvalidations in deep water scenarios. This work deals with equipping soft arms\nwith the intelligence necessary to move precisely in wave-dominated\nenvironments, such as shallow waters where marine renewable devices are\nlocated. This scenario is substantially more challenging than calm deep water\nsince, at low operational depths, hydrodynamic wave disturbances can represent\na significant impediment. We propose a control strategy based on Nonlinear\nModel Predictive Control that can account for wave disturbances explicitly,\noptimising control actions by considering an estimate of oncoming hydrodynamic\nloads. The proposed strategy is validated through a set of tasks covering\nset-point regulation, trajectory tracking and mechanical failure compensation,\nall under a broad range of varying significant wave heights and peak spectral\nperiods. The proposed control methodology displays positional error reductions\nas large as 84% with respect to a baseline controller, proving the\neffectiveness of the method. These initial findings present a first step in the\ndevelopment and deployment of soft manipulators for performing tasks in\nhazardous water environments.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY"}, {"arxiv_id": "2401.13441", "title": "Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance\n  Control", "abstract": "Integrating Brain-Machine Interfaces into non-clinical applications like\nrobot motion control remains difficult - despite remarkable advancements in\nclinical settings. Specifically, EEG-based motor imagery systems are still\nerror-prone, posing safety risks when rigid robots operate near humans. This\nwork presents an alternative pathway towards safe and effective operation by\ncombining wearable EEG with physically embodied safety in soft robots. We\nintroduce and test a pipeline that allows a user to move a soft robot's end\neffector in real time via brain waves that are measured by as few as three EEG\nchannels. A robust motor imagery algorithm interprets the user's intentions to\nmove the position of a virtual attractor to which the end effector is\nattracted, thanks to a new Cartesian impedance controller. We specifically\nfocus here on planar soft robot-based architected metamaterials, which require\nthe development of a novel control architecture to deal with the peculiar\nnonlinearities - e.g., non-affinity in control. We preliminarily but\nquantitatively evaluate the approach on the task of setpoint regulation. We\nobserve that the user reaches the proximity of the setpoint in 66% of steps and\nthat for successful steps, the average response time is 21.5s. We also\ndemonstrate the execution of simple real-world tasks involving interaction with\nthe environment, which would be extremely hard to perform if it were not for\nthe robot's softness.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY"}, {"arxiv_id": "2401.13442", "title": "Finite-Precision Arithmetic Transceiver for Massive MIMO Systems", "abstract": "Efficient implementation of massive multiple-input-multiple-output (MIMO)\ntransceivers is essential for the next-generation wireless networks. To reduce\nthe high computational complexity of the massive MIMO transceiver, in this\npaper, we propose a new massive MIMO architecture using finite-precision\narithmetic. First, we conduct the rounding error analysis and derive the lower\nbound of the achievable rate for single-input-multiple-output (SIMO) using\nmaximal ratio combining (MRC) and multiple-input-single-output (MISO) systems\nusing maximal ratio transmission (MRT) with finite-precision arithmetic. Then,\nconsidering the multi-user scenario, the rounding error analysis of\nzero-forcing (ZF) detection and precoding is derived by using the normal\nequations (NE) method. The corresponding lower bounds of the achievable sum\nrate are also derived and asymptotic analyses are presented. Built upon\ninsights from these analyses and lower bounds, we propose a mixed-precision\narchitecture for massive MIMO systems to offset performance gaps due to\nfinite-precision arithmetic. The corresponding analysis of rounding errors and\ncomputational costs is obtained. Simulation results validate the derived bounds\nand underscore the superiority of the proposed mixed-precision architecture to\nthe conventional structure.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.13444", "title": "Clue-Guided Path Exploration: An Efficient Knowledge Base\n  Question-Answering Framework with Low Computational Resource Consumption", "abstract": "In recent times, large language models (LLMs) have showcased remarkable\ncapabilities. However, updating their knowledge poses challenges, potentially\nleading to inaccuracies when confronted with unfamiliar queries. While\nintegrating knowledge graphs with LLMs has been explored, existing approaches\ntreat LLMs as primary decision-makers, imposing high demands on their\ncapabilities. This is particularly unsuitable for LLMs with lower computational\ncosts and relatively poorer performance. In this paper, we introduce a\nClue-Guided Path Exploration framework (CGPE) that efficiently merges a\nknowledge base with an LLM, placing less stringent requirements on the model's\ncapabilities. Inspired by the method humans use to manually retrieve knowledge,\nCGPE employs information from the question as clues to systematically explore\nthe required knowledge path within the knowledge base. Experiments on\nopen-source datasets reveal that CGPE outperforms previous methods and is\nhighly applicable to LLMs with fewer parameters. In some instances, even\nChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.\nFurthermore, the results indicate a minimal invocation frequency of CGPE on\nLLMs, suggesting reduced computational overhead. For organizations and\nindividuals facing constraints in computational resources, our research offers\nsignificant practical value.", "field": "Computer Science", "categories": "cs.CL,cs.AI"}, {"arxiv_id": "2401.13447", "title": "Symbolic Equation Solving via Reinforcement Learning", "abstract": "Machine-learning methods are gradually being adopted in a great variety of\nsocial, economic, and scientific contexts, yet they are notorious for\nstruggling with exact mathematics. A typical example is computer algebra, which\nincludes tasks like simplifying mathematical terms, calculating formal\nderivatives, or finding exact solutions of algebraic equations. Traditional\nsoftware packages for these purposes are commonly based on a huge database of\nrules for how a specific operation (e.g., differentiation) transforms a certain\nterm (e.g., sine function) into another one (e.g., cosine function). Thus far,\nthese rules have usually needed to be discovered and subsequently programmed by\nhumans. Focusing on the paradigmatic example of solving linear equations in\nsymbolic form, we demonstrate how the process of finding elementary\ntransformation rules and step-by-step solutions can be automated using\nreinforcement learning with deep neural networks.", "field": "Computer Science", "categories": "cs.LG,cs.SC"}, {"arxiv_id": "2401.13448", "title": "Decentralized Collaborative Learning with Adaptive Reference Data for\n  On-Device POI Recommendation", "abstract": "In Location-based Social Networks, Point-of-Interest (POI) recommendation\nhelps users discover interesting places. There is a trend to move from the\ncloud-based model to on-device recommendations for privacy protection and\nreduced server reliance. Due to the scarcity of local user-item interactions on\nindividual devices, solely relying on local instances is not adequate.\nCollaborative Learning (CL) emerges to promote model sharing among users, where\nreference data is an intermediary that allows users to exchange their soft\ndecisions without directly sharing their private data or parameters, ensuring\nprivacy and benefiting from collaboration. However, existing CL-based\nrecommendations typically use a single reference for all users. Reference data\nvaluable for one user might be harmful to another, given diverse user\npreferences. Users may not offer meaningful soft decisions on items outside\ntheir interest scope. Consequently, using the same reference data for all\ncollaborations can impede knowledge exchange and lead to sub-optimal\nperformance. To address this gap, we introduce the Decentralized Collaborative\nLearning with Adaptive Reference Data (DARD) framework, which crafts adaptive\nreference data for effective user collaboration. It first generates a\ndesensitized public reference data pool with transformation and probability\ndata generation methods. For each user, the selection of adaptive reference\ndata is executed in parallel by training loss tracking and influence function.\nLocal models are trained with individual private data and collaboratively with\nthe geographical and semantic neighbors. During the collaboration between two\nusers, they exchange soft decisions based on a combined set of their adaptive\nreference data. Our evaluations across two real-world datasets highlight DARD's\nsuperiority in recommendation performance and addressing the scarcity of\navailable reference data.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13451", "title": "Experimental validation of ultra-shortened 3D finite element models for\n  frequency-domain analyses of three-core armored cables", "abstract": "Recently, large offshore wind power plants have been installed far from the\nshore, using long HVAC three-core armored cables to export power. Its high\ncapacitance may contribute to the appearance of unwanted phenomena, such as\novervoltages or resonances at low frequencies. To adequately assess these\nproblems, detailed and reliable cable models are required to develop\ntime-domain/frequency-domain analyses on this type of cables. This paper\npresents, for the first time in the literature, an assessment on the\nperformance of 3D finite element method-based (3D-FEM) models for developing\nfrequency-domain analyses on three-core armored cables, confronting simulation\nresults with experimental measurements found in the literature for three real\ncables. To this aim, a simplified ultra-shortened 3D-FEM model is proposed to\nreduce the simulation time during frequency sweeps, through which relevant\naspects never analyzed before with frequency-domain 3D-FEM simulations are\naddressed, such as total losses, induced sheath current, magnetic field around\nthe power cable, positive and zero sequence harmonic impedances, as well as\nresonant frequencies. Also, a time-domain example derived from the\nfrequency-domain analysis is provided. Remarkable results are obtained when\ncomparing computed values and measurements, presenting the simplified\nultra-shortened 3DFEM model as a valuable tool for the frequency-domain\nanalysis of these cables.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.1346", "title": "Multi-Agent Diagnostics for Robustness via Illuminated Diversity", "abstract": "In the rapidly advancing field of multi-agent systems, ensuring robustness in\nunfamiliar and adversarial settings is crucial. Notwithstanding their\noutstanding performance in familiar environments, these systems often falter in\nnew situations due to overfitting during the training phase. This is especially\npronounced in settings where both cooperative and competitive behaviours are\npresent, encapsulating a dual nature of overfitting and generalisation\nchallenges. To address this issue, we present Multi-Agent Diagnostics for\nRobustness via Illuminated Diversity (MADRID), a novel approach for generating\ndiverse adversarial scenarios that expose strategic vulnerabilities in\npre-trained multi-agent policies. Leveraging the concepts from open-ended\nlearning, MADRID navigates the vast space of adversarial settings, employing a\ntarget policy's regret to gauge the vulnerabilities of these settings. We\nevaluate the effectiveness of MADRID on the 11vs11 version of Google Research\nFootball, one of the most complex environments for multi-agent reinforcement\nlearning. Specifically, we employ MADRID for generating a diverse array of\nadversarial settings for TiZero, the state-of-the-art approach which \"masters\"\nthe game through 45 days of training on a large-scale distributed\ninfrastructure. We expose key shortcomings in TiZero's tactical\ndecision-making, underlining the crucial importance of rigorous evaluation in\nmulti-agent systems.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.MA"}, {"arxiv_id": "2401.13462", "title": "Growing from Exploration: A self-exploring framework for robots based on\n  foundation models", "abstract": "Intelligent robot is the ultimate goal in the robotics field. Existing works\nleverage learning-based or optimization-based methods to accomplish\nhuman-defined tasks. However, the challenge of enabling robots to explore\nvarious environments autonomously remains unresolved. In this work, we propose\na framework named GExp, which enables robots to explore and learn autonomously\nwithout human intervention. To achieve this goal, we devise modules including\nself-exploration, knowledge-base-building, and close-loop feedback based on\nfoundation models. Inspired by the way that infants interact with the world,\nGExp encourages robots to understand and explore the environment with a series\nof self-generated tasks. During the process of exploration, the robot will\nacquire skills from beneficial experiences that are useful in the future. GExp\nprovides robots with the ability to solve complex tasks through\nself-exploration. GExp work is independent of prior interactive knowledge and\nhuman intervention, allowing it to adapt directly to different scenarios,\nunlike previous studies that provided in-context examples as few-shot learning.\nIn addition, we propose a workflow of deploying the real-world robot system\nwith self-learned skills as an embodied assistant.", "field": "Computer Science", "categories": "cs.RO,cs.AI"}, {"arxiv_id": "2401.13463", "title": "SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken\n  Question Answering", "abstract": "Spoken Question Answering (SQA) is essential for machines to reply to user's\nquestion by finding the answer span within a given spoken passage. SQA has been\npreviously achieved without ASR to avoid recognition errors and\nOut-of-Vocabulary (OOV) problems. However, the real-world problem of\nOpen-domain SQA (openSQA), in which the machine needs to first retrieve\npassages that possibly contain the answer from a spoken archive in addition,\nwas never considered. This paper proposes the first known end-to-end framework,\nSpeech Dense Passage Retriever (SpeechDPR), for the retrieval component of the\nopenSQA problem. SpeechDPR learns a sentence-level semantic representation by\ndistilling knowledge from the cascading model of unsupervised ASR (UASR) and\ntext dense retriever (TDR). No manually transcribed speech data is needed.\nInitial experiments showed performance comparable to the cascading model of\nUASR and TDR, and significantly better when UASR was poor, verifying this\napproach is more robust to speech recognition errors.", "field": "Computer Science", "categories": "cs.CL,cs.IR,cs.SD,eess.AS"}, {"arxiv_id": "2401.13464", "title": "Analysis and implementation of the Buck-Boost Modified Series Forward\n  converter applied to photovoltaic systems", "abstract": "The mismatching phenomenon is one of the main issues in photovoltaic (PV)\napplications. It could reduce the generated power of a string when a PV panel\nhas different performances from the other PV panels connected to the same\nstring. Distributed Maximum Power Point Tracking (DMPPT) architectures are one\nof the most promising solutions to overcome the drawbacks associated with\nmismatching phenomena in PV applications. In this kind of architectures, a\nDC-DC module integrated converter (MIC) manages each PV panel, isolating it\nfrom the rest of the PV panels, for harvesting the maximum available power from\nthe Sun. Due to the high number of DCDC converters used in a grid-tied PV\ninstallation, the most desired MIC requirements are high efficiency, low cost\nand the capability of voltage step-up and step-down. This paper proposes the\nBuck-Boost Modified Forward (BBMSF) converter as a good candidate to be applied\nin DMPPT architectures. A complete analysis of the BBMSF converter is carried\nout, including the steady-state analysis as well as the small signal analysis\nin continuous conduction mode. The main advantages of the BBMSF converter are\nits step-up and step-down voltage transfer function; a higher simplicity, since\nit only includes a single controlled switch; the soft switching characteristics\nin all the diodes and MOSFET, reaching in some cases ZVS and ZCS, and yielding\nhigh efficiencies; the use of an autotransformer, with better performances than\na typical Forward transformer; and the good dynamic performance, like the\nForward converter ones. The theoretical analyses are validated through the\nexperimental results in a 225 W BBMSF prototype designed and built under the\nrequirements of a 100 kW grid-tied PV installation, achieving an efficiency up\nto 93.6%.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13478", "title": "SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval", "abstract": "Multi-modal information retrieval (MMIR) is a rapidly evolving field, where\nsignificant progress, particularly in image-text pairing, has been made through\nadvanced representation learning and cross-modality alignment research.\nHowever, current benchmarks for evaluating MMIR performance in image-text\npairing within the scientific domain show a notable gap, where chart and table\nimages described in scholarly language usually do not play a significant role.\nTo bridge this gap, we develop a specialised scientific MMIR (SciMMIR)\nbenchmark by leveraging open-access paper collections to extract data relevant\nto the scientific domain. This benchmark comprises 530K meticulously curated\nimage-text pairs, extracted from figures and tables with detailed captions in\nscientific documents. We further annotate the image-text pairs with two-level\nsubset-subcategory hierarchy annotations to facilitate a more comprehensive\nevaluation of the baselines. We conducted zero-shot and fine-tuning evaluations\non prominent multi-modal image-captioning and visual language models, such as\nCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific\ndomain, including the impact of pre-training and fine-tuning settings and the\ninfluence of the visual and textual encoders. All our data and checkpoints are\npublicly available at https://github.com/Wusiwei0410/SciMMIR.", "field": "Computer Science", "categories": "cs.IR,cs.CL,cs.CV,cs.MM"}, {"arxiv_id": "2401.1348", "title": "The Dynamics of (Not) Unfollowing Misinformation Spreaders", "abstract": "Many studies explore how people 'come into' misinformation exposure. But much\nless is known about how people 'come out of' misinformation exposure. Do people\norganically sever ties to misinformation spreaders? And what predicts doing so?\nOver six months, we tracked the frequency and predictors of ~1M followers\nunfollowing ~5K health misinformation spreaders on Twitter. We found that\nmisinformation ties are persistent. Monthly unfollowing rates are just 0.52%.\nUsers are also 31% more likely to unfollow non-misinformation spreaders than\nthey are to unfollow misinformation spreaders. Although generally infrequent,\nthe factors most associated with unfollowing misinformation spreaders are (1)\nredundancy and (2) ideology. First, users initially following many spreaders,\nor who follow spreaders that tweet often, are most likely to unfollow later.\nSecond, liberals are more likely to unfollow than conservatives. Overall, we\nobserve strong persistence of misinformation ties. The fact that users rarely\nunfollow misinformation spreaders suggests a need for external nudges and the\nimportance of preventing exposure from arising in the first place.", "field": "Computer Science", "categories": "cs.SI,cs.CY,cs.HC"}, {"arxiv_id": "2401.13481", "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment", "abstract": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- mimics the interdependent process of cultural creation: creative ideas are\nbuilt upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI, and that participants were more likely to knowingly adopt AI\nideas when the task was difficult. Our findings suggest that introducing AI\nideas into society may increase collective diversity but not individual\ncreativity.", "field": "Computer Science", "categories": "cs.CY,cs.AI,cs.CL,cs.HC"}, {"arxiv_id": "2401.13483", "title": "Radial perfectly matched layers and infinite elements for the\n  anisotropic wave equation", "abstract": "We consider the scalar anisotropic wave equation. Recently a convergence\nanalysis for radial perfectly matched layers (PML) in the frequency domain was\nreported and in the present article we continue this approach into the time\ndomain. First we explain why there is a good hope that radial complex scalings\ncan overcome the instabilities of PML methods caused by anisotropic materials.\nNext we discuss some sensitive details, which seem like a paradox at the first\nglance: if the absorbing layer and the inhomogeneities are sufficiently\nseparated, then the solution is indeed stable. However, for more general data\nthe problem becomes unstable. In numerical computations we observe\ninstabilities regardless of the position of the inhomogeneities, although the\ninstabilities arise only for fine enough discretizations. As a remedy we\npropose a complex frequency shifted scaling and discretizations by Hardy space\ninfinite elements or truncation-free PMLs. We show numerical experiments which\nconfirm the stability and convergence of these methods.", "field": "Computer Science", "categories": "math.NA,cs.NA,35L05, 78M10, 35L04, 65M60, 65M80"}, {"arxiv_id": "2401.13486", "title": "Separable Physics-Informed Neural Networks for the solution of\n  elasticity problems", "abstract": "A method for solving elasticity problems based on separable physics-informed\nneural networks (SPINN) in conjunction with the deep energy method (DEM) is\npresented. Numerical experiments have been carried out for a number of problems\nshowing that this method has a significantly higher convergence rate and\naccuracy than the vanilla physics-informed neural networks (PINN) and even\nSPINN based on a system of partial differential equations (PDEs). In addition,\nusing the SPINN in the framework of DEM approach it is possible to solve\nproblems of the linear theory of elasticity on complex geometries, which is\nunachievable with the help of PINNs in frames of partial differential\nequations. Considered problems are very close to the industrial problems in\nterms of geometry, loading, and material parameters.", "field": "Computer Science", "categories": "math.NA,cs.AI,cs.LG,cs.NA,physics.app-ph,68T07, 65Z05, 65M99,I.2.1; I.2.7; J.2"}, {"arxiv_id": "2401.13488", "title": "Fast Inverse Model Transformation: Algebraic Framework for Fast Data\n  Plane Verification", "abstract": "Data plane verification (DPV) analyzes routing tables and detects routing\nabnormalities and policy violations during network operation and planning.\nThus, it has become an important tool to harden the networking infrastructure\nand the computing systems building on top. Substantial advancements have been\nmade in the last decade and state-of-the-art DPV systems can achieve sub-us\nverification for an update of a single forwarding rule.\n  In this paper, we introduce fast inverse model transformation (FIMT), the\nfirst theoretical framework to systematically model and analyze centralized DPV\nsystems. FIMT reveals the algebraic structure in the model update process, a\nkey step in fast DPV systems. Thus, it can systematically analyze the\ncorrectness of several DPV systems, using algebraic properties. The theory also\nguides the design and implementation of NeoFlash, a refactored version of Flash\nwith new optimization techniques. Evaluations show that NeoFlash outperforms\nexisting state-of-the-art centralized DPV systems in various datasets and\nreveal insights to key techniques towards fast DPV.", "field": "Computer Science", "categories": "cs.NI"}, {"arxiv_id": "2401.1349", "title": "Visualization of rank-citation curves for fast detection of h-index\n  anomalies in university metrics", "abstract": "University rankings, despite facing criticism, continue to maintain their\npopularity. In the 2023 Scopus Ranking of Ukrainian Universities, certain\ninstitutions stood out due to their high h-index, despite modest publication\nand citation numbers. This phenomenon can be attributed to influential research\ntopics or involvement in international collaborative research. However, these\nresults may also be due to the authors' own efforts to increase the number of\ncitations of their publications in order to improve their h-index. To\ninvestigate this, the publications from the top 30 universities in the ranking\nwere analysed, revealing humpback rank-citation curves for two universities.\nThese humpbacks indicate unusual trends in the citation data, especially\nconsidering the high percentage of self-citations and FWCI of analysed papers.\nWhile quantitative analysis has limitations, the combination of humped\nrank-citation curves, self-citations, FWCI, and previous research findings\nraises concerns about the possible causes of these anomalies in the citation\ndata of the analysed universities. The method presented in this paper can aid\nranking compilers and citation databases managers in identifying potential\ninstances of citation data anomalies, emphasizing the importance of expert\nassessment for accurate conclusions.", "field": "Computer Science", "categories": "cs.DL"}, {"arxiv_id": "2401.13493", "title": "Towards an Autonomous Compost Turner: Current State of Research", "abstract": "This preprint presents the current status of research into the development\nand application of an autonomous, self-driving compost turner. The aim is to\novercome challenges in the composting industry, such as adverse working\nconditions, by automating the composting process. The preprint provides a\ncomprehensive overview of the overall concept of the self-driving compost\nturner, including the hardware architecture with sensors, navigation module and\ncontrol module. In addition, the methodical development of the architecture of\nconcepts, models and their subsequent software integration in ROS using\nmodel-based systems engineering is described. The validation and verification\nof the overall system is carried out in an industrial environment using three\nscenarios. The capabilities of the compost turner are demonstrated by\nautonomously following predefined trajectories in the composting plant and\nperforming the required composting tasks. The results show that the autonomous\ncompost turner is capable of performing the required activities. In addition,\nthe compost turner has intelligent processing capabilities for compost data as\nwell as its transmission, visualization and storage in a cloud server. It is\nimportant to note that this work is a preprint that represents the current\nstate of research. The authors aim to publish the full paper in a peer-reviewed\njournal in the near future.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY"}, {"arxiv_id": "2401.13494", "title": "NSNO: Neumann Series Neural Operator for Solving Helmholtz Equations in\n  Inhomogeneous Medium", "abstract": "In this paper, we propose Neumann Series Neural Operator (NSNO) to learn the\nsolution operator of Helmholtz equation from inhomogeneity coefficients and\nsource terms to solutions. Helmholtz equation is a crucial partial differential\nequation (PDE) with applications in various scientific and engineering fields.\nHowever, efficient solver of Helmholtz equation is still a big challenge\nespecially in the case of high wavenumber. Recently, deep learning has shown\ngreat potential in solving PDEs especially in learning solution operators.\nInspired by Neumann series in Helmholtz equation, we design a novel network\narchitecture in which U-Net is embedded inside to capture the multiscale\nfeature. Extensive experiments show that the proposed NSNO significantly\noutperforms the state-of-the-art FNO with at least 60\\% lower relative\n$L^2$-error, especially in the large wavenumber case, and has 50\\% lower\ncomputational cost and less data requirement. Moreover, NSNO can be used as the\nsurrogate model in inverse scattering problems. Numerical tests show that NSNO\nis able to give comparable results with traditional finite difference forward\nsolver while the computational cost is reduced tremendously.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.13496", "title": "Transient Forward Harmonic Adjoint Sensitivity Analysis", "abstract": "This paper presents a transient forward harmonic adjoint sensitivity analysis\n(TFHA), which is a combination of a transient forward circuit analysis with a\nharmonic balance based adjoint sensitivity analysis. TFHA provides\nsensitivities of quantities of interest from time-periodic problems w.r.t. many\ndesign parameters, as used in the design process of power-electronics devices.\nThe TFHA shows advantages in applications where the harmonic balance based\nadjoint sensitivity analysis or finite difference approaches for sensitivity\nanalysis perform poorly. In contrast to existing methods, the TFHA can be used\nin combination with arbitrary forward solvers, i.e. general transient solvers.", "field": "Computer Science", "categories": "math.NA,cs.NA"}, {"arxiv_id": "2401.13498", "title": "Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific\n  Input Representation and Diffusion Outpainting", "abstract": "Synthesizing performing guitar sound is a highly challenging task due to the\npolyphony and high variability in expression. Recently, deep generative models\nhave shown promising results in synthesizing expressive polyphonic instrument\nsounds from music scores, often using a generic MIDI input. In this work, we\npropose an expressive acoustic guitar sound synthesis model with a customized\ninput representation to the instrument, which we call guitarroll. We implement\nthe proposed approach using diffusion-based outpainting which can generate\naudio with long-term consistency. To overcome the lack of MIDI/audio-paired\ndatasets, we used not only an existing guitar dataset but also collected data\nfrom a high quality sample-based guitar synthesizer. Through quantitative and\nqualitative evaluations, we show that our proposed model has higher audio\nquality than the baseline model and generates more realistic timbre sounds than\nthe previous leading work.", "field": "Computer Science", "categories": "cs.SD,cs.AI,cs.LG,eess.AS,eess.SP"}, {"arxiv_id": "2401.13499", "title": "LDCA: Local Descriptors with Contextual Augmentation for Few-Shot\n  Learning", "abstract": "Few-shot image classification has emerged as a key challenge in the field of\ncomputer vision, highlighting the capability to rapidly adapt to new tasks with\nminimal labeled data. Existing methods predominantly rely on image-level\nfeatures or local descriptors, often overlooking the holistic context\nsurrounding these descriptors. In this work, we introduce a novel approach\ntermed \"Local Descriptor with Contextual Augmentation (LDCA)\". Specifically,\nthis method bridges the gap between local and global understanding uniquely by\nleveraging an adaptive global contextual enhancement module. This module\nincorporates a visual transformer, endowing local descriptors with contextual\nawareness capabilities, ranging from broad global perspectives to intricate\nsurrounding nuances. By doing so, LDCA transcends traditional descriptor-based\napproaches, ensuring each local feature is interpreted within its larger visual\nnarrative. Extensive experiments underscore the efficacy of our method, showing\na maximal absolute improvement of 20\\% over the next-best on fine-grained\nclassification datasets, thus demonstrating significant advancements in\nfew-shot classification tasks.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13502", "title": "Faster Combinatorial k-Clique Algorithms", "abstract": "Detecting if a graph contains a $k$-Clique is one of the most fundamental\nproblems in computer science. The asymptotically fastest algorithm runs in time\n$O(n^{\\omega k/3})$, where $\\omega$ is the exponent of Boolean matrix\nmultiplication. To date, this is the only technique capable of beating the\ntrivial $O(n^k)$ bound by a polynomial factor. Due to this technique's various\nlimitations, much effort has gone into designing \"combinatorial\" algorithms\nthat improve over exhaustive search via other techniques.\n  The first contribution of this work is a faster combinatorial algorithm for\n$k$-Clique, improving Vassilevska's bound of $O(n^{k}/\\log^{k-1}{n})$ by two\nlog factors. Technically, our main result is a new reduction from $k$-Clique to\nTriangle detection that exploits the same divide-and-conquer at the core of\nrecent combinatorial algorithms by Chan (SODA'15) and Yu (ICALP'15).\n  Our second contribution is exploiting combinatorial techniques to improve the\nstate-of-the-art (even of non-combinatorial algorithms) for generalizations of\nthe $k$-Clique problem. In particular, we give the first $o(n^k)$ algorithm for\n$k$-clique in hypergraphs and an $O(n^3/\\log^{2.25}{n} + t)$ algorithm for\nlisting $t$ triangles in a graph.", "field": "Computer Science", "categories": "cs.DS"}, {"arxiv_id": "2401.13503", "title": "Learning Representations for Clustering via Partial Information\n  Discrimination and Cross-Level Interaction", "abstract": "In this paper, we present a novel deep image clustering approach termed PICI,\nwhich enforces the partial information discrimination and the cross-level\ninteraction in a joint learning framework. In particular, we leverage a\nTransformer encoder as the backbone, through which the masked image modeling\nwith two paralleled augmented views is formulated. After deriving the class\ntokens from the masked images by the Transformer encoder, three partial\ninformation learning modules are further incorporated, including the PISD\nmodule for training the auto-encoder via masked image reconstruction, the PICD\nmodule for employing two levels of contrastive learning, and the CLI module for\nmutual interaction between the instance-level and cluster-level subspaces.\nExtensive experiments have been conducted on six real-world image datasets,\nwhich demononstrate the superior clustering performance of the proposed PICI\napproach over the state-of-the-art deep clustering approaches. The source code\nis available at https://github.com/Regan-Zhang/PICI.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13504", "title": "Research about the Ability of LLM in the Tamper-Detection Area", "abstract": "In recent years, particularly since the early 2020s, Large Language Models\n(LLMs) have emerged as the most powerful AI tools in addressing a diverse range\nof challenges, from natural language processing to complex problem-solving in\nvarious domains. In the field of tamper detection, LLMs are capable of\nidentifying basic tampering activities.To assess the capabilities of LLMs in\nmore specialized domains, we have collected five different LLMs developed by\nvarious companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. This\ndiverse range of models allows for a comprehensive evaluation of their\nperformance in detecting sophisticated tampering instances.We devised two\ndomains of detection: AI-Generated Content (AIGC) detection and manipulation\ndetection. AIGC detection aims to test the ability to distinguish whether an\nimage is real or AI-generated. Manipulation detection, on the other hand,\nfocuses on identifying tampered images. According to our experiments, most LLMs\ncan identify composite pictures that are inconsistent with logic, and only more\npowerful LLMs can distinguish logical, but visible signs of tampering to the\nhuman eye. All of the LLMs can't identify carefully forged images and very\nrealistic images generated by AI. In the area of tamper detection, LLMs still\nhave a long way to go, particularly in reliably identifying highly\nsophisticated forgeries and AI-generated images that closely mimic reality.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13505", "title": "Generative Human Motion Stylization in Latent Space", "abstract": "Human motion stylization aims to revise the style of an input motion while\nkeeping its content unaltered. Unlike existing works that operate directly in\npose space, we leverage the latent space of pretrained autoencoders as a more\nexpressive and robust representation for motion extraction and infusion.\nBuilding upon this, we present a novel generative model that produces diverse\nstylization results of a single motion (latent) code. During training, a motion\ncode is decomposed into two coding components: a deterministic content code,\nand a probabilistic style code adhering to a prior distribution; then a\ngenerator massages the random combination of content and style codes to\nreconstruct the corresponding motion codes. Our approach is versatile, allowing\nthe learning of probabilistic style space from either style labeled or\nunlabeled motions, providing notable flexibility in stylization as well. In\ninference, users can opt to stylize a motion using style cues from a reference\nmotion or a label. Even in the absence of explicit style input, our model\nfacilitates novel re-stylization by sampling from the unconditional style prior\ndistribution. Experimental results show that our proposed stylization models,\ndespite their lightweight design, outperform the state-of-the-arts in style\nreeanactment, content preservation, and generalization across various\napplications and settings. Project Page: https://yxmu.foo/GenMoStyle", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13509", "title": "TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient\n  and Effective Retrieval", "abstract": "This paper considers Pseudo-Relevance Feedback (PRF) methods for dense\nretrievers in a resource constrained environment such as that of cheap cloud\ninstances or embedded systems (e.g., smartphones and smartwatches), where\nmemory and CPU are limited and GPUs are not present. For this, we propose a\ntransformer-based PRF method (TPRF), which has a much smaller memory footprint\nand faster inference time compared to other deep language models that employ\nPRF mechanisms, with a marginal effectiveness loss. TPRF learns how to\neffectively combine the relevance feedback signals from dense passage\nrepresentations. Specifically, TPRF provides a mechanism for modelling\nrelationships and weights between the query and the relevance feedback signals.\nThe method is agnostic to the specific dense representation used and thus can\nbe generally applied to any dense retriever.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13512", "title": "Can GPT-3.5 Generate and Code Discharge Summaries?", "abstract": "Objective: To investigate GPT-3.5 in generating and coding medical documents\nwith ICD-10 codes for data augmentation on low-resources labels.\n  Materials and Methods: Employing GPT-3.5 we generated and coded 9,606\ndischarge summaries based on lists of ICD-10 code descriptions of patients with\ninfrequent (generation) codes within the MIMIC-IV dataset. Combined with the\nbaseline training set, this formed an augmented training set. Neural coding\nmodels were trained on baseline and augmented data and evaluated on a MIMIC-IV\ntest set. We report micro- and macro-F1 scores on the full codeset, generation\ncodes, and their families. Weak Hierarchical Confusion Matrices were employed\nto determine within-family and outside-of-family coding errors in the latter\ncodesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided\nself-generated data and real MIMIC-IV data. Clinical professionals evaluated\nthe clinical acceptability of the generated documents.\n  Results: Augmentation slightly hinders the overall performance of the models\nbut improves performance for the generation candidate codes and their families,\nincluding one unseen in the baseline training data. Augmented models display\nlower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the\nprompted descriptions, but performs poorly on real data. Evaluators note the\ncorrectness of generated concepts while suffering in variety, supporting\ninformation, and narrative.\n  Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.\nAugmentation positively affects generation code families but mainly benefits\ncodes with existing examples. Augmentation reduces out-of-family errors.\nDischarge summaries generated by GPT-3.5 state prompted concepts correctly but\nlack variety, and authenticity in narratives. They are unsuitable for clinical\npractice.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13516", "title": "Delocate: Detection and Localization for Deepfake Videos with\n  Randomly-Located Tampered Traces", "abstract": "Deepfake videos are becoming increasingly realistic, showing subtle tampering\ntraces on facial areasthat vary between frames. Consequently, many existing\nDeepfake detection methods struggle to detect unknown domain Deepfake videos\nwhile accurately locating the tampered region. To address thislimitation, we\npropose Delocate, a novel Deepfake detection model that can both recognize\nandlocalize unknown domain Deepfake videos. Ourmethod consists of two stages\nnamed recoveringand localization. In the recovering stage, the modelrandomly\nmasks regions of interest (ROIs) and reconstructs real faces without tampering\ntraces, resulting in a relatively good recovery effect for realfaces and a poor\nrecovery effect for fake faces. Inthe localization stage, the output of the\nrecoveryphase and the forgery ground truth mask serve assupervision to guide\nthe forgery localization process. This process strategically emphasizes the\nrecovery phase of fake faces with poor recovery, facilitating the localization\nof tampered regions. Ourextensive experiments on four widely used benchmark\ndatasets demonstrate that Delocate not onlyexcels in localizing tampered areas\nbut also enhances cross-domain detection performance.", "field": "Computer Science", "categories": "cs.CV,cs.CR"}, {"arxiv_id": "2401.13518", "title": "Addressing Data Quality Challenges in Observational Ambulatory Studies:\n  Analysis, Methodologies and Practical Solutions for Wrist-worn Wearable\n  Monitoring", "abstract": "Chronic disease management and follow-up are vital for realizing sustained\npatient well-being and optimal health outcomes. Recent advancements in wearable\nsensing technologies, particularly wrist-worn devices, offer promising\nsolutions for longitudinal patient follow-up by shifting from subjective,\nintermittent self-reporting to objective, continuous monitoring. However,\ncollecting and analyzing wearable data presents unique challenges, such as data\nentry errors, non-wear periods, missing wearable data, and wearable artifacts.\nWe therefore present an in-depth exploration of data analysis challenges tied\nto wrist-worn wearables and ambulatory label acquisition, using two real-world\ndatasets (i.e., mBrain21 and ETRI lifelog2020). We introduce novel practical\ncountermeasures, including participant compliance visualizations,\ninteraction-triggered questionnaires to assess personal bias, and an optimized\nwearable non-wear detection pipeline. Further, we propose a visual analytics\napproach to validate processing pipelines using scalable tools such as tsflex\nand Plotly-Resampler. Lastly, we investigate the impact of missing wearable\ndata on \"window-of-interest\" analysis methodologies. Prioritizing transparency\nand reproducibility, we offer open access to our detailed code examples,\nfacilitating adaptation in future wearable research. In conclusion, our\ncontributions provide actionable approaches for wearable data collection and\nanalysis in chronic disease management.", "field": "Computer Science", "categories": "cs.CE"}, {"arxiv_id": "2401.13527", "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation", "abstract": "Benefiting from effective speech modeling, current Speech Large Language\nModels (SLLMs) have demonstrated exceptional capabilities in in-context speech\ngeneration and efficient generalization to unseen speakers. However, the\nprevailing information modeling process is encumbered by certain redundancies,\nleading to inefficiencies in speech generation. We propose Chain-of-Information\nGeneration (CoIG), a method for decoupling semantic and perceptual information\nin large-scale speech generation. Building on this, we develop SpeechGPT-Gen,\nan 8-billion-parameter SLLM efficient in semantic and perceptual information\nmodeling. It comprises an autoregressive model based on LLM for semantic\ninformation modeling and a non-autoregressive model employing flow matching for\nperceptual information modeling. Additionally, we introduce the novel approach\nof infusing semantic information into the prior distribution to enhance the\nefficiency of flow matching. Extensive experimental results demonstrate that\nSpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice\nconversion, and speech-to-speech dialogue, underscoring CoIG's remarkable\nproficiency in capturing and modeling speech's semantic and perceptual\ndimensions. Code and models are available at\nhttps://github.com/0nutation/SpeechGPT.", "field": "Computer Science", "categories": "cs.CL,cs.SD,eess.AS"}, {"arxiv_id": "2401.1353", "title": "Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein\n  Probabilistic Space", "abstract": "Recently, optimization on the Riemannian manifold has provided new insights\nto the optimization community. In this regard, the manifold taken as the\nprobability measure metric space equipped with the second-order Wasserstein\ndistance is of particular interest, since optimization on it can be linked to\npractical sampling processes. In general, the oracle (continuous) optimization\nmethod on Wasserstein space is Riemannian gradient flow (i.e., Langevin\ndynamics when minimizing KL divergence). In this paper, we aim to enrich the\ncontinuous optimization methods in the Wasserstein space by extending the\ngradient flow into the stochastic gradient descent (SGD) flow and stochastic\nvariance reduction gradient (SVRG) flow. The two flows on Euclidean space are\nstandard stochastic optimization methods, while their Riemannian counterparts\nare not explored yet. By leveraging the structures in Wasserstein space, we\nconstruct a stochastic differential equation (SDE) to approximate the discrete\ndynamics of desired stochastic methods in the corresponded random vector space.\nThen, the flows of probability measures are naturally obtained by applying\nFokker-Planck equation to such SDE. Furthermore, the convergence rates of the\nproposed Riemannian stochastic flows are proven, and they match the results in\nEuclidean space.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.13531", "title": "QAGait: Revisit Gait Recognition from a Quality Perspective", "abstract": "Gait recognition is a promising biometric method that aims to identify\npedestrians from their unique walking patterns. Silhouette modality, renowned\nfor its easy acquisition, simple structure, sparse representation, and\nconvenient modeling, has been widely employed in controlled in-the-lab\nresearch. However, as gait recognition rapidly advances from in-the-lab to\nin-the-wild scenarios, various conditions raise significant challenges for\nsilhouette modality, including 1) unidentifiable low-quality silhouettes\n(abnormal segmentation, severe occlusion, or even non-human shape), and 2)\nidentifiable but challenging silhouettes (background noise, non-standard\nposture, slight occlusion). To address these challenges, we revisit gait\nrecognition pipeline and approach gait recognition from a quality perspective,\nnamely QAGait. Specifically, we propose a series of cost-effective quality\nassessment strategies, including Maxmial Connect Area and Template Match to\neliminate background noises and unidentifiable silhouettes, Alignment strategy\nto handle non-standard postures. We also propose two quality-aware loss\nfunctions to integrate silhouette quality into optimization within the\nembedding space. Extensive experiments demonstrate our QAGait can guarantee\nboth gait reliability and performance enhancement. Furthermore, our quality\nassessment strategies can seamlessly integrate with existing gait datasets,\nshowcasing our superiority. Code is available at\nhttps://github.com/wzb-bupt/QAGait.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13535", "title": "On the Approximate Core and Nucleon of Flow Games", "abstract": "The flow game with public arcs is a cooperative revenue game derived from a\nflow network. In this game, each player possesses an arc, while certain arcs,\nknown as public arcs, are not owned by any specific player and are accessible\nto any coalition. The aim of this game is to maximize the flow that can be\nrouted in the network through strategic coalition formation. By exploring its\nconnection to the maximum partially disjoint path problem, we investigate the\napproximate core and nucleon of the flow game with public arcs. The approximate\ncore is an extension of the core that allows for some deviation in group\nrationality, while the nucleon is a multiplicative analogue of the nucleolus.\nIn this paper, we provide two complete characterizations for the optimal\napproximate core and show that the nucleon can be computed in polynomial time.", "field": "Computer Science", "categories": "cs.GT,05C57, 91A12, 91A43, 91A46"}, {"arxiv_id": "2401.13539", "title": "Dynamic Risk Management in Cyber Physical Systems", "abstract": "Cyber Physical Systems (CPS) enable new kinds of applications as well as\nsignificant improvements of existing ones in numerous different application\ndomains. A major trait of upcoming CPS is an increasing degree of automation up\nto the point of autonomy, as there is a huge potential for economic success as\nwell as for ecologic and societal improvements. However, to unlock the full\npotential of such (cooperative and automated) CPS, we first need to overcome\nseveral significant engineering challenges, where safety assurance is a\nparticularly important one. Unfortunately, established safety assurance methods\nand standards do not live up to this task, as they have been designed with\nclosed and less complex systems in mind. This paper structures safety assurance\nchallenges of cooperative automated CPS, provides an overview on our vision of\ndynamic risk management and describes already existing building blocks.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.1354", "title": "State Estimation for Continuum Multi-Robot Systems on SE(3)", "abstract": "In contrast to conventional robots, accurately modeling the kinematics and\nstatics of continuum robots is challenging due to partially unknown material\nproperties, parasitic effects, or unknown forces acting on the continuous body.\nConsequentially, state estimation approaches that utilize additional sensor\ninformation to predict the shape of continuum robots have garnered significant\ninterest. This paper presents a novel approach to state estimation for systems\nwith multiple coupled continuum robots, which allows estimating the shape and\nstrain variables of multiple continuum robots in an arbitrary coupled topology.\nSimulations and experiments demonstrate the capabilities and versatility of the\nproposed method, while achieving accurate and continuous estimates for the\nstate of such systems, resulting in average end-effector errors of 3.3 mm and\n5.02{\\deg} depending on the sensor setup. It is further shown, that the\napproach offers fast computation times of below 10 ms, enabling its utilization\nin quasi-static real-time scenarios with average update rates of 100-200 Hz. An\nopen-source C++ implementation of the proposed state estimation method is made\npublicly available to the community.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13544", "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?", "abstract": "Recently, interpretable machine learning has re-explored concept bottleneck\nmodels (CBM), comprising step-by-step prediction of the high-level concepts\nfrom the raw features and the target variable from the predicted concepts. A\ncompelling advantage of this model class is the user's ability to intervene on\nthe predicted concept values, affecting the model's downstream output. In this\nwork, we introduce a method to perform such concept-based interventions on\nalready-trained neural networks, which are not interpretable by design, given\nan annotated validation set. Furthermore, we formalise the model's\nintervenability as a measure of the effectiveness of concept-based\ninterventions and leverage this definition to fine-tune black-box models.\nEmpirically, we explore the intervenability of black-box classifiers on\nsynthetic tabular and natural image benchmarks. We demonstrate that fine-tuning\nimproves intervention effectiveness and often yields better-calibrated\npredictions. To showcase the practical utility of the proposed techniques, we\napply them to deep chest X-ray classifiers and show that fine-tuned black boxes\ncan be as intervenable and more performant than CBMs.", "field": "Computer Science", "categories": "cs.LG,stat.ML"}, {"arxiv_id": "2401.13545", "title": "Fine-grained Contract NER using instruction based model", "abstract": "Lately, instruction-based techniques have made significant strides in\nimproving performance in few-shot learning scenarios. They achieve this by\nbridging the gap between pre-trained language models and fine-tuning for\nspecific downstream tasks. Despite these advancements, the performance of Large\nLanguage Models (LLMs) in information extraction tasks like Named Entity\nRecognition (NER), using prompts or instructions, still falls short of\nsupervised baselines. The reason for this performance gap can be attributed to\nthe fundamental disparity between NER and LLMs. NER is inherently a sequence\nlabeling task, where the model must assign entity-type labels to individual\ntokens within a sentence. In contrast, LLMs are designed as a text generation\ntask. This distinction between semantic labeling and text generation leads to\nsubpar performance. In this paper, we transform the NER task into a\ntext-generation task that can be readily adapted by LLMs. This involves\nenhancing source sentences with task-specific instructions and answer choices,\nallowing for the identification of entities and their types within natural\nlanguage. We harness the strength of LLMs by integrating supervised learning\nwithin them. The goal of this combined strategy is to boost the performance of\nLLMs in extraction tasks like NER while simultaneously addressing hallucination\nissues often observed in LLM-generated content. A novel corpus Contract NER\ncomprising seven frequently observed contract categories, encompassing named\nentities associated with 18 distinct legal entity types is released along with\nour baseline models. Our models and dataset are available to the community for\nfuture research * .", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13546", "title": "Analysis, design, and implementation of the AFZ converter applied to\n  photovoltaic systems", "abstract": "Grid-tied photovoltaic (PV) installations with Distributed Maximum Power\nPoint Tracking (DMPPT) architectures include a DC-DC Module Integrated\nConverter (MIC) for managing each PV panel, isolating it from the others,\nreducing the mismatching effect and maximizing the harvested power. In this\npaper, the Autotransformer Forward converter with type-Zeta resonant reset\n(AFZ) is proposed as a DMPPT architecture MIC candidate. The main\ncharacteristics of the AFZ converter are the high versatility due to its\nvoltage step-up and step-down capability; the use of an optimized\nautotransformer with only two windings, reducing the complexity and power\nlosses of this component; the good dynamic performances, like the Forward\nconverter ones; the low number of components and the simplicity and high\nfeasibility associated to the use of just one active switch. Besides, soft\nswitching transitions are achieved thanks to the autotransformer type-Zeta\nresonant reset. The steady-state theoretical analysis, considering the effect\nof the autotransformer leakage inductance, is presented. The converter is also\nstudied in the frequency domain, obtaining the small-signal transfer functions.\nA design procedure based on the requirements of a 100 kW grid-tied photovoltaic\ninstallation is described, yielding in a 225 W prototype with efficiencies up\nto 95.6 %. Experimental results validate the theoretical analysis.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13548", "title": "A Phoneme-Scale Assessment of Multichannel Speech Enhancement Algorithms", "abstract": "In the intricate acoustic landscapes where speech intelligibility is\nchallenged by noise and reverberation, multichannel speech enhancement emerges\nas a promising solution for individuals with hearing loss. Such algorithms are\ncommonly evaluated at the utterance level. However, this approach overlooks the\ngranular acoustic nuances revealed by phoneme-specific analysis, potentially\nobscuring key insights into their performance. This paper presents an in-depth\nphoneme-scale evaluation of 3 state-of-the-art multichannel speech enhancement\nalgorithms. These algorithms -- FasNet, MVDR, and Tango -- are extensively\nevaluated across different noise conditions and spatial setups, employing\nrealistic acoustic simulations with measured room impulse responses, and\nleveraging diversity offered by multiple microphones in a binaural hearing\nsetup. The study emphasizes the fine-grained phoneme-level analysis, revealing\nthat while some phonemes like plosives are heavily impacted by environmental\nacoustics and challenging to deal with by the algorithms, others like nasals\nand sibilants see substantial improvements after enhancement. These\ninvestigations demonstrate important improvements in phoneme clarity in noisy\nconditions, with insights that could drive the development of more personalized\nand phoneme-aware hearing aid technologies.", "field": "Computer Science", "categories": "cs.SD,eess.AS"}, {"arxiv_id": "2401.13551", "title": "Interleaving One-Class and Weakly-Supervised Models with Adaptive\n  Thresholding for Unsupervised Video Anomaly Detection", "abstract": "Without human annotations, a typical Unsupervised Video Anomaly Detection\n(UVAD) method needs to train two models that generate pseudo labels for each\nother. In previous work, the two models are closely entangled with each other,\nand it is not known how to upgrade their method without modifying their\ntraining framework significantly. Second, previous work usually adopts fixed\nthresholding to obtain pseudo labels, however the user-specified threshold is\nnot reliable which inevitably introduces errors into the training process. To\nalleviate these two problems, we propose a novel interleaved framework that\nalternately trains a One-Class Classification (OCC) model and a\nWeakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can\nbe easily replaced with other OCC or WS models, which facilitates our method to\nupgrade with the most recent developments in both fields. For handling the\nfixed thresholding problem, we break through the conventional cognitive\nboundary and propose a weighted OCC model that can be trained on both normal\nand abnormal data. We also propose an adaptive mechanism for automatically\nfinding the optimal threshold for the WS model in a loose to strict manner.\nExperiments demonstrate that the proposed UVAD method outperforms previous\napproaches.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13552", "title": "On the Constrained CAV Platoon Control Problem", "abstract": "The main objective of the connected and automated vehicle (CAV) platoon\ncontrol problem is to regulate CAVs' position while ensuring stability and\naccounting for vehicle dynamics. Although this problem has been studied in the\nliterature, existing research has some limitations. This paper presents two new\ntheoretical results that address these limitations: (i) the synthesis of\nunrealistic high-gain control parameters due to the lack of a systematic way to\nincorporate the lower and upper bounds on the control parameters, and (ii) the\nperformance sensitivity to the communication delay due to inaccurate Taylor\nseries approximation. To be more precise, taking advantage of the wellknown\nPade approximation, this paper proposes a constrained CAV platoon controller\nsynthesis that (i) systematically incorporates the lower and upper bounds on\nthe control parameters, and (ii) significantly improves the performance\nsensitivity to the communication delay. The effectiveness of the presented\nresults is verified through conducting extensive numerical simulations. The\nproposed controller effectively attenuates the stop-and-go disturbance -- a\nsingle cycle of deceleration followed by acceleration -- amplification\nthroughout the mixed platoon (consisting of CAVs and human-driven vehicles).\nModern transportation systems will benefit from the proposed CAV controls in\nterms of effective disturbance attenuation as it will potentially reduce\ncollisions.", "field": "Computer Science", "categories": "eess.SY,cs.SY,math.OC"}, {"arxiv_id": "2401.13554", "title": "PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour\n  Recognition", "abstract": "We present the PanAf20K dataset, the largest and most diverse open-access\nannotated video dataset of great apes in their natural environment. It\ncomprises more than 7 million frames across ~20,000 camera trap videos of\nchimpanzees and gorillas collected at 18 field sites in tropical Africa as part\nof the Pan African Programme: The Cultured Chimpanzee. The footage is\naccompanied by a rich set of annotations and benchmarks making it suitable for\ntraining and testing a variety of challenging and ecologically important\ncomputer vision tasks including ape detection and behaviour recognition.\nFurthering AI analysis of camera trap information is critical given the\nInternational Union for Conservation of Nature now lists all species in the\ngreat ape family as either Endangered or Critically Endangered. We hope the\ndataset can form a solid basis for engagement of the AI community to improve\nperformance, efficiency, and result interpretation in order to support\nassessments of great ape presence, abundance, distribution, and behaviour and\nthereby aid conservation efforts.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13555", "title": "Benchmarking the Fairness of Image Upsampling Methods", "abstract": "Recent years have witnessed a rapid development of deep generative models for\ncreating synthetic media, such as images and videos. While the practical\napplications of these models in everyday tasks are enticing, it is crucial to\nassess the inherent risks regarding their fairness. In this work, we introduce\na comprehensive framework for benchmarking the performance and fairness of\nconditional generative models. We develop a set of\nmetrics$\\unicode{x2013}$inspired by their supervised fairness\ncounterparts$\\unicode{x2013}$to evaluate the models on their fairness and\ndiversity. Focusing on the specific application of image upsampling, we create\na benchmark covering a wide variety of modern upsampling methods. As part of\nthe benchmark, we introduce UnfairFace, a subset of FairFace that replicates\nthe racial distribution of common large-scale face datasets. Our empirical\nstudy highlights the importance of using an unbiased training set and reveals\nvariations in how the algorithms respond to dataset imbalances. Alarmingly, we\nfind that none of the considered methods produces statistically fair and\ndiverse results.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.LG"}, {"arxiv_id": "2401.13556", "title": "Extension of the Injected-Absorbed-Current Method applied to DC-DC\n  Converters with Input Filter, Output Post-filter and Feedforward\n  Compensations", "abstract": "In railway applications, it is common to use an LC filter connected between\nthe catenary and the input port of the main converter of the auxiliary and\ntraction systems. In addition, in the auxiliary systems, there is a converter\noperating as a battery charger, which requires a very low ripple in the output\ncurrent and output voltage, so a postfilter may be placed at the output port of\nthe converter. This article proposes a step-by-step methodology to extend the\ninjected-absorbed-current (IAC) method in order to obtain transfer functions\nthat consider the effects of the input filter, output postfilter, and some\nfeedforward compensations. The proposed methodology allows reusing the\ncharacteristic coefficients of the DC-DC converter model derived from the\nexisting IAC method. One of the advantages of the proposed methodology is that\nthe transfer functions obtained in this article are valid for cases where both,\none or none of the filters, are implemented. Finally, for the experimental\nvalidation of the proposed methodology, the phase-shifted full-bridge converter\nwas selected as a convenient example. Furthermore, the experimental\nmeasurements have been performed on two prototypes.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13558", "title": "Task structure and nonlinearity jointly determine learned\n  representational geometry", "abstract": "The utility of a learned neural representation depends on how well its\ngeometry supports performance in downstream tasks. This geometry depends on the\nstructure of the inputs, the structure of the target outputs, and the\narchitecture of the network. By studying the learning dynamics of networks with\none hidden layer, we discovered that the network's activation function has an\nunexpectedly strong impact on the representational geometry: Tanh networks tend\nto learn representations that reflect the structure of the target outputs,\nwhile ReLU networks retain more information about the structure of the raw\ninputs. This difference is consistently observed across a broad class of\nparameterized tasks in which we modulated the degree of alignment between the\ngeometry of the task inputs and that of the task labels. We analyzed the\nlearning dynamics in weight space and show how the differences between the\nnetworks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic\nbehavior of ReLU, which leads feature neurons to specialize for different\nregions of input space. By contrast, feature neurons in Tanh networks tend to\ninherit the task label structure. Consequently, when the target outputs are low\ndimensional, Tanh networks generate neural representations that are more\ndisentangled than those obtained with a ReLU nonlinearity. Our findings shed\nlight on the interplay between input-output geometry, nonlinearity, and learned\nrepresentations in neural networks.", "field": "Computer Science", "categories": "cs.LG"}, {"arxiv_id": "2401.1356", "title": "SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image\n  Segmentation", "abstract": "The Transformer architecture has shown a remarkable ability in modeling\nglobal relationships. However, it poses a significant computational challenge\nwhen processing high-dimensional medical images. This hinders its development\nand widespread adoption in this task. Mamba, as a State Space Model (SSM),\nrecently emerged as a notable manner for long-range dependencies in sequential\nmodeling, excelling in natural language processing filed with its remarkable\nmemory efficiency and computational speed. Inspired by its success, we\nintroduce SegMamba, a novel 3D medical image \\textbf{Seg}mentation\n\\textbf{Mamba} model, designed to effectively capture long-range dependencies\nwithin whole volume features at every scale. Our SegMamba, in contrast to\nTransformer-based methods, excels in whole volume feature modeling from a state\nspace model standpoint, maintaining superior processing speed, even with volume\nfeatures at a resolution of {$64\\times 64\\times 64$}. Comprehensive experiments\non the BraTS2023 dataset demonstrate the effectiveness and efficiency of our\nSegMamba. The code for SegMamba is available at:\nhttps://github.com/ge-xing/SegMamba", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13561", "title": "Pricing of Short Circuit Current in High IBR-Penetrated System", "abstract": "With the growing penetration of Inverter-Based Resources (IBRs) in power\nsystems, stability service markets have emerged to incentivize technologies\nthat ensure power system stability and reliability. Among the various\nchallenges faced in power system operation and stability, a prominent issue\nraised from the increasing integration of large-scale IBRs is the significant\nreduction of the Short-Circuit Current (SCC) level in the system, which poses a\nconsiderable threat to system voltage stability and protection. Thus, a proper\nmarket mechanism to incentivize the provision of SCC as a stability service is\ndesired. However, the pricing of this service within the future stability\nmarket has not yet been fully developed, due to the nonconvex nature of SCC\nconstraints and the locational property of SCC. To address these problems, this\nwork aims to explore, for the first time, a pricing model for SCC service by\nincorporating a linearized SCC constraint into the Unit Commitment (UC)\nproblem, to achieve the desired SCC level and extract the shadow price for SCC\nthrough different pricing methods.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13564", "title": "RIS Empowered Near-Field Covert Communications", "abstract": "This paper studies an extremely large-scale reconfigurable intelligent\nsurface (XL-RIS) empowered covert communication system in the near-field\nregion. Alice covertly transmits messages to Bob with the assistance of the\nXL-RIS, while evading detection by Willie. To enhance the covert communication\nperformance, we maximize the achievable covert rate by jointly optimizing the\nhybrid analog and digital beamformers at Alice, as well as the reflection\ncoefficient matrix at the XL-RIS. An alternating optimization algorithm is\nproposed to solve the joint beamforming design problem. For the hybrid\nbeamformer design, a semi-closed-form solution for fully digital beamformer is\nfirst obtained by a weighted minimum mean-square error based algorithm, then\nthe baseband digital and analog beamformers at Alice are designed by\napproximating the fully digital beamformer via manifold optimization. For the\nXL-RIS's reflection coefficient matrix design, a low-complexity alternating\ndirection method of multipliers based algorithm is proposed to address the\nchallenge of large-scale variables and unit-modulus constraints. Numerical\nresults unveil that i) the near-field communications can achieve a higher\ncovert rate than the far-field covert communications in general, and still\nrealize covert transmission even if Willie is located at the same direction as\nBob and closer to the XL-RIS; ii) the proposed algorithm can enhance the covert\nrate significantly compared to the benchmark schemes; iii) the proposed\nalgorithm leads to a beam diffraction pattern that can bypass Willie and\nachieve high-rate covert transmission to Bob.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.13565", "title": "Large Malaysian Language Model Based on Mistral for Enhanced Local\n  Language Understanding", "abstract": "In this paper, we present significant advancements in the pretraining of\nMistral 7B, a large-scale language model, using a dataset of 32.6 GB,\nequivalent to 1.1 billion tokens. We explore the impact of extending the\ncontext length, releasing models with context lengths of 4096 and 32768 tokens,\nand further refining performance with a specialized 16384 context length\ninstruction-tuned model, we called it Malaysian Mistral.\n  Our experiments demonstrate the efficacy of continue pretraining and the\ninfluence of extended context lengths on Mistral 7B's language understanding\ncapabilities. Additionally, we release a model specifically tuned with a 16384\ncontext length instruction, showcasing its potential for capturing nuanced\nlanguage intricacies.\n  Furthermore, our research contributes to the benchmarking of Malaysian\nMistral against prominent language models, including ChatGPT3.5 and Claude 2.\nWe present compelling results indicating Malaysian Mistral's superior\nperformance on Tatabahasa (Malay grammar) test set, particularly when\nfine-tuned with instructions.\n  All models released at\nhttps://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13566", "title": "A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in\n  Recommendation", "abstract": "When devising recommendation services, it is important to account for the\ninterests of all content providers, encompassing not only newcomers but also\nminority demographic groups. In various instances, certain provider groups find\nthemselves underrepresented in the item catalog, a situation that can influence\nrecommendation results. Hence, platform owners often seek to regulate the\nexposure of these provider groups in the recommended lists. In this paper, we\npropose a novel cost-sensitive approach designed to guarantee these target\nexposure levels in pairwise recommendation models. This approach quantifies,\nand consequently mitigate, the discrepancies between the volume of\nrecommendations allocated to groups and their contribution in the item catalog,\nunder the principle of equity. Our results show that this approach, while\naligning groups exposure with their assigned levels, does not compromise to the\noriginal recommendation utility. Source code and pre-processed data can be\nretrieved at\nhttps://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.13568", "title": "Investigating the Performance of Soft Robotic Adaptive Feet with\n  Longitudinal and Transverse Arches", "abstract": "Biped robots usually adopt feet with a rigid structure that simplifies\nwalking on flat grounds and yet hinders ground adaptation in unstructured\nenvironments, thus jeopardizing stability. We recently explored in the SoftFoot\nthe idea of adapting a robotic foot to ground irregularities along the sagittal\nplane. Building on the previous results, we propose in this paper a novel\nrobotic foot able to adapt both in the sagittal and frontal planes, similarly\nto the human foot. It features five parallel modules with intrinsic\nlongitudinal adaptability that can be combined in many possible designs through\noptional rigid or elastic connections. By following a methodological design\napproach, we narrow down the design space to five candidate foot designs and\nimplement them on a modular system. Prototypes are tested experimentally via\ncontrolled application of force, through a robotic arm, onto a sensorized plate\nendowed with different obstacles. Their performance is compared, using also a\nrigid foot and the previous SoftFoot as a baseline. Analysis of footprint\nstability shows that the introduction of the transverse arch, by elastically\nconnecting the five parallel modules, is advantageous for obstacle negotiation,\nespecially when obstacles are located under the forefoot. In addition to biped\nrobots' locomotion, this finding might also benefit lower-limb prostheses\ndesign.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13569", "title": "SPARC-LoRa: A Scalable, Power-efficient, Affordable, Reliable, and Cloud\n  Service-enabled LoRa Networking System for Agriculture Applications", "abstract": "With the rapid development of cloud and edge computing, Internet of Things\n(IoT) applications have been deployed in various aspects of human life. In this\npaper, we design and implement a holistic LoRa-based IoT system with LoRa\ncommunication capabilities, named SPARC-LoRa, which consists of field sensor\nnodes and a gateway connected to the Internet. SPARC-LoRa has the following\nimportant features. First, the proposed wireless network of SPARC-LoRa is\neven-driven and using off-the-shelf microcontroller and LoRa communication\nmodules with a customized PCB design to integrate all the hardware. This\nenables SPARC-LoRa to achieve low power consumption, long range communication,\nand low cost. With a new connection-based upper layer protocol design, the\nscalability and communication reliability of SPARC-loRa can be achieved.\nSecond, an open source software including sensor nodes and servers is designed\nbased on Docker container with cloud storage, computing, and LTE\nfunctionalities. In order to achieve reliable wireless communication under\nextreme conditions, a relay module is designed and applied to SPARC-LoRa to\nforward the data from sensor nodes to the gateway node. The system design and\nimplementation is completely open source and hosted on the DigitalOcean Droplet\nCloud. Hence, the proposed system enables further research and applications in\nboth academia and industry. The proposed system has been tested in real fields\nunder different and extreme environmental conditions in Salt Lake City, Utah\nand the University of Nebraska-Lincoln. The experimental results validate the\nfeatures of SPARC-LoRa including low power, reliability, and cloud services\nprovided by SPARC-LoRa.", "field": "Computer Science", "categories": "cs.NI,cs.DC"}, {"arxiv_id": "2401.1357", "title": "Guided Diffusion for Fast Inverse Design of Density-based Mechanical\n  Metamaterials", "abstract": "Mechanical metamaterial is a synthetic material that can possess\nextraordinary physical characteristics, such as abnormal elasticity, stiffness,\nand stability, by carefully designing its internal structure. To make\nmetamaterials contain delicate local structures with unique mechanical\nproperties, it is a potential method to represent them through high-resolution\nvoxels. However, it brings a substantial computational burden. To this end,\nthis paper proposes a fast inverse design method, whose core is an advanced\ndeep generative AI algorithm, to generate voxel-based mechanical metamaterials.\nSpecifically, we use the self-conditioned diffusion model, capable of\ngenerating a microstructure with a resolution of $128^3$ to approach the\nspecified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid\nreverse design tool facilitates the exploration of extreme metamaterials, the\nsequence interpolation in metamaterials, and the generation of diverse\nmicrostructures for multi-scale design. This flexible and adaptive generative\ntool is of great value in structural engineering or other mechanical systems\nand can stimulate more subsequent research.", "field": "Computer Science", "categories": "cs.CE,cs.LG"}, {"arxiv_id": "2401.13573", "title": "Distributed matrix multiplication with straggler tolerance using\n  algebraic function fields", "abstract": "The problem of straggler mitigation in distributed matrix multiplication\n(DMM) is considered for a large number of worker nodes and a fixed small finite\nfield. Polynomial codes and matdot codes are generalized by making use of\nalgebraic function fields (i.e., algebraic functions over an algebraic curve)\nover a finite field. The construction of optimal solutions is translated to a\ncombinatorial problem on the Weierstrass semigroups of the corresponding\nalgebraic curves. Optimal or almost optimal solutions are provided. These have\nthe same computational complexity per worker as classical polynomial and matdot\ncodes, and their recovery thresholds are almost optimal in the asymptotic\nregime (growing number of workers and a fixed finite field).", "field": "Computer Science", "categories": "cs.IT,math.IT"}, {"arxiv_id": "2401.13575", "title": "CNN architecture extraction on edge GPU", "abstract": "Neural networks have become popular due to their versatility and\nstate-of-the-art results in many applications, such as image classification,\nnatural language processing, speech recognition, forecasting, etc. These\napplications are also used in resource-constrained environments such as\nembedded devices. In this work, the susceptibility of neural network\nimplementations to reverse engineering is explored on the NVIDIA Jetson Nano\nmicrocomputer via side-channel analysis. To this end, an architecture\nextraction attack is presented. In the attack, 15 popular convolutional neural\nnetwork architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented\non the GPU of Jetson Nano and the electromagnetic radiation of the GPU is\nanalyzed during the inference operation of the neural networks. The results of\nthe analysis show that neural network architectures are easily distinguishable\nusing deep learning-based side-channel analysis.", "field": "Computer Science", "categories": "cs.CR,cs.LG"}, {"arxiv_id": "2401.13578", "title": "WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition", "abstract": "This work explores an emerging security threat against deep neural networks\n(DNNs) based image classification, i.e., backdoor attack. In this scenario, the\nattacker aims to inject a backdoor into the model by manipulating training\ndata, such that the backdoor could be activated by a particular trigger and\nbootstraps the model to make a target prediction at inference. Currently, most\nexisting data poisoning-based attacks struggle to achieve success at low\npoisoning ratios, increasing the risk of being defended by defense methods. In\nthis paper, we propose a novel frequency-based backdoor attack via Wavelet\nPacket Decomposition (WPD), WPD decomposes the original image signal to a\nspectrogram that contains frequency information with different semantic\nmeanings. We leverage WPD to statistically analyze the frequency distribution\nof the dataset to infer the key frequency regions the DNNs would focus on, and\nthe trigger information is only injected into the key frequency regions. Our\nmethod mainly includes three parts: 1) the selection of the poisoning frequency\nregions in spectrogram; 2) trigger generation; 3) the generation of the\npoisoned dataset. Our method is stealthy and precise, evidenced by the 98.12%\nAttack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio\n0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can\nbypass most existing defense methods. Besides, we also provide visualization\nanalyses to explain why our method works.", "field": "Computer Science", "categories": "cs.CR,I.4.9"}, {"arxiv_id": "2401.13581", "title": "Towards Efficient and Effective Deep Clustering with Dynamic Grouping\n  and Prototype Aggregation", "abstract": "Previous contrastive deep clustering methods mostly focus on instance-level\ninformation while overlooking the member relationship within groups/clusters,\nwhich may significantly undermine their representation learning and clustering\ncapability. Recently, some group-contrastive methods have been developed,\nwhich, however, typically rely on the samples of the entire dataset to obtain\npseudo labels and lack the ability to efficiently update the group assignments\nin a batch-wise manner. To tackle these critical issues, we present a novel\nend-to-end deep clustering framework with dynamic grouping and prototype\naggregation, termed as DigPro. Specifically, the proposed dynamic grouping\nextends contrastive learning from instance-level to group-level, which is\neffective and efficient for timely updating groups. Meanwhile, we perform\ncontrastive learning on prototypes in a spherical feature space, termed as\nprototype aggregation, which aims to maximize the inter-cluster distance.\nNotably, with an expectation-maximization framework, DigPro simultaneously\ntakes advantage of compact intra-cluster connections, well-separated clusters,\nand efficient group updating during the self-supervised training. Extensive\nexperiments on six image benchmarks demonstrate the superior performance of our\napproach over the state-of-the-art. Code is available at\nhttps://github.com/Regan-Zhang/DigPro.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.13584", "title": "Securing the Invisible Thread: A Comprehensive Analysis of BLE Tracker\n  Security in Apple AirTags and Samsung SmartTags", "abstract": "This study presents an in-depth analysis of the security landscape in\nBluetooth Low Energy (BLE) tracking systems, with a particular emphasis on\nApple AirTags and Samsung SmartTags, including their cryptographic frameworks.\nOur investigation traverses a wide spectrum of attack vectors such as physical\ntampering, firmware exploitation, signal spoofing, eavesdropping, jamming, app\nsecurity flaws, Bluetooth security weaknesses, location spoofing, threats to\nowner devices, and cloud-related vulnerabilities. Moreover, we delve into the\nsecurity implications of the cryptographic methods utilized in these systems.\nOur findings reveal that while BLE trackers like AirTags and SmartTags offer\nsubstantial utility, they also pose significant security risks. Notably,\nApple's approach, which prioritizes user privacy by removing intermediaries,\ninadvertently leads to device authentication challenges, evidenced by\nsuccessful AirTag spoofing instances. Conversely, Samsung SmartTags, designed\nto thwart beacon spoofing, raise critical concerns about cloud security and\nuser privacy. Our analysis also highlights the constraints faced by these\ndevices due to their design focus on battery life conservation, particularly\nthe absence of secure boot processes, which leaves them susceptible to OS\nmodification and a range of potential attacks. The paper concludes with\ninsights into the anticipated evolution of these tracking systems. We predict\nthat future enhancements will likely focus on bolstering security features,\nespecially as these devices become increasingly integrated into the broader IoT\necosystem and face evolving privacy regulations. This shift is imperative to\naddress the intricate balance between functionality and security in\nnext-generation BLE tracking systems.", "field": "Computer Science", "categories": "cs.CR"}, {"arxiv_id": "2401.13585", "title": "Latency vs precision: Stability preserving perception scheduling", "abstract": "In robotic systems, perception latency is a term that refers to the computing\ntime measured from the data acquisition to the moment in which perception\noutput is ready to be used to compute control commands. There is a compromise\nbetween perception latency, precision for the overall robotic system, and\ncomputational resource usage referred to here as the latency-precision\ntrade-off. In this work, we analyze a robot model given by a linear system, a\nzero-order hold controller, and measurements taken by several perception mode\npossibilities with different noise levels. We show that the analysis of this\nsystem is reduced to studying an equivalent switching system. Our goal is to\nschedule perception modes such that stability is attained while optimizing a\ncost function that models the latency-precision trade-off. Our solution\nframework comprises three main tools: the construction of perception scheduling\npolicy candidates, admissibility verification for policy candidates, and\noptimal strategies based on admissible policies.", "field": "Computer Science", "categories": "eess.SY,cs.RO,cs.SY,math.OC"}, {"arxiv_id": "2401.13586", "title": "Prompt Weight Experiments for LLM Instruction Fine-Tuning", "abstract": "We present a small study analyzing how prompt token classification loss\nweighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on\ninstruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1\nand LLaMA 2 using multiple instruction datasets. We found that models\nfine-tuned on our short-completion dataset have a negative quadratic\nrelationship with PLW while models fine-tuned on long-completion datasets were\nunaffected by PLW.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.CL"}, {"arxiv_id": "2401.13587", "title": "Deep Learning Based Adaptive Joint mmWave Beam Alignment", "abstract": "The challenging propagation environment, combined with the hardware\nlimitations of mmWave systems, gives rise to the need for accurate initial\naccess beam alignment strategies with low latency and high achievable\nbeamforming gain. Much of the recent work in this area either focuses on\none-sided beam alignment, or, joint beam alignment methods where both sides of\nthe link perform a sequence of fixed channel probing steps. Codebook-based\nnon-adaptive beam alignment schemes have the potential to allow multiple user\nequipment (UE) to perform initial access beam alignment in parallel whereas\nadaptive schemes are favourable in achievable beamforming gain. This work\nintroduces a novel deep learning based joint beam alignment scheme that aims to\ncombine the benefits of adaptive, codebook-free beam alignment at the UE side\nwith the advantages of a codebook-sweep based scheme at the base station. The\nproposed end-to-end trainable scheme is compatible with current cellular\nstandard signaling and can be readily integrated into the standard without\nrequiring significant changes to it. Extensive simulations demonstrate superior\nperformance of the proposed approach over purely codebook-based ones.", "field": "Computer Science", "categories": "cs.IT,eess.SP,math.IT"}, {"arxiv_id": "2401.13588", "title": "Evaluation of General Large Language Models in Contextually Assessing\n  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record\n  Notes", "abstract": "The field of healthcare has increasingly turned its focus towards Large\nLanguage Models (LLMs) due to their remarkable performance. However, their\nperformance in actual clinical applications has been underexplored. Traditional\nevaluations based on question-answering tasks don't fully capture the nuanced\ncontexts. This gap highlights the need for more in-depth and practical\nassessments of LLMs in real-world healthcare settings. Objective: We sought to\nevaluate the performance of LLMs in the complex clinical context of adult\ncritical care medicine using systematic and comprehensible analytic methods,\nincluding clinician annotation and adjudication. Methods: We investigated the\nperformance of three general LLMs in understanding and processing real-world\nclinical notes. Concepts from 150 clinical notes were identified by MetaMap and\nthen labeled by 9 clinicians. Each LLM's proficiency was evaluated by\nidentifying the temporality and negation of these concepts using different\nprompts for an in-depth analysis. Results: GPT-4 showed overall superior\nperformance compared to other LLMs. In contrast, both GPT-3.5 and\ntext-davinci-003 exhibit enhanced performance when the appropriate prompting\nstrategies are employed. The GPT family models have demonstrated considerable\nefficiency, evidenced by their cost-effectiveness and time-saving capabilities.\nConclusion: A comprehensive qualitative performance evaluation framework for\nLLMs is developed and operationalized. This framework goes beyond singular\nperformance aspects. With expert annotations, this methodology not only\nvalidates LLMs' capabilities in processing complex medical data but also\nestablishes a benchmark for future LLM evaluations across specialized domains.", "field": "Computer Science", "categories": "cs.CL,cs.AI,cs.SE"}, {"arxiv_id": "2401.13594", "title": "Graph Guided Question Answer Generation for Procedural\n  Question-Answering", "abstract": "In this paper, we focus on task-specific question answering (QA). To this\nend, we introduce a method for generating exhaustive and high-quality training\ndata, which allows us to train compact (e.g., run on a mobile device),\ntask-specific QA models that are competitive against GPT variants. The key\ntechnological enabler is a novel mechanism for automatic question-answer\ngeneration from procedural text which can ingest large amounts of textual\ninstructions and produce exhaustive in-domain QA training data. While current\nQA data generation methods can produce well-formed and varied data, their\nnon-exhaustive nature is sub-optimal for training a QA model. In contrast, we\nleverage the highly structured aspect of procedural text and represent each\nstep and the overall flow of the procedure as graphs. We then condition on\ngraph nodes to automatically generate QA pairs in an exhaustive and\ncontrollable manner. Comprehensive evaluations of our method show that: 1)\nsmall models trained with our data achieve excellent performance on the target\nQA task, even exceeding that of GPT3 and ChatGPT despite being several orders\nof magnitude smaller. 2) semantic coverage is the key indicator for downstream\nQA performance. Crucially, while large language models excel at syntactic\ndiversity, this does not necessarily result in improvements on the end QA\nmodel. In contrast, the higher semantic coverage provided by our method is\ncritical for QA performance.", "field": "Computer Science", "categories": "cs.CL,cs.AI,I.2.7"}, {"arxiv_id": "2401.13596", "title": "PLATE: A perception-latency aware estimator,", "abstract": "Target tracking is a popular problem with many potential applications. There\nhas been a lot of effort on improving the quality of the detection of targets\nusing cameras through different techniques. In general, with higher\ncomputational effort applied, i.e., a longer perception-latency, a better\ndetection accuracy is obtained. However, it is not always useful to apply the\nlongest perception-latency allowed, particularly when the environment doesn't\nrequire to and when the computational resources are shared between other tasks.\nIn this work, we propose a new Perception-LATency aware Estimator (PLATE),\nwhich uses different perception configurations in different moments of time in\norder to optimize a certain performance measure. This measure takes into\naccount a perception-latency and accuracy trade-off aiming for a good\ncompromise between quality and resource usage. Compared to other heuristic\nframe-skipping techniques, PLATE comes with a formal complexity and optimality\nanalysis. The advantages of PLATE are verified by several experiments including\nan evaluation over a standard benchmark with real data and using state of the\nart deep learning object detection methods for the perception stage.", "field": "Computer Science", "categories": "eess.SY,cs.CV,cs.SY,math.OC"}, {"arxiv_id": "2401.13598", "title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for\n  Zero-shot Document-level Relation Triplet Extraction", "abstract": "Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in\ninformation systems that aims to simultaneously extract entities with semantic\nrelations from a document. Existing methods heavily rely on a substantial\namount of fully labeled data. However, collecting and annotating data for newly\nemerging relations is time-consuming and labor-intensive. Recent advanced Large\nLanguage Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text\ngeneration capabilities, inspiring us to explore an alternative approach for\nobtaining auto-labeled documents with new relations. In this paper, we propose\na Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework,\nwhich generates labeled data by retrieval and denoising knowledge from LLMs,\ncalled GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide\nChatGPT to generate labeled long-text data step by step. To improve the quality\nof synthetic data, we propose a denoising strategy based on the consistency of\ncross-document knowledge. Leveraging our denoised synthetic data, we proceed to\nfine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.\nWe perform experiments for both zero-shot document-level relation and triplet\nextraction on two public datasets. The experimental results illustrate that our\nGenRDK framework outperforms strong baselines.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13601", "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models", "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone\nsubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs or\noutputs via cost-effective training strategies. The resulting models not only\npreserve the inherent reasoning and decision-making capabilities of LLMs but\nalso empower a diverse range of MM tasks. In this paper, we provide a\ncomprehensive survey aimed at facilitating further research of MM-LLMs.\nSpecifically, we first outline general design formulations for model\narchitecture and training pipeline. Subsequently, we provide brief\nintroductions of $26$ existing MM-LLMs, each characterized by its specific\nformulations. Additionally, we review the performance of MM-LLMs on mainstream\nbenchmarks and summarize key training recipes to enhance the potency of\nMM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently\nmaintaining a real-time tracking website for the latest developments in the\nfield. We hope that this survey contributes to the ongoing advancement of the\nMM-LLMs domain.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13602", "title": "Perception-latency aware distributed target tracking", "abstract": "This work is devoted to the problem of distributed target tracking when a\nteam of robots detect the target through a variable perception-latency\nmechanism. A reference for the robots to track is constructed in terms of a\ndesired formation around the estimation of the target position. However, it is\nnoted that due to the perception-latency, classical estimation techniques have\nsmoothness issues which prevent asymptotic stability for the formation control.\nWe propose a near-optimal smooth-output estimator which circumvents this issue.\nMoreover, local estimations are fused using novel dynamic consensus techniques.\nThe advantages of the proposal as well as a comparison with a non-smooth\noptimal alternative are discussed through simulation examples.", "field": "Computer Science", "categories": "eess.SY,cs.SY,math.OC"}, {"arxiv_id": "2401.13604", "title": "Stream-based perception for cognitive agents in mobile ecosystems", "abstract": "Cognitive agent abstractions can help to engineer intelligent systems across\nmobile devices. On smartphones, the data obtained from onboard sensors can give\nvaluable insights into the user's current situation. Unfortunately, today's\ncognitive agent frameworks cannot cope well with the challenging\ncharacteristics of sensor data. Sensor data is located on a low abstraction\nlevel and the individual data elements are not meaningful when observed in\nisolation. In contrast, cognitive agents operate on high-level percepts and\nlack the means to effectively detect complex spatio-temporal patterns in\nsequences of multiple percepts. In this paper, we present a stream-based\nperception approach that enables the agents to perceive meaningful situations\nin low-level sensor data streams. We present a crowdshipping case study where\nautonomous, self-interested agents collaborate to deliver parcels to their\ndestinations. We show how situations derived from smartphone sensor data can\ntrigger and guide auctions, which the agents use to reach agreements.\nExperiments with real smartphone data demonstrate the benefits of stream-based\nagent perception.", "field": "Computer Science", "categories": "cs.AI,cs.MA"}, {"arxiv_id": "2401.13605", "title": "Regulating AI-Based Remote Biometric Identification. Investigating the\n  Public Demand for Bans, Audits, and Public Database Registrations", "abstract": "AI is increasingly being used in the public sector, including public\nsecurity. In this context, the use of AI-powered remote biometric\nidentification (RBI) systems is a much-discussed technology. RBI systems are\nused to identify criminal activity in public spaces, but are criticised for\ninheriting biases and violating fundamental human rights. It is therefore\nimportant to ensure that such systems are developed in the public interest,\nwhich means that any technology that is deployed for public use needs to be\nscrutinised. While there is a consensus among business leaders, policymakers\nand scientists that AI must be developed in an ethical and trustworthy manner,\nscholars have argued that ethical guidelines do not guarantee ethical AI, but\nrather prevent stronger regulation of AI. As a possible counterweight, public\nopinion can have a decisive influence on policymakers to establish boundaries\nand conditions under which AI systems should be used -- if at all. However, we\nknow little about the conditions that lead to regulatory demand for AI systems.\nIn this study, we focus on the role of trust in AI as well as trust in law\nenforcement as potential factors that may lead to demands for regulation of AI\ntechnology. In addition, we explore the mediating effects of discrimination\nperceptions regarding RBI. We test the effects on four different use cases of\nRBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose\nof use (persecution of criminals vs. safeguarding public events) in a survey\namong German citizens. We found that German citizens do not differentiate\nbetween the different modes of application in terms of their demand for RBI\nregulation. Furthermore, we show that perceptions of discrimination lead to a\ndemand for stronger regulation, while trust in AI and trust in law enforcement\nlead to opposite effects in terms of demand for a ban on RBI systems.", "field": "Computer Science", "categories": "cs.CY"}, {"arxiv_id": "2401.13606", "title": "Run-to-Run Control With Bayesian Optimization for Soft Landing of\n  Short-Stroke Reluctance Actuators", "abstract": "There is great interest in minimizing the impact forces of reluctance\nactuators during commutations, in order to reduce contact bouncing, acoustic\nnoise and mechanical wear. In this regard, a run-to-run control algorithm is\nproposed to decrease the contact velocity, by exploiting the repetitive\noperations of these devices. The complete control is presented, with special\nfocus on the optimization method and the input definition. The search method is\nbased on Bayesian optimization, and several additions are introduced for its\napplication in run-to-run control, e.g. the removal of stored points and the\ndefinition of a new acquisition function. Additionally, methods for the input\nparametrization and dimension reduction are presented. For analysis, Monte\nCarlo simulations are performed using a dynamic model of a commercial solenoid\nvalve, comparing the proposed search method with two alternatives. Furthermore,\nthe control strategy is validated through experimental testing, using several\ndevices from the same ensemble of solenoid valves.", "field": "Computer Science", "categories": "eess.SY,cs.SY"}, {"arxiv_id": "2401.13609", "title": "Building Contextual Knowledge Graphs for Personalized Learning\n  Recommendations using Text Mining and Semantic Graph Completion", "abstract": "Modelling learning objects (LO) within their context enables the learner to\nadvance from a basic, remembering-level, learning objective to a higher-order\none, i.e., a level with an application- and analysis objective. While\nhierarchical data models are commonly used in digital learning platforms, using\ngraph-based models enables representing the context of LOs in those platforms.\nThis leads to a foundation for personalized recommendations of learning paths.\nIn this paper, the transformation of hierarchical data models into knowledge\ngraph (KG) models of LOs using text mining is introduced and evaluated. We\nutilize custom text mining pipelines to mine semantic relations between\nelements of an expert-curated hierarchical model. We evaluate the KG structure\nand relation extraction using graph quality-control metrics and the comparison\nof algorithmic semantic-similarities to expert-defined ones. The results show\nthat the relations in the KG are semantically comparable to those defined by\ndomain experts, and that the proposed KG improves representing and linking the\ncontexts of LOs through increasing graph communities and betweenness\ncentrality.", "field": "Computer Science", "categories": "cs.IR"}, {"arxiv_id": "2401.1361", "title": "Scale-free vision-based aerial control of a ground formation with hybrid\n  topology", "abstract": "We present a novel vision-based control method to make a group of ground\nmobile robots achieve a specified formation shape with unspecified size. Our\napproach uses multiple aerial control units equipped with downward-facing\ncameras, each observing a partial subset of the multirobot team. The units\ncompute the control commands from the ground robots' image projections, using\nneither calibration nor scene scale information, and transmit them to the\nrobots. The control strategy relies on the calculation of image similarity\ntransformations, and we show it to be asymptotically stable if the overlaps\nbetween the subsets of controlled robots satisfy certain conditions. The\npresence of the supervisory units, which coordinate their motions to guarantee\na correct control performance, gives rise to a hybrid system topology. All in\nall, the proposed system provides relevant practical advantages in simplicity\nand flexibility. Within the problem of controlling a team shape, our\ncontribution lies in addressing several simultaneous challenges: the controller\nneeds only partial information of the robotic group, does not use distance\nmeasurements or global reference frames, is designed for unicycle agents, and\ncan accommodate topology changes. We present illustrative simulation results.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY,math.OC"}, {"arxiv_id": "2401.13611", "title": "Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired\n  Users using Intermediate ASR Features and Human Memory Models", "abstract": "Neural networks have been successfully used for non-intrusive speech\nintelligibility prediction. Recently, the use of feature representations\nsourced from intermediate layers of pre-trained self-supervised and\nweakly-supervised models has been found to be particularly useful for this\ntask. This work combines the use of Whisper ASR decoder layer representations\nas neural network input features with an exemplar-based, psychologically\nmotivated model of human memory to predict human intelligibility ratings for\nhearing-aid users. Substantial performance improvement over an established\nintrusive HASPI baseline system is found, including on enhancement systems and\nlisteners unseen in the training data, with a root mean squared error of 25.3\ncompared with the baseline of 28.7.", "field": "Computer Science", "categories": "cs.SD,cs.AI,eess.AS"}, {"arxiv_id": "2401.13612", "title": "Intermittent Connectivity Maintenance With Heterogeneous Robots", "abstract": "We consider a scenario of cooperative task servicing, with a team of\nheterogeneous robots with different maximum speeds and communication radii, in\ncharge of keeping the network intermittently connected. We abstract the task\nlocations into a $1D$ cycle graph that is traversed by the communicating\nrobots, and we discuss intermittent communication strategies so that each task\nlocation is periodically visited, with a worst--case revisiting time. Robots\nmove forward and backward along the cycle graph, exchanging data with their\nprevious and next neighbors when they meet, and updating their region\nboundaries. Asymptotically, each robot is in charge of a region of the cycle\ngraph, depending on its capabilities. The method is distributed, and robots\nonly exchange data when they meet.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY"}, {"arxiv_id": "2401.13613", "title": "Enhancing Image Retrieval : A Comprehensive Study on Photo Search using\n  the CLIP Mode", "abstract": "Photo search, the task of retrieving images based on textual queries, has\nwitnessed significant advancements with the introduction of CLIP (Contrastive\nLanguage-Image Pretraining) model. CLIP leverages a vision-language pre\ntraining approach, wherein it learns a shared representation space for images\nand text, enabling cross-modal understanding. This model demonstrates the\ncapability to understand the semantic relationships between diverse image and\ntext pairs, allowing for efficient and accurate retrieval of images based on\nnatural language queries. By training on a large-scale dataset containing\nimages and their associated textual descriptions, CLIP achieves remarkable\ngeneralization, providing a powerful tool for tasks such as zero-shot learning\nand few-shot classification. This abstract summarizes the foundational\nprinciples of CLIP and highlights its potential impact on advancing the field\nof photo search, fostering a seamless integration of natural language\nunderstanding and computer vision for improved information retrieval in\nmultimedia applications", "field": "Computer Science", "categories": "cs.CV,cs.AI"}, {"arxiv_id": "2401.13614", "title": "Equitable Persistent Coverage of Non-Convex Environments with\n  Graph-Based Planning", "abstract": "In this paper we tackle the problem of persistently covering a complex\nnon-convex environment with a team of robots. We consider scenarios where the\ncoverage quality of the environment deteriorates with time, requiring to\nconstantly revisit every point. As a first step, our solution finds a partition\nof the environment where the amount of work for each robot, weighted by the\nimportance of each point, is equal. This is achieved using a power diagram and\nfinding an equitable partition through a provably correct distributed control\nlaw on the power weights. Compared to other existing partitioning methods, our\nsolution considers a continuous environment formulation with non-convex\nobstacles. In the second step, each robot computes a graph that gathers\nsweep-like paths and covers its entire partition. At each planning time, the\ncoverage error at the graph vertices is assigned as weights of the\ncorresponding edges. Then, our solution is capable of efficiently finding the\noptimal open coverage path through the graph with respect to the coverage error\nper distance traversed. Simulation and experimental results are presented to\nsupport our proposal.", "field": "Computer Science", "categories": "cs.RO"}, {"arxiv_id": "2401.13621", "title": "DenoSent: A Denoising Objective for Self-Supervised Sentence\n  Representation Learning", "abstract": "Contrastive-learning-based methods have dominated sentence representation\nlearning. These methods regularize the representation space by pulling similar\nsentence representations closer and pushing away the dissimilar ones and have\nbeen proven effective in various NLP tasks, e.g., semantic textual similarity\n(STS) tasks. However, it is challenging for these methods to learn fine-grained\nsemantics as they only learn from the inter-sentence perspective, i.e., their\nsupervision signal comes from the relationship between data samples. In this\nwork, we propose a novel denoising objective that inherits from another\nperspective, i.e., the intra-sentence perspective. By introducing both discrete\nand continuous noise, we generate noisy sentences and then train our model to\nrestore them to their original form. Our empirical evaluations demonstrate that\nthis approach delivers competitive results on both semantic textual similarity\n(STS) and a wide range of transfer tasks, standing up well in comparison to\ncontrastive-learning-based methods. Notably, the proposed intra-sentence\ndenoising objective complements existing inter-sentence contrastive\nmethodologies and can be integrated with them to further enhance performance.\nOur code is available at https://github.com/xinghaow99/DenoSent.", "field": "Computer Science", "categories": "cs.CL"}, {"arxiv_id": "2401.13622", "title": "Cooperative Periodic Coverage With Collision Avoidance", "abstract": "In this paper we propose a periodic solution to the problem of persistently\ncovering a finite set of interest points with a group of autonomous mobile\nagents. These agents visit periodically the points and spend some time carrying\nout the coverage task, which we call coverage time. Since this periodic\npersistent coverage problem is NP-hard, we split it into three subproblems to\ncounteract its complexity. In the first place, we plan individual closed paths\nfor the agents to cover all the points. Second, we formulate a quadratically\nconstrained linear program to find the optimal coverage times and actions that\nsatisfy the coverage objective. Finally, we join together the individual plans\nof the agents in a periodic team plan by obtaining a schedule that guarantees\ncollision avoidance. To this end, we solve a mixed integer linear program that\nminimizes the time in which two or more agents move at the same time.\nEventually, we apply the proposed solution to an induction hob with mobile\ninductors for a domestic heating application and show its performance with\nexperiments on a real prototype.", "field": "Computer Science", "categories": "cs.RO,cs.SY,eess.SY"}, {"arxiv_id": "2401.13623", "title": "What Makes a Great Software Quality Assurance Engineer?", "abstract": "Software Quality Assurance (SQA) Engineers are responsible for assessing a\nproduct during every phase of the software development process to ensure that\nthe outcomes of each phase and the final product possess the desired qualities.\nIn general, a great SQA engineer needs to have a different set of abilities\nfrom development engineers to effectively oversee the entire product\ndevelopment process from beginning to end. Recent empirical studies identified\nimportant attributes of software engineers and managers, but the quality\nassurance role is overlooked. As software quality aspects have become more of a\npriority in the life cycle of software development, employers seek\nprofessionals that best suit the company's objectives and new graduates desire\nto make a valuable contribution through their job as an SQA engineer, but what\nmakes them great? We addressed this knowledge gap by conducting 25\nsemi-structured interviews and 363 survey respondents with software quality\nassurance engineers from different companies around the world. We use the data\ncollected from these activities to derive a comprehensive set of attributes\nthat are considered important. As a result of the interviews, twenty-five\nattributes were identified and grouped into five main categories: personal,\nsocial, technical, management, and decision-making attributes. Through a rating\nsurvey, we confirmed that the distinguishing characteristics of great SQA\nengineers are curiosity, the ability to communicate effectively, and critical\nthinking skills. This work will guide further studies with SQA practitioners,\nby considering contextual factors and providing some implications for research\nand practice.", "field": "Computer Science", "categories": "cs.SE"}, {"arxiv_id": "2401.13627", "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic\n  Image Restoration In the Wild", "abstract": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image\nrestoration method that harnesses generative prior and the power of model\nscaling up. Leveraging multi-modal techniques and advanced generative prior,\nSUPIR marks a significant advance in intelligent and realistic image\nrestoration. As a pivotal catalyst within SUPIR, model scaling dramatically\nenhances its capabilities and demonstrates new potential for image restoration.\nWe collect a dataset comprising 20 million high-resolution, high-quality images\nfor model training, each enriched with descriptive text annotations. SUPIR\nprovides the capability to restore images guided by textual prompts, broadening\nits application scope and potential. Moreover, we introduce negative-quality\nprompts to further improve perceptual quality. We also develop a\nrestoration-guided sampling method to suppress the fidelity issue encountered\nin generative-based restoration. Experiments demonstrate SUPIR's exceptional\nrestoration effects and its novel capacity to manipulate restoration through\ntextual prompts.", "field": "Computer Science", "categories": "cs.CV"}, {"arxiv_id": "2401.1363", "title": "Enabling Seamless Data Security, Consensus, and Trading in Vehicular\n  Networks", "abstract": "Cooperative driving is an emerging paradigm to enhance the safety and\nefficiency of autonomous vehicles. To ensure successful cooperation, road users\nmust reach a consensus for making collective decisions, while recording\nvehicular data to analyze and address failures related to such agreements. This\ndata has the potential to provide valuable insights into various vehicular\nevents, while also potentially improving accountability measures. Furthermore,\nvehicles may benefit from the ability to negotiate and trade services among\nthemselves, adding value to the cooperative driving framework. However, the\nmajority of proposed systems aiming to ensure data security, consensus, or\nservice trading, lack efficient and thoroughly validated mechanisms that\nconsider the distinctive characteristics of vehicular networks. These\nlimitations are amplified by a dependency on the centralized support provided\nby the infrastructure. Furthermore, corresponding mechanisms must diligently\naddress security concerns, especially regarding potential malicious or\nmisbehaving nodes, while also considering inherent constraints of the wireless\nmedium. We introduce the Verifiable Event Extension (VEE), an applicational\nextension designed for Intelligent Transportation System (ITS) messages. The\nVEE operates seamlessly with any existing standardized vehicular communications\nprotocol, addressing crucial aspects of data security, consensus, and trading\nwith minimal overhead. To achieve this, we employ blockchain techniques,\nByzantine fault tolerance (BFT) consensus protocols, and cryptocurrency-based\nmechanics. To assess our proposal's feasibility and lightweight nature, we\nemployed a hardware-in-the-loop setup for analysis. Experimental results\ndemonstrate the viability and efficiency of the VEE extension in overcoming the\nchallenges posed by the distributed and opportunistic nature of wireless\nvehicular communications.", "field": "Computer Science", "categories": "cs.DC"}, {"arxiv_id": "2401.13631", "title": "Quantifying the Impact of Frame Preemption on Combined TSN Shapers", "abstract": "Different scheduling mechanisms in Time Sensitive Networking (TSN) can be\nintegrated together to design and support complex architectures with enhanced\ncapabilities for mixed critical networks. Integrating Frame Preemption (FP)\nwith Credit-Based Shaper (CBS) and Gate Control List (GCL) opens up different\nmodes and configuration choices resulting in a complex evaluation of several\npossibilities and their impact on the Quality of Service (QoS). In this paper,\nwe implement and quantify the integration of preemptive CBS with GCL by\nincorporating FP into the architecture. Our experiments show that the\nend-to-end delay of Audio Video Bridging (AVB) flows shaped by CBS reduces\nsignificantly (up to 40\\%) when AVB flows are set to preemptable class. We\nfurther show that the jitter of Time Triggered (TT) traffic remains unaffected\nin \"with Hold/Release\" mode. Furthermore, we propose to introduce Guardband\n(GB) in the \"without Hold/Release\" to reduce the jitter of the TT flow. We\ncompare all the different integration modes, starting with CBS with GCL,\nextending it further to FP. We evaluate all feasible combinations in both\nsynthetic and realistic scenarios and offer recommendations for practical\nconfiguration methods.", "field": "Computer Science", "categories": "cs.NI"}, {"arxiv_id": "2401.13639", "title": "Winding Clearness for Differentiable Point Cloud Optimization", "abstract": "We propose to explore the properties of raw point clouds through the\n\\emph{winding clearness}, a concept we first introduce for assessing the\nclarity of the interior/exterior relationships represented by the winding\nnumber field of the point cloud. In geometric modeling, the winding number is a\npowerful tool for distinguishing the interior and exterior of a given surface\n$\\partial \\Omega$, and it has been previously used for point normal orientation\nand surface reconstruction. In this work, we introduce a novel approach to\nassess and optimize the quality of point clouds based on the winding clearness.\nWe observe that point clouds with reduced noise tend to exhibit improved\nwinding clearness. Accordingly, we propose an objective function that\nquantifies the error in winding clearness, solely utilizing the positions of\nthe point clouds. Moreover, we demonstrate that the winding clearness error is\ndifferentiable and can serve as a loss function in optimization-based and\nlearning-based point cloud processing. In the optimization-based method, the\nloss function is directly back-propagated to update the point positions,\nresulting in an overall improvement of the point cloud. In the learning-based\nmethod, we incorporate the winding clearness as a geometric constraint in the\ndiffusion-based 3D generative model. Experimental results demonstrate the\neffectiveness of optimizing the winding clearness in enhancing the quality of\nthe point clouds. Our method exhibits superior performance in handling noisy\npoint clouds with thin structures, highlighting the benefits of the global\nperspective enabled by the winding number.", "field": "Computer Science", "categories": "cs.GR"}, {"arxiv_id": "2401.13641", "title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition,\n  Soft Biometrics, and Explainability", "abstract": "Large Language Models (LLMs) such as GPT developed by OpenAI, have already\nshown astonishing results, introducing quick changes in our society. This has\nbeen intensified by the release of ChatGPT which allows anyone to interact in a\nsimple conversational way with LLMs, without any experience in the field\nneeded. As a result, ChatGPT has been rapidly applied to many different tasks\nsuch as code- and song-writer, education, virtual assistants, etc., showing\nimpressive results for tasks for which it was not trained (zero-shot learning).\n  The present study aims to explore the ability of ChatGPT, based on the recent\nGPT-4 multimodal LLM, for the task of face biometrics. In particular, we\nanalyze the ability of ChatGPT to perform tasks such as face verification,\nsoft-biometrics estimation, and explainability of the results. ChatGPT could be\nvery valuable to further increase the explainability and transparency of the\nautomatic decisions in human scenarios. Experiments are carried out in order to\nevaluate the performance and robustness of ChatGPT, using popular public\nbenchmarks and comparing the results with state-of-the-art methods in the\nfield. The results achieved in this study show the potential of LLMs such as\nChatGPT for face biometrics, especially to enhance explainability. For\nreproducibility reasons, we release all the code in GitHub.", "field": "Computer Science", "categories": "cs.CV,cs.AI,cs.CY,cs.LG"}, {"arxiv_id": "2401.13643", "title": "Design, Development, and Deployment of Context-Adaptive AI Systems for\n  Enhanced End-User Adoption", "abstract": "My research centers on the development of context-adaptive AI systems to\nimprove end-user adoption through the integration of technical methods. I\ndeploy these AI systems across various interaction modalities, including user\ninterfaces and embodied agents like robots, to expand their practical\napplicability. My research unfolds in three key stages: design, development,\nand deployment. In the design phase, user-centered approaches were used to\nunderstand user experiences with AI systems and create design tools for user\nparticipation in crafting AI explanations. In the ongoing development stage, a\nsafety-guaranteed AI system for a robot agent was created to automatically\nprovide adaptive solutions and explanations for unforeseen scenarios. The next\nsteps will involve the implementation and evaluation of context-adaptive AI\nsystems in various interaction forms. I seek to prioritize human needs in\ntechnology development, creating AI systems that tangibly benefit end-users in\nreal-world applications and enhance interaction experiences.", "field": "Computer Science", "categories": "cs.HC,cs.RO"}, {"arxiv_id": "2401.13645", "title": "Employing polyhedral methods to optimize stencils on FPGAs with\n  stencil-specific caches, data reuse, and wide data bursts", "abstract": "It is well known that to accelerate stencil codes on CPUs or GPUs and to\nexploit hardware caches and their lines optimizers must find spatial and\ntemporal locality of array accesses to harvest data-reuse opportunities. On\nFPGAs there is the burden that there are no built-in caches (or only pre-built\nhardware descriptions for cache blocks that are inefficient for stencil codes).\nBut this paper demonstrates that this lack is also a chance as polyhedral\nmethods can be used to generate stencil-specific cache-structures of the right\nsizes on the FPGA and to fill and flush them efficiently with wide bursts\nduring stencil execution. The paper shows how to derive the appropriate\ndirectives and code restructurings from stencil codes so that the FPGA compiler\ngenerates fast stencil hardware. Switching on our optimization improves the\nruntime of a set of 10 stencils by between 43x and 156x.", "field": "Computer Science", "categories": "cs.PL"}, {"arxiv_id": "2401.13649", "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web\n  Tasks", "abstract": "Autonomous agents capable of planning, reasoning, and executing actions on\nthe web offer a promising avenue for automating computer tasks. However, the\nmajority of existing benchmarks primarily focus on text-based agents,\nneglecting many natural tasks that require visual information to effectively\nsolve. Given that most computer interfaces cater to human perception, visual\ninformation often augments textual data in ways that text-only models struggle\nto harness effectively. To bridge this gap, we introduce VisualWebArena, a\nbenchmark designed to assess the performance of multimodal web agents on\nrealistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set\nof diverse and complex web-based tasks that evaluate various capabilities of\nautonomous multimodal agents. To perform on this benchmark, agents need to\naccurately process image-text inputs, interpret natural language instructions,\nand execute actions on websites to accomplish user-defined objectives. We\nconduct an extensive evaluation of state-of-the-art LLM-based autonomous\nagents, including several multimodal models. Through extensive quantitative and\nqualitative analysis, we identify several limitations of text-only LLM agents,\nand reveal gaps in the capabilities of state-of-the-art multimodal language\nagents. VisualWebArena provides a framework for evaluating multimodal\nautonomous language agents, and offers insights towards building stronger\nautonomous agents for the web. Our code, baseline models, and data is publicly\navailable at https://jykoh.com/vwa.", "field": "Computer Science", "categories": "cs.LG,cs.CL,cs.CV"}, {"arxiv_id": "2401.13652", "title": "Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity\n  Detectors", "abstract": "In this paper, we present a novel approach for detecting the discontinuity\ninterfaces of a discontinuous function. This approach leverages Graph-Informed\nNeural Networks (GINNs) and sparse grids to address discontinuity detection\nalso in domains of dimension larger than 3. GINNs, trained to identify troubled\npoints on sparse grids, exploit graph structures built on the grids to achieve\nefficient and accurate discontinuity detection performances. We also introduce\na recursive algorithm for general sparse grid-based detectors, characterized by\nconvergence properties and easy applicability. Numerical experiments on\nfunctions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust\ngeneralization of GINNs in detecting discontinuity interfaces. Notably, the\ntrained GINNs offer portability and versatility, allowing integration into\nvarious algorithms and sharing among users.", "field": "Computer Science", "categories": "cs.LG,cs.AI,cs.NA,math.NA,68T07, 03D32, 65D40"}, {"arxiv_id": "2401.13653", "title": "HetDAPAC: Distributed Attribute-Based Private Access Control with\n  Heterogeneous Attributes", "abstract": "Verifying user attributes to provide fine-grained access control to databases\nis fundamental to an attribute-based authentication system. In such systems,\neither a single (central) authority verifies all attributes, or multiple\nindependent authorities verify individual attributes distributedly to allow a\nuser to access records stored on the servers. While a \\emph{central} setup is\nmore communication cost efficient, it causes privacy breach of \\emph{all} user\nattributes to a central authority. Recently, Jafarpisheh et al. studied an\ninformation theoretic formulation of the \\emph{distributed} multi-authority\nsetup with $N$ non-colluding authorities, $N$ attributes and $K$ possible\nvalues for each attribute, called an $(N,K)$ distributed attribute-based\nprivate access control (DAPAC) system, where each server learns only one\nattribute value that it verifies, and remains oblivious to the remaining $N-1$\nattributes. We show that off-loading a subset of attributes to a central server\nfor verification improves the achievable rate from $\\frac{1}{2K}$ in\nJafarpisheh et al. to $\\frac{1}{K+1}$ in this paper, thus \\emph{almost doubling\nthe rate} for relatively large $K$, while sacrificing the privacy of a few\npossibly non-sensitive attributes.", "field": "Computer Science", "categories": "cs.IT,cs.CR,cs.NI,eess.SP,math.IT"}, {"arxiv_id": "2401.13656", "title": "Navigating Multidimensional Ideologies with Reddit's Political Compass:\n  Economic Conflict and Social Affinity", "abstract": "The prevalent perspective in quantitative research on opinion dynamics\nflattens the landscape of the online political discourse into a traditional\nleft--right dichotomy. While this approach helps simplify the analysis and\nmodeling effort, it also neglects the intrinsic multidimensional richness of\nideologies. In this study, we analyze social interactions on Reddit, under the\nlens of a multi-dimensional ideological framework: the political compass. We\nexamine over 8 million comments posted on the subreddits /r/PoliticalCompass\nand /r/PoliticalCompassMemes during 2020--2022. By leveraging their\nself-declarations, we disentangle the ideological dimensions of users into\neconomic (left--right) and social (libertarian--authoritarian) axes. In\naddition, we characterize users by their demographic attributes (age, gender,\nand affluence).\n  We find significant homophily for interactions along the social axis of the\npolitical compass and demographic attributes. Compared to a null model,\ninteractions among individuals of similar ideology surpass expectations by 6%.\nIn contrast, we uncover a significant heterophily along the economic axis:\nleft/right interactions exceed expectations by 10%. Furthermore, heterophilic\ninteractions are characterized by a higher language toxicity than homophilic\ninteractions, which hints at a conflictual discourse between every opposite\nideology. Our results help reconcile apparent contradictions in recent\nliterature, which found a superposition of homophilic and heterophilic\ninteractions in online political discussions. By disentangling such\ninteractions into the economic and social axes we pave the way for a deeper\nunderstanding of opinion dynamics on social media.", "field": "Computer Science", "categories": "cs.SI,cs.CY,physics.soc-ph,stat.AP"}, {"arxiv_id": "2401.13657", "title": "Inadequacy of common stochastic neural networks for reliable clinical\n  decision support", "abstract": "Widespread adoption of AI for medical decision making is still hindered due\nto ethical and safety-related concerns. For AI-based decision support systems\nin healthcare settings it is paramount to be reliable and trustworthy. Common\ndeep learning approaches, however, have the tendency towards overconfidence\nunder data shift. Such inappropriate extrapolation beyond evidence-based\nscenarios may have dire consequences. This highlights the importance of\nreliable estimation of local uncertainty and its communication to the end user.\nWhile stochastic neural networks have been heralded as a potential solution to\nthese issues, this study investigates their actual reliability in clinical\napplications. We centered our analysis on the exemplary use case of mortality\nprediction for ICU hospitalizations using EHR from MIMIC3 study. For\npredictions on the EHR time series, Encoder-Only Transformer models were\nemployed. Stochasticity of model functions was achieved by incorporating common\nmethods such as Bayesian neural network layers and model ensembles. Our models\nachieve state of the art performance in terms of discrimination performance\n(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality\nprediction benchmark. However, epistemic uncertainty is critically\nunderestimated by the selected stochastic deep learning methods. A heuristic\nproof for the responsible collapse of the posterior distribution is provided.\nOur findings reveal the inadequacy of commonly used stochastic deep learning\napproaches to reliably recognize OoD samples. In both methods, unsubstantiated\nmodel confidence is not prevented due to strongly biased functional posteriors,\nrendering them inappropriate for reliable clinical decision support. This\nhighlights the need for approaches with more strictly enforced or inherent\ndistance-awareness to known data points, e.g., using kernel-based techniques.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.1366", "title": "MambaByte: Token-free Selective State Space Model", "abstract": "Token-free language models learn directly from raw bytes and remove the bias\nof subword tokenization. Operating on bytes, however, results in significantly\nlonger sequences, and standard autoregressive Transformers scale poorly in such\nsettings. We experiment with MambaByte, a token-free adaptation of the Mamba\nstate space model, trained autoregressively on byte sequences. Our experiments\nindicate the computational efficiency of MambaByte compared to other byte-level\nmodels. We also find MambaByte to be competitive with and even outperform\nstate-of-the-art subword Transformers. Furthermore, owing to linear scaling in\nlength, MambaByte benefits from fast inference compared to Transformers. Our\nfindings establish the viability of MambaByte in enabling token-free language\nmodeling.", "field": "Computer Science", "categories": "cs.CL,cs.LG"}, {"arxiv_id": "2401.13662", "title": "The Definitive Guide to Policy Gradients in Deep Reinforcement Learning:\n  Theory, Algorithms and Implementations", "abstract": "In recent years, various powerful policy gradient algorithms have been\nproposed in deep reinforcement learning. While all these algorithms build on\nthe Policy Gradient Theorem, the specific design choices differ significantly\nacross algorithms. We provide a holistic overview of on-policy policy gradient\nalgorithms to facilitate the understanding of both their theoretical\nfoundations and their practical implementations. In this overview, we include a\ndetailed proof of the continuous version of the Policy Gradient Theorem,\nconvergence results and a comprehensive discussion of practical algorithms. We\ncompare the most prominent algorithms on continuous control environments and\nprovide insights on the benefits of regularization. All code is available at\nhttps://github.com/Matt00n/PolicyGradientsJax.", "field": "Computer Science", "categories": "cs.LG,cs.AI"}, {"arxiv_id": "2401.13666", "title": "Algebraic methods for solving recognition problems with non-crossing\n  classes", "abstract": "In this paper, we propose to consider various models of pattern recognition.\nAt the same time, it is proposed to consider models in the form of two\noperators: a recognizing operator and a decision rule. Algebraic operations are\nintroduced on recognizing operators, and based on the application of these\noperators, a family of recognizing algorithms is created. An upper estimate is\nconstructed for the model, which guarantees the completeness of the extension.", "field": "Computer Science", "categories": "cs.CV,68T10"}, {"arxiv_id": "2401.13667", "title": "Predicting the Impact of Crashes Across Release Channels", "abstract": "Software maintenance faces a persistent challenge with crash bugs, especially\nacross diverse release channels catering to distinct user bases. Nightly\nbuilds, favoured by enthusiasts, often reveal crashes that are cheaper to fix\nbut may differ significantly from those in stable releases. In this paper, we\nemphasize the need for a data-driven solution to predict the impact of crashes\nhappening on nightly channels once they are released to stable channels. We\nalso list the challenges that need to be considered when approaching this\nproblem.", "field": "Computer Science", "categories": "cs.SE"}]}